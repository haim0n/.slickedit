<!DOCTYPE DeltaFile SYSTEM "http://www.slickedit.com/dtd/vse/vsdelta/9.0/vsdelta.dtd">
<DeltaFile FormatVersion="9.0.0">
<MostRecent Version="6" Comment="" Date="2013/07/09" Time="18:26:45000" NL="\10" Encoding="text">
<Insert>/************************************************************************
* Copyright (C) 2013, Marvell Technology Group Ltd.
* All Rights Reserved.
*
* This is UNPUBLISHED PROPRIETARY SOURCE CODE of Marvell Technology Group;
* the contents of this file may not be disclosed to third parties, copied
* or duplicated in any form, in whole or in part, without the prior
* written permission of Marvell Technology Group.
*
* eth.c
*
* DESCRIPTION:
*     Ethernet device driver implementation.
*
*******************************************************************************/

//#include &lt;linux/kernel.h&gt; /* FIXME Remove and fix types */
#include &lt;adp_os.h&gt;
#include &lt;adp_log.h&gt;
#include &lt;adp_perf_cnt.h&gt;
#include &lt;eth/eth.h&gt;
#include &lt;eth/eth_regs.h&gt;
#include &lt;util/queue.h&gt;
#include &lt;util/string.h&gt;
#include &lt;util/mem.h&gt;
#include &lt;util/cache_ops.h&gt;

//#define ETH_DEBUG

//#define ETH_ERROR(format, args...)  printk("&lt;0&gt;"format"\n", ##args)
#define ETH_ERROR(format, args...)  ADP_ERROR(format, ##args)
#define ETH_INFO(format, args...) ADP_INFO(format, ##args)

/*
#ifdef ETH_DEBUG
#define ETH_DBG(format, args...)  printk(format"\n", ##args)
#else
*/
  #define ETH_DBG(format, args...)
/*
#endif
*/

/* Maximum number of descriptors per Rx queue */
#define ETH_MAX_RX_DESC_NUM 1024

/* Maximum number of descriptors per Tx queue */
#define ETH_MAX_TX_DESC_NUM 1024

/* Maximum MTU */
#define ETH_MAX_MTU 2048 /* NOTE: ZTE MC_SFU requirement */

/* Rx fraem size: MTU + 2 (Marvell Header) + 4 (VLAN) + 14 (MAC hdr) + 4 (CRC) */
#define ETH_FRAME_SIZE(mtu) \
    MEM_ALIGN_UP((mtu) + 2 + 4 + 14 /* ETH header */ + 4, CPU_D_CACHE_LINE_SIZE)

/* Buffer headroom size */
#define ETH_FRAME_PAD 64

/* Total Rx packet buffer size */
#define ETH_PKT_SIZE(mtu) (ETH_FRAME_SIZE(mtu) + ETH_FRAME_PAD)

/* Maximum time to wait for Tx complete on shutdown, mseconds */
#define ETH_TX_COMPLETE_TIMEOUT 100

/* Maximum number of packets buffers in a pool per port */
#define ETH_MAX_PKT_POOL_SIZE (ETH_MAX_RXQ_NUM * (ETH_MAX_RX_DESC_NUM + 16))

/* Rx interrupt mask */
#define ETH_RX_INTR_MASK  (((1 &lt;&lt; ETH_MAX_RXQ_NUM) - 1) &lt;&lt; NETA_CAUSE_RXQ_OCCUP_DESC_OFFS)

/* Tx ring index */
#define ETH_TXQ_INDEX(queue, txp) (ETH_MAX_TXQ_NUM * (txp) + (queue))


/* Descriptors ring */
typedef struct {
  uint8_t port;
  uint8_t queue;
  void    *mem;
  uint32_t mem_size;
  uint32_t first;
  uint32_t last;
  uint32_t next;
  uint32_t num;
  uint32_t pending;
  uint32_t pending_bytes;
} eth_ring_t;


/* Ethernet port */

struct eth_port_t;

typedef struct eth_port eth_port_t;

struct eth_port {
  bool             init; /* true if initialized */
  uint8_t          port; /* Physical port number */
  uint8_t          rxq_num; /* Number of Rx queues */
  uint8_t          txq_num; /* Number of Tx queues */
  uint8_t          txp_num; /* Number of TConts */
  uint32_t         mtu; /* MTU */
  uint32_t         irq; /* Interrupt number */
  uint32_t         hw_cmd; /* HW command spec for Gunit */
  uint32_t         rx_desc_num; /* Number of Rx descriptors */
  uint32_t         tx_desc_num; /* Number of Tx descriptors */
  eth_ops_t        ops; /* Callbacks */
  eth_ring_t      *rxq[ETH_MAX_RXQ_NUM]; /* Rx packet buffer rings */
  eth_ring_t      *txq[ETH_MAX_TXQ_NUM * ETH_MAX_TXP_NUM]; /* Tx packet buffer rings */
  queue_t         *pkt_pool; /* Rx packet buffers */
  uint32_t         pkt_size; /* Size of packet buffers in pkt_pool */
  uint32_t         pkt_pool_size; /* Required size of the packet pool */
  adp_eth_stats_t  stats; /* Port statistics */
  uint32_t         txq_pending; /* bit-mask per queue with any pending pkts */
  uint32_t         txp_pending; /* bit-mask per txp with any pending pkts */
  eth_port_t      *next; /* Linked list */
};


/* Available Ethernet ports. Not all can be active (initialized) */
static eth_port_t eth_ports[ADP_ETH_MAX_PORT_NUM];
static eth_port_t *eth0 = &amp;eth_ports[0];
static eth_port_t *eth1 = &amp;eth_ports[1];
static eth_port_t *eth2 = &amp;eth_ports[2];

extern void dump_data(uint8_t *data, int len);


/************************ Neta helpers ****************/

static inline void eth_neta_rxq_set_offset(int port, int queue, int offset) {
  uint32_t val;

  val = ADP_REG_READ(NETA_RXQ_CONFIG_REG(port, queue));
  val &amp;= ~NETA_RXQ_PACKET_OFFSET_ALL_MASK;

  val |= NETA_RXQ_PACKET_OFFSET_MASK(offset &gt;&gt; 3);

  ADP_REG_WRITE(NETA_RXQ_CONFIG_REG(port, queue), val);
}

/* Get number of sent descriptors */
static inline uint32_t eth_neta_txq_get_sent_desc_num(uint8_t port, uint8_t txp, uint8_t queue) {
  uint32_t val;
  int      sent;

  val = ADP_REG_READ(NETA_TXQ_STATUS_REG(port, txp, queue));
  sent = (val &amp; NETA_TXQ_SENT_DESC_MASK) &gt;&gt; NETA_TXQ_SENT_DESC_OFFS;

  return sent;
}


/* Decrement sent descriptors counter */
static inline void eth_neta_dec_sent_desc_num(uint8_t port, uint8_t txp, uint8_t queue, uint32_t sent) {
  uint32_t val;

  /* Only 255 TX descriptors can be updated at once */
  while (sent &gt; 0xFF) {
    val = (0xFF &lt;&lt; NETA_TXQ_DEC_SENT_OFFS);
    ADP_REG_WRITE(NETA_TXQ_UPDATE_REG(port, txp, queue), val);
    sent = sent - 0xFF;
  }

  val = (sent &lt;&lt; NETA_TXQ_DEC_SENT_OFFS);
  ADP_REG_WRITE(NETA_TXQ_UPDATE_REG(port, txp, queue), val);
}


/* Get number of pending Tx descriptors */
static inline uint32_t eth_neta_txq_get_pending_desc_num(uint8_t port, uint8_t txp, uint8_t queue)
{
  uint32_t val;

  val = ADP_REG_READ(NETA_TXQ_STATUS_REG(port, txp, queue));

  return (val &amp; NETA_TXQ_PENDING_DESC_MASK) &gt;&gt; NETA_TXQ_PENDING_DESC_OFFS;
}


/* Get number of sent descriptors and descrement. Number of sent descriptors is returned. */
static inline uint32_t eth_neta_txq_update_sent_desc(uint8_t port, uint8_t txp, uint8_t queue) {
  uint32_t sent;

  /* Get number of sent descriptors */
  sent = eth_neta_txq_get_sent_desc_num(port, txp, queue);

  /* Decrement sent descriptors counter */
  if (sent) {
      eth_neta_dec_sent_desc_num(port, txp, queue, sent);
  }

  return sent;
}

/* Update HW with number of TX descriptors to be sent */
static inline void eth_neta_txq_pendind_desc_add(uint8_t port, uint8_t txp,
                                                 uint8_t queue, uint8_t pend_desc_num)
{
  uint32_t val;

  /* Only 255 descriptors can be added at once - we don't check it for performance */
  /* Assume caller process TX desriptors in quanta less than 256 */
  val = (pend_desc_num &lt;&lt; NETA_TXQ_ADD_PENDING_OFFS);
  ADP_REG_WRITE(NETA_TXQ_UPDATE_REG(port, txp, queue), val);
}


/* Enable/disable Rx queue */
static inline void eth_neta_rxq_enable(uint8_t port, uint8_t queue, bool enable) {
  uint32_t q_map = (1 &lt;&lt; queue);
  if (!enable) {
      q_map = q_map &lt;&lt; ETH_RXQ_DISABLE_OFFSET;
  }

  ADP_REG_WRITE(ETH_RX_QUEUE_COMMAND_REG(port), q_map);
}

/* Enable/disable Tx queue */
static inline void eth_neta_txq_enable(uint8_t port, uint8_t queue, uint8_t txp, bool enable) {
  uint32_t q_map = (1 &lt;&lt; queue);

  if (!enable) {
      q_map = q_map &lt;&lt; ETH_TXQ_DISABLE_OFFSET;
  }

  ADP_REG_WRITE(ETH_TX_QUEUE_COMMAND_REG(port, txp), q_map);
}

/* Get number of RX descriptors occupied by received packets */
static inline int32_t eth_neta_rxq_get_busy_desc_num(int port, int queue) {
  uint32_t val;

  val = ADP_REG_READ(NETA_RXQ_STATUS_REG(port, queue));

  return (val &amp; NETA_RXQ_OCCUPIED_DESC_ALL_MASK) &gt;&gt; NETA_RXQ_OCCUPIED_DESC_OFFS;
}


/* Decrement number of occupied descriptors, increment number of Non-occupied descriptors.
   Both are combined into simngle parameter rx_filled */
static inline void eth_neta_rxq_desc_num_update(uint32_t port, uint32_t queue, uint32_t rx_filled) {
  uint32_t val;

  if (rx_filled &lt;= 0xFF) {
      val = (rx_filled &lt;&lt; NETA_RXQ_DEC_OCCUPIED_OFFS) | (rx_filled &lt;&lt; NETA_RXQ_ADD_NON_OCCUPIED_OFFS);
      ADP_REG_WRITE(NETA_RXQ_STATUS_UPDATE_REG(port, queue), val);
      return;
  }

  /* Only 255 descriptors can be added at once */
  while (rx_filled &gt; 0) {
    if (rx_filled &lt;= 0xFF) {
      val = (rx_filled &lt;&lt; NETA_RXQ_DEC_OCCUPIED_OFFS) | (rx_filled &lt;&lt; NETA_RXQ_ADD_NON_OCCUPIED_OFFS);
      rx_filled = 0;
    } else {
      val = (0xFF &lt;&lt; NETA_RXQ_DEC_OCCUPIED_OFFS) | (0xFF &lt;&lt; NETA_RXQ_ADD_NON_OCCUPIED_OFFS);
      rx_filled -= 0xFF;
    }
    ADP_REG_WRITE(NETA_RXQ_STATUS_UPDATE_REG(port, queue), val);
  }
}

static inline uint32_t eth_neta_get_next_tx_desc_num(uint8_t port, uint8_t txp,
                                                     uint8_t queue) {
   return ADP_REG_READ(NETA_TXQ_INDEX_REG(port, txp, queue));
}

static inline uint32_t eth_neta_get_next_rx_desc_num(uint8_t port, uint8_t queue) {
   return ADP_REG_READ(NETA_RXQ_INDEX_REG(port, queue));
}

/* Disabel HW Buffer Managament */
static inline uint32_t eth_neta_rxq_bm_disable(uint8_t port, uint8_t queue)
{
  uint32_t val;

  val = ADP_REG_READ(NETA_RXQ_CONFIG_REG(port, queue));
  val &amp;= ~NETA_RXQ_HW_BUF_ALLOC_MASK;
  ADP_REG_WRITE(NETA_RXQ_CONFIG_REG(port, queue), val);

  return ADP_OK;
}

/* Set buffer size for Rx descriptors */
static inline uint32_t eth_neta_rxq_set_buff_size(uint8_t port, uint8_t queue,
                                                  uint32_t buff_size) {
  uint32_t val;

  val = ADP_REG_READ(NETA_RXQ_SIZE_REG(port, queue));
  val &amp;= ~NETA_RXQ_BUF_SIZE_MASK;
  val |= ((buff_size &gt;&gt; 3) &lt;&lt; NETA_RXQ_BUF_SIZE_OFFS);
  ADP_REG_WRITE(NETA_RXQ_SIZE_REG(port, queue), val);

  return ADP_OK;
}

/* Add number of descriptors are ready to receive new packets */
static inline void eth_neta_rxq_add_non_occup_desc(uint8_t port, uint8_t queue,
                                                   uint32_t rx_desc_num) {
  uint32_t val;

  /* Only 255 descriptors can be added at once */
  while (rx_desc_num &gt; 0xFF) {
    val = (0xFF &lt;&lt; NETA_RXQ_ADD_NON_OCCUPIED_OFFS);
    ADP_REG_WRITE(NETA_RXQ_STATUS_UPDATE_REG(port, queue), val);
    rx_desc_num = rx_desc_num - 0xFF;
  }
  val = (rx_desc_num &lt;&lt; NETA_RXQ_ADD_NON_OCCUPIED_OFFS);
  ADP_REG_WRITE(NETA_RXQ_STATUS_UPDATE_REG(port, queue), val);
}



static inline void eth_neta_pon_txq_bytes_add(uint8_t port, uint8_t txp, uint8_t queue, uint32_t bytes)
{
  uint32_t  reg_val;

  reg_val = (NETA_TX_NEW_BYTES_MASK(bytes) | NETA_TX_NEW_BYTES_TXQ_MASK(queue) | NETA_TX_NEW_BYTES_COLOR_GREEN);

  ADP_REG_WRITE(NETA_TX_ADD_BYTES_REG(port, txp), reg_val);
}


/* Get pointer to next Rx descriptor to be processed by SW */
/* TODO Optimize */
static inline eth_rx_desc_t *eth_rxq_next_desc(eth_ring_t *rxq)
{
  uint32_t curr = rxq-&gt;next;

  if ((rxq-&gt;next += NETA_DESC_ALIGNED_SIZE) &gt; rxq-&gt;last) {
      rxq-&gt;next = rxq-&gt;first;
  }

  return (eth_rx_desc_t *) curr;
}


/* Get pointer to next Tx descriptor to be processed by HW */
static inline eth_tx_desc_t *eth_txq_next_desc(eth_ring_t *txq)
{
  uint32_t curr = txq-&gt;next;

  if ((txq-&gt;next += NETA_DESC_ALIGNED_SIZE) &gt; txq-&gt;last) {
      txq-&gt;next = txq-&gt;first;
  }

  return (eth_tx_desc_t *) curr;
}


static inline bool eth_pkt_pool_is_empty(eth_port_t *p) {
  return queue_is_empty(p-&gt;pkt_pool);
}


/* Attach new packet buffer to rx descriptor */
static inline int32_t eth_rx_desc_refill(eth_port_t *p, eth_rx_desc_t *rx_desc) {
  register eth_pkt_t *pkt = (eth_pkt_t*) queue_get(p-&gt;pkt_pool);
  if (pkt == NULL) {
      return ADP_OUT_OF_MEMORY;
  }

  rx_desc-&gt;buf_cookie = (uint32_t) pkt;
  rx_desc-&gt;buf_phys_addr = pkt-&gt;buff_dma;
  pkt-&gt;rx_desc = rx_desc;

  dcache_clean_single_line((uint32_t) rx_desc);

  return ADP_OK;
}


/* Invalidate pending Rx descriptors */
static inline void eth_rx_desc_invalidate(eth_ring_t *rxq, uint32_t rx_pending) {
  register uint32_t curr = rxq-&gt;next;
  register uint32_t top = curr + NETA_DESC_ALIGNED_SIZE * (rx_pending - 1);

  if (top &lt;= rxq-&gt;last) {
      dcache_invalidate_multi_line(curr, top);
      /* FIXME Found to cause instability under high traffic
      for (addr = curr; addr &lt;= top; addr += 32) {
          _PLD(addr);
      }*/
  } else {
      dcache_invalidate_multi_line(curr, rxq-&gt;last);
      /* FIXME Found to cause instability under high traffic
      for (addr = curr; addr &lt;= rxq-&gt;last; addr += 32) {
          _PLD(addr);
      }*/
      dcache_invalidate_multi_line(rxq-&gt;first, top - NETA_DESC_ALIGNED_SIZE * rxq-&gt;num);
      /* FIXME Found to cause instability under high traffic
      for (addr = rxq-&gt;first; addr &lt;= top - NETA_DESC_ALIGNED_SIZE * rxq-&gt;num; addr += 32) {
          _PLD(addr);
      }*/
  }
}

/* Reset port statistics */
void eth_stats_reset(uint8_t port) {
 if (port &lt; ADP_ETH_MAX_PORT_NUM) {
     adp_memset(&amp;eth_ports[port].stats, 0, sizeof(adp_eth_stats_t));
 }
}

/* Suspend the Rx queue and deallocate */
static void eth_rxq_shutdown(eth_port_t *p, uint8_t queue) {

  eth_ring_t    *rxq;
  eth_rx_desc_t *rx_desc;
  uint32_t       rx_pending, i;

  if (queue &gt; (ETH_MAX_RXQ_NUM - 1)) {
      ETH_ERROR("Rx ring number out of range, q (%d), port (%d).", queue, p-&gt;port);
      return;
  }

  rxq = p-&gt;rxq[queue];

  /* Not initialized */
  if (rxq == NULL) {
      return;
  }

  if (rxq-&gt;mem != NULL) {
      /* Disable recv, flush pending Rx descriptors */
      eth_neta_rxq_enable(rxq-&gt;port, rxq-&gt;queue, false);
      rx_pending = eth_neta_rxq_get_busy_desc_num(rxq-&gt;port, rxq-&gt;queue);

      while (rx_pending--) {
        rx_desc = eth_rxq_next_desc(rxq);
      }
      eth_neta_rxq_desc_num_update(rxq-&gt;port, rxq-&gt;queue, rx_pending);

      /* Free associated packets */
      for (i = 0; i &lt; rxq-&gt;num; i++) {
          rx_desc = (eth_rx_desc_t*) (rxq-&gt;first + i * NETA_DESC_ALIGNED_SIZE);
          if (rx_desc-&gt;buf_cookie != 0) {
              p-&gt;ops.pkt_free((eth_pkt_t*) rx_desc-&gt;buf_cookie);
          }
      }

      /* Clear Rx descriptors queue starting address and size */
      ADP_REG_WRITE(NETA_RXQ_BASE_ADDR_REG(rxq-&gt;port, rxq-&gt;queue), 0);
      ADP_REG_WRITE(NETA_RXQ_SIZE_REG(rxq-&gt;port, rxq-&gt;queue), 0);

      /* Free the ring */
      ADP_FREE(rxq-&gt;mem);
  }

  ADP_FREE(rxq);
  p-&gt;rxq[queue] = NULL;
}


/* Init Rx ring for queue */
static int32_t eth_rxq_init(eth_port_t *p, uint8_t queue) {
  eth_ring_t    *rxq;
  eth_rx_desc_t *rx_desc;
  int32_t        err, i, next_rx_num;
  uint32_t       dma;

  if (queue &gt; (ETH_MAX_RXQ_NUM - 1)) {
      ETH_ERROR("Rx ring number out of range, q (%d), port (%d).", queue, p-&gt;port);
      return ADP_INTERNAL_ERROR;
  }

  if (p-&gt;rxq[queue] != NULL) {
      ETH_ERROR("Rx ring already allocated, q (%d), port (%d).", queue, p-&gt;port);
      return ADP_INTERNAL_ERROR;
  }

  /* Alloc queue meta-data */
  rxq = ADP_MALLOC(sizeof(eth_ring_t));
  if (rxq == NULL) {
      ETH_ERROR("Can't allocate Rx ring, q (%d), port (%d).", queue, p-&gt;port);
      err = ADP_OUT_OF_MEMORY;
      goto fail;
  }
  adp_memset(rxq, 0, sizeof(eth_ring_t));

  p-&gt;rxq[queue] = rxq;
  rxq-&gt;port = p-&gt;port;
  rxq-&gt;queue = queue;

   /* Rx descriptors */
  rxq-&gt;mem_size = (p-&gt;rx_desc_num * NETA_DESC_ALIGNED_SIZE) + CPU_D_CACHE_LINE_SIZE;
  rxq-&gt;mem = ADP_MALLOC(rxq-&gt;mem_size);
  if (rxq-&gt;mem == NULL) {
      ETH_ERROR("Can't allocate Rx descriptors, q (%d), port (%d).", queue, p-&gt;port);
      err =  ADP_OUT_OF_MEMORY;
      goto fail;
  }

  /* Reset the ring */
  adp_memset(rxq-&gt;mem, 0, rxq-&gt;mem_size);
  rxq-&gt;num = p-&gt;rx_desc_num;
  rxq-&gt;first = MEM_ALIGN_UP((uint32_t) rxq-&gt;mem, CPU_D_CACHE_LINE_SIZE);
  rxq-&gt;last = rxq-&gt;first + NETA_DESC_ALIGNED_SIZE * (rxq-&gt;num - 1);

  /* Fix of a glitch resulting in skipping non filled descriptors upon reinit */
  next_rx_num = eth_neta_get_next_rx_desc_num(p-&gt;port, queue);

  rxq-&gt;next = rxq-&gt;first + NETA_DESC_ALIGNED_SIZE * next_rx_num;

  ETH_DBG("Next to use Rx desc (%d), q (%d), port (%d)",
            next_rx_num, queue, p-&gt;port);

  /* Rx descriptors starting address */
  dma = adp_os_virt_to_phys((uint8_t*)rxq-&gt;first, NETA_DESC_ALIGNED_SIZE * rxq-&gt;num);
  ADP_REG_WRITE(NETA_RXQ_BASE_ADDR_REG(p-&gt;port, queue), dma);
  ADP_REG_WRITE(NETA_RXQ_SIZE_REG(p-&gt;port, queue), rxq-&gt;num);

  eth_neta_rxq_set_offset(p-&gt;port, queue, 0 /* headroom */);

  /* Refill the Rx descriptors with buffers from the pool */
  eth_neta_rxq_bm_disable(p-&gt;port, queue);
  for (i = 0; i &lt; rxq-&gt;num; i++) {
      rx_desc = (eth_rx_desc_t*) (rxq-&gt;first + i * NETA_DESC_ALIGNED_SIZE);
      adp_memset(rx_desc, 0, sizeof(eth_rx_desc_t));

      err = eth_rx_desc_refill(p, rx_desc);
      if (err) {
          goto fail;
      }
  }
  eth_neta_rxq_set_buff_size(p-&gt;port, queue, p-&gt;pkt_size);
  eth_neta_rxq_add_non_occup_desc(p-&gt;port, queue, rxq-&gt;num);

  /* TODO Move to open() */
  eth_neta_rxq_enable(rxq-&gt;port, rxq-&gt;queue, true);

  return ADP_OK;

fail:
  eth_rxq_shutdown(p, queue);

  return err;
}


/* Complete Tx pending packets and deallocate the queue */
static void eth_txq_shutdown(eth_port_t *p, uint8_t queue, uint8_t txp) {
  eth_ring_t *txq;
  uint32_t tx_pending, elapsed, i;

  if (queue &gt; (ETH_MAX_TXQ_NUM - 1)) {
      ETH_ERROR("Tx ring number out of range, q (%d), port (%d).", queue, p-&gt;port);
      return;
  }

  if (txp &gt; (ETH_MAX_TXP_NUM - 1)) {
      ETH_ERROR("Tcont number out of range, txp (%d), port (%d).", txp, p-&gt;port);
      return;
  }

  i = ETH_TXQ_INDEX(queue, txp);
  txq = p-&gt;txq[i];

  /* Not init */
  if (txq == NULL) {
      return;
  }

  if (txq-&gt;mem != NULL) {
    /* Complete pending Tx */
    elapsed = 0;
    while (elapsed &lt; ETH_TX_COMPLETE_TIMEOUT &amp;&amp;
           (tx_pending = eth_neta_txq_get_pending_desc_num(txq-&gt;port, txp, txq-&gt;queue))) {
      elapsed += 1;
      adp_os_msleep(1);
    }

    if (tx_pending) {
        ETH_ERROR("Exceeded Tx complete timeout (%d) ms, (%d) decsriptors pending in port (%d), queue (%d)).",
                  ETH_TX_COMPLETE_TIMEOUT, tx_pending, txq-&gt;port, txq-&gt;queue);
    }

    eth_neta_txq_enable(txq-&gt;port, txq-&gt;queue, txp, false);

    /* Zero out sent descriptors */
    eth_neta_txq_update_sent_desc(txq-&gt;port, txp, txq-&gt;queue);

    /* Set minimum bandwidth */
    ADP_REG_WRITE(NETA_TXQ_TOKEN_CNTR_REG(txq-&gt;port, txp, txq-&gt;queue), 0);

    /* Reset Tx descriptors queue starting address and size */
    ADP_REG_WRITE(NETA_TXQ_BASE_ADDR_REG(txq-&gt;port, txp, txq-&gt;queue), 0);
    ADP_REG_WRITE(NETA_TXQ_SIZE_REG(txq-&gt;port, txp, txq-&gt;queue), 0);

    /* Free the ring memory */
    ADP_FREE(txq-&gt;mem);
  }

  ADP_FREE(txq);
  p-&gt;txq[i] = NULL;
}


static uint32_t eth_txq_init(eth_port_t *p, uint8_t queue, uint8_t txp) {
  eth_ring_t    *txq;
  int32_t        err;
  uint32_t       dma, i;
  eth_tx_desc_t *tx_desc;

  if (queue &gt; (ETH_MAX_TXQ_NUM - 1)) {
      ETH_ERROR("Tx ring number out of range, q (%d), port (%d).", queue, p-&gt;port);
      return ADP_INTERNAL_ERROR;
  }

  if (txp &gt; (ETH_MAX_TXP_NUM - 1)) {
      ETH_ERROR("Tcont number out of range, txp (%d), port (%d).", txp, p-&gt;port);
      return ADP_INTERNAL_ERROR;
  }

  i = ETH_TXQ_INDEX(queue, txp);
  if (p-&gt;txq[i] != NULL) {
      ETH_ERROR("Tx ring already allocated, q (%d), port (%d).", queue, p-&gt;port);
      return ADP_INTERNAL_ERROR;
  }

  /* Alloc queue meta-data */
  txq = ADP_MALLOC(sizeof(eth_ring_t));
  if (txq == NULL) {
      ETH_ERROR("Can't allocate Tx ring, q (%d), port (%d).", queue, p-&gt;port);
      err = ADP_OUT_OF_MEMORY;
      goto fail;
  }

  adp_memset(txq, 0, sizeof(eth_ring_t));

  p-&gt;txq[i] = txq;
  txq-&gt;port = p-&gt;port;
  txq-&gt;queue = queue;

   /* Memory for Tx descriptors */
  txq-&gt;mem_size = (p-&gt;tx_desc_num * NETA_DESC_ALIGNED_SIZE) + CPU_D_CACHE_LINE_SIZE;
  txq-&gt;mem = ADP_MALLOC(txq-&gt;mem_size);
  if (txq-&gt;mem == NULL) {
      ETH_ERROR("Can't allocate Tx descriptors, q (%d), port (%d).", queue, p-&gt;port);
      err = ADP_OUT_OF_MEMORY;
      goto fail;
  }

  /* Reset the ring */
  adp_memset(txq-&gt;mem, 0, txq-&gt;mem_size);
  txq-&gt;num = p-&gt;tx_desc_num;

  txq-&gt;first = MEM_ALIGN_UP((uint32_t) txq-&gt;mem, CPU_D_CACHE_LINE_SIZE);
  txq-&gt;last = txq-&gt;first + NETA_DESC_ALIGNED_SIZE * (txq-&gt;num - 1);
  txq-&gt;next = txq-&gt;first + NETA_DESC_ALIGNED_SIZE * eth_neta_get_next_tx_desc_num(p-&gt;port, txp, queue);

  for (i = 0; i &lt; txq-&gt;num; i++) {
      tx_desc = (eth_tx_desc_t*) (txq-&gt;first + i * NETA_DESC_ALIGNED_SIZE);
      tx_desc-&gt;hw_cmd = p-&gt;hw_cmd;
  }

  ETH_DBG("Next to use Tx desc (%d), q (%d), port (%d)",
          eth_neta_get_next_tx_desc_num(p-&gt;port, txp, queue), queue, p-&gt;port);

  /* Maximum bandwidth */
  ADP_REG_WRITE(NETA_TXQ_TOKEN_CNTR_REG(p-&gt;port, txp, queue), NETA_TXQ_TOKEN_CNTR_MAX);

  /* Tx descriptors ring starting address */
  dma = adp_os_virt_to_phys((uint8_t*)txq-&gt;first, NETA_DESC_ALIGNED_SIZE * txq-&gt;num);
  ADP_REG_WRITE(NETA_TXQ_BASE_ADDR_REG(p-&gt;port, txp, queue), dma);


  ADP_REG_WRITE(NETA_TXQ_SIZE_REG(p-&gt;port, txp, queue), NETA_TXQ_DESC_NUM_MASK(p-&gt;tx_desc_num));

  /* TODO Move to open() */
  eth_neta_txq_enable(txq-&gt;port, txq-&gt;queue, txp, true);

  return ADP_OK;

fail:
  eth_txq_shutdown(p, queue, txp);

  return err;
}


/* Transmit pending packets on all queues */
static inline void eth_tx(eth_port_t *p) {
  register eth_ring_t *txq;
  register uint8_t     queue;
  register uint32_t    sent = 0;
  register uint8_t     port = p-&gt;port;
  register uint32_t    txq_pending = p-&gt;txq_pending;

  /* Transmit */
  while (txq_pending) {
    queue = adp_fls(txq_pending) - 1;

    txq = p-&gt;txq[queue];

    eth_neta_txq_pendind_desc_add(p-&gt;port, 0 /* txp */, queue, txq-&gt;pending);
    txq_pending ^= (1 &lt;&lt; queue);
    txq-&gt;pending = 0;
    txq-&gt;pending_bytes = 0;
    sent += eth_neta_txq_update_sent_desc(port, 0 /* txp */, queue);
  }

  p-&gt;txq_pending  = 0;

  if (sent) {
      p-&gt;ops.tx_done(port, sent);
  }
}

#ifdef ETH_PON
/* Transmit pending packets on all queues */
static inline void eth_tx_pon(eth_port_t *p) {
  register eth_ring_t *txq;
  register uint8_t     queue;
  register uint32_t    sent = 0;
  register uint8_t     port = p-&gt;port;
  register uint32_t    txq_pending;
  register uint32_t    txp_pending = p-&gt;txp_pending;
  uint32_t             txp;

  /* Transmit */
   while (txp_pending) {
       printk(KERN_ERR "(%s:%d) txp_pending %#x\n", __func__, __LINE__, txp_pending);
       txp = adp_fls(txp_pending) - 1;

       txq_pending = p-&gt;txq_pending;
       while (txq_pending) {
           queue = adp_fls(txq_pending) - 1;
           txq = p-&gt;txq[ETH_TXQ_INDEX(queue, txp)];
           if (txq-&gt;pending) {
               eth_neta_txq_pendind_desc_add(p-&gt;port, txp, queue, txq-&gt;pending);
               eth_neta_pon_txq_bytes_add(port, txp, queue, txq-&gt;pending_bytes);
               sent += eth_neta_txq_update_sent_desc(port, txp, queue);
               txq-&gt;pending = 0;
           }

           txq_pending ^= (1 &lt;&lt; queue);
       }

       txp_pending ^= (1 &lt;&lt; txp);
  }

  p-&gt;txq_pending  = 0;
  p-&gt;txp_pending  = 0;

  if (sent) {
      p-&gt;ops.tx_done(port, sent);
  }
}
#endif /* ETH_PON */

/* Return Rx descriptors back to HW  */
static inline void eth_rx_desc_return(eth_port_t *p, uint8_t queue) {
  register uint32_t *rx_pending = &amp;p-&gt;rxq[queue]-&gt;pending;

  /* Rx queue must be refilled by now */
  eth_neta_rxq_desc_num_update(p-&gt;port, queue, *rx_pending /* Processed &amp; newly added */);
  *rx_pending = 0;
}


/* Remove all pkts from all ports all queues and return rx decsriptors */
void eth_recv_all(void) {
   uint32_t rx_pending, i, q;

   for (i = 0; i &lt; ADP_ETH_MAX_PORT_NUM; i++) {
       for (q = 0; q &lt; ETH_MAX_RXQ_NUM; q++) {
            rx_pending = eth_neta_rxq_get_busy_desc_num(i, q);
            if (rx_pending) {
                //printk("&lt;0&gt;ETH: recved %d pkts, port %d, q %d\n", rx_pending, i, q);
            }
            eth_neta_rxq_desc_num_update(i, q, rx_pending);
        }
   }
}


/* Calculate period and tokens accordingly with required rate and accuracy */
int32_t eth_calc_rate(int32_t rate, uint32_t accuracy, uint32_t *period, uint32_t *tokens)
{
  /* Calculate refill tokens and period - rate [Kbps] = tokens [bits] * 1000 / period [usec] */
  /* Assume:  Tclock [MHz] / BasicRefillNoOfClocks = 1 */
  uint32_t per, tok, calc;

  if (rate == 0) {
      /* Disable traffic from the port: tokens = 0 */
      if (period != NULL)
          *period = 1000;
      if (tokens != NULL)
          *tokens = 0;

      return ADP_OK;
  }

  /* Find values of "period" and "tokens" match "rate" and "accuracy" when period is minimal */
  for (per = 1; per &lt;= 1000; per++) {
        tok = 1;
        while (true) {
          calc = (tok * 1000) / per;
          if (((MEM_ABS(calc - rate) * 100) / rate) &lt;= accuracy) {
              if (period != NULL)
                  *period = per;

              if (tokens != NULL)
                 *tokens = tok;

              return ADP_OK;
          }
          if (calc &gt; rate)
              break;

          tok++;
        }
  }

  return ADP_FAIL;
}

int32_t eth_check_port_args(uint8_t port, uint8_t txp) {
  int32_t err = ADP_OK;

  if (port &gt; ADP_ETH_MAX_PORT_NUM || !eth_ports[port].init) {
      ETH_ERROR("Port (%d) invalid or inactive.", port);
      err = ADP_PORT_INACTIVE;
  } else if (txp &gt; ETH_MAX_TXP_NUM) {
      ETH_ERROR("TCont (%d) invalid.", txp);
      err = ADP_PORT_INACTIVE;
  }

  return err;
}

/* Set port egress rate limit, rate 0 means no limit */
int32_t eth_egress_rate_set(uint8_t port, uint8_t txp, uint32_t rate) {
  uint32_t reg_val;
  uint32_t tokens, period, accuracy;
  int32_t err;

  if (eth_check_port_args(port, txp) != ADP_OK) {
      return ADP_INVALID_PARAM;
  }

  if (rate != 0) {
      tokens = period = accuracy = 0;

      err = eth_calc_rate(rate, accuracy, &amp;period, &amp;tokens);
      if (err != ADP_OK) {
          ETH_ERROR("Can't provide rate of %d [Kbps] with accuracy of %d.", rate, accuracy);
          return err;
      }
      if (tokens &gt; NETA_TXP_REFILL_TOKENS_MAX)
          tokens = NETA_TXP_REFILL_TOKENS_MAX;

      if (period &gt; NETA_TXP_REFILL_PERIOD_MAX)
          period = NETA_TXP_REFILL_PERIOD_MAX;
  } else { /* no limit */
      tokens = NETA_TXP_REFILL_TOKENS_MAX;
      period = NETA_TXP_REFILL_PERIOD_MAX;
  }

  reg_val = ADP_REG_READ(NETA_TXP_REFILL_REG(port, txp));

  reg_val &amp;= ~NETA_TXP_REFILL_TOKENS_ALL_MASK;
  reg_val |= NETA_TXP_REFILL_TOKENS_MASK(tokens);

  reg_val &amp;= ~NETA_TXP_REFILL_PERIOD_ALL_MASK;
  reg_val |= NETA_TXP_REFILL_PERIOD_MASK(period);

  ADP_REG_WRITE(NETA_TXP_REFILL_REG(port, txp), reg_val);

  return ADP_OK;
}


/* Set port egress burst size (kbit), before port rate limit takes places */
int32_t eth_egress_burst_size_set(uint8_t port, uint8_t txp, uint32_t burst_size) {
  uint32_t mtu;

  if (eth_check_port_args(port, txp) != ADP_OK) {
      return ADP_INVALID_PARAM;
  }

  burst_size *= 8;

  if (burst_size &gt; NETA_TXP_TOKEN_SIZE_MAX)
      burst_size = NETA_TXP_TOKEN_SIZE_MAX;

  /* Token bucket size must be larger then MTU */
  mtu = ADP_REG_READ(NETA_TXP_MTU_REG(port, txp));
  if (burst_size &lt; ETH_MAX_MTU) {
      ETH_ERROR("Burst size (%d, bits) &lt; mtu (%d, bits).", burst_size, mtu);
      return ADP_INVALID_PARAM;
  }
  ADP_REG_WRITE(NETA_TXP_TOKEN_SIZE_REG(port, txp), burst_size);

  return MV_OK;
}


/* Set Tx queue to work in Strict Priority mode */
int32_t eth_txq_strict_pri_set(uint8_t port, uint8_t txp, uint8_t queue) {
  uint32_t reg_val;

  if (eth_check_port_args(port, txp) != ADP_OK) {
      return ADP_INVALID_PARAM;
  }

  if (queue &gt; ETH_MAX_RXQ_NUM - 1) {
      ETH_ERROR("Invalid Tx queue (%d).", queue);
      return ADP_INVALID_PARAM;
  }

  reg_val = ADP_REG_READ(NETA_TX_FIXED_PRIO_CFG_REG(port, txp));
  reg_val |= (1 &lt;&lt; queue);
  ADP_REG_WRITE(NETA_TX_FIXED_PRIO_CFG_REG(port, txp), reg_val);

  return ADP_OK;
}

/* Set Tx queue to work in Weighted Round Robin mode */
int32_t eth_txq_wrr_pri_set(uint8_t port, uint8_t txp, uint8_t queue, uint16_t weight)
{
  uint32_t reg_val, mtu;

  if (eth_check_port_args(port, txp) != ADP_OK) {
      return ADP_INVALID_PARAM;
  }

  if (queue &gt; ETH_MAX_RXQ_NUM - 1) {
      ETH_ERROR("Invalid Tx queue (%d).", queue);
      return ADP_INVALID_PARAM;
  }

  /* Weight * 256 bytes * 8 bits must be larger then MTU [bits] */
  mtu = ADP_REG_READ(NETA_TXP_MTU_REG(port, txp));

  ETH_ERROR("%s: port=%d, txp=%d, txq=%d, weight=%d, mtu=%d (bits)",
      __func__, port, txp, queue, weight, mtu);

  if ((weight * 8 * 256 &lt;= mtu) || (weight &gt; NETA_TXQ_WRR_WEIGHT_MAX)) {
      ETH_ERROR("Set WRR failed, weight (%d) out of range (%d, %d].",
                weight, mtu, NETA_TXQ_WRR_WEIGHT_MAX);
    return ADP_INVALID_PARAM;
  }

  reg_val = ADP_REG_READ(NETA_TXQ_WRR_ARBITER_REG(port, txp, queue));

  reg_val &amp;= ~NETA_TXQ_WRR_WEIGHT_ALL_MASK;
  reg_val |= NETA_TXQ_WRR_WEIGHT_MASK(weight);
  ADP_REG_WRITE(NETA_TXQ_WRR_ARBITER_REG(port, txp, queue), reg_val);

  reg_val = ADP_REG_READ(NETA_TX_FIXED_PRIO_CFG_REG(port, txp));
  reg_val &amp;= ~(1 &lt;&lt; queue);
  ADP_REG_WRITE(NETA_TX_FIXED_PRIO_CFG_REG(port, txp), reg_val);

  return ADP_OK;
}


/* Recv packets from queue */
static inline void eth_rx(eth_port_t *p, uint8_t queue) {
  register eth_ring_t    *rxq;
  register eth_rx_desc_t *rx_desc;
  register eth_pkt_t     *pkt;
  register uint8_t       *phy_hdr;
  register uint8_t        port = p-&gt;port;
  uint32_t                rx_pending, rx_status;

  rxq = p-&gt;rxq[queue];

  if ((rxq-&gt;pending = (rx_pending = eth_neta_rxq_get_busy_desc_num(port, queue)))) {

    ETH_DBG("q = %d, rxq-&gt;pending = %d", queue, rxq-&gt;pending);

    /* Invalidate pending Rx descriptors */
    /* FIXME Found to cause instability under high traffic */
    //eth_rx_desc_invalidate(rxq, rx_pending);

    do {
      /* Next Rx descriptor */
      rx_desc = eth_rxq_next_desc(rxq);

      dcache_invalidate_single_line((uint32_t)rx_desc);

      ETH_DBG("rx_desc = 0x%x, status = 0x%x", (uint32_t)rx_desc, rx_desc-&gt;status);

      rx_status = rx_desc-&gt;status;

      if (((rx_status &amp; NETA_RX_FL_DESC_MASK) != NETA_RX_FL_DESC_MASK) ||
          (rx_status &amp; NETA_RX_ES_MASK)) {

          ETH_DBG("Error recving pkt, rx_status (0x%x), port (%d)",
                   rx_status, p-&gt;port);
          p-&gt;stats.rx_errors++;
          /* Descriptor stays with the same buffer, no need to refill */
          continue;
      }

      /* Recved pkt meta-data and body */
      pkt = (eth_pkt_t *) rx_desc-&gt;buf_cookie;
      phy_hdr = pkt-&gt;phy_hdr = pkt-&gt;buff;
      pkt-&gt;phy_hdr_dma = pkt-&gt;buff_dma;

      /* Invalidate &amp; preload pkt header */
      dcache_invalidate_double_line((uint32_t)(phy_hdr));

      pkt-&gt;status = ETH_PKT_STATUS_NONE;
      pkt-&gt;port = port;
      pkt-&gt;queue = queue;
      pkt-&gt;size = (rx_desc-&gt;data_size - (ETH_CRC_SIZE + ETH_MH_SIZE));
      pkt-&gt;eth_hdr = (adp_eth_hdr_t*)(phy_hdr + ETH_MH_SIZE);
#ifdef CONFIG_MV_ETH_PNC
      pkt-&gt;ip_hdr = (adp_ip_hdr_t*) (phy_hdr + NETA_RX_GET_IPHDR_OFFSET(rx_desc));
#else
  if (NETA_RX_IS_VLAN(pkt-&gt;rx_desc))
      pkt-&gt;ip_hdr = (adp_ip_hdr_t*) (pkt-&gt;phy_hdr + MV_ETH_MH_SIZE + sizeof(adp_vlan_ethhdr_t));
  else
      pkt-&gt;ip_hdr = (adp_ip_hdr_t*) (pkt-&gt;phy_hdr + MV_ETH_MH_SIZE + sizeof(adp_eth_hdr_t));
      /* HAIM FIXME : add parsing for pppoe &amp;&amp; DVLAN */
#endif
      ETH_DBG("Recvd pkt size port %d, desc bytes %d, %d bytes",
               port, rx_desc-&gt;data_size, pkt-&gt;size);

#if 0
      dcache_invalidate_multi_line((uint32_t)pkt-&gt;eth_hdr, pkt-&gt;size - 1);
      ETH_DBG("____ PORT %d, %d _____** RX ** size=%d", p-&gt;port, queue, pkt-&gt;size);
      dump_data(pkt-&gt;eth_hdr, 32);
#endif

      /* Drop if in panic mode */
      if (eth_pkt_pool_is_empty(p)) {
          p-&gt;stats.rx_dropped++;
          continue;
      }

      /* Forward pkt for handling */
      p-&gt;ops.pkt_rx(pkt);
      switch (pkt-&gt;status) {
        case ETH_PKT_STATUS_FWD:
          eth_pkt_tx(pkt);
          break;
        case ETH_PKT_STATUS_LCL:
          /* Must succeed */
          eth_rx_desc_refill(p, rx_desc);
          break;

        default:
          p-&gt;stats.rx_dropped++;
          continue;
      }

      p-&gt;stats.rx_pkts++;
      p-&gt;stats.rx_bytes += pkt-&gt;size;

    } while (--rx_pending);

    p-&gt;ops.rx_done(port);
  }
}


/* Transmit pending packets on all active ports */
inline void eth_tx_all(void) {
  /* Complete cache write to DDR */
  dcache_drain_write_buffer();

  /* FIXME Make optimized loop for eth0 ... eth3 */
  if (eth0-&gt;init) {
      eth_tx(eth0);
  }

  if (eth1-&gt;init) {
      eth_tx(eth1);
  }

  if (eth2-&gt;init) {
      eth_tx_pon(eth2);
  }
}



/* Receive packets from port active queues */
adp_irqreturn_t eth_poll(int32_t irq, void *dev) {
  register eth_port_t *p = (eth_port_t *) dev;
  register uint32_t cause, cause_save;

  ETH_DBG("eth_poll, irq %d", irq);

  cause = ADP_REG_READ(NETA_INTR_NEW_CAUSE_REG(p-&gt;port)) &amp; MV_ETH_RX_INTR_MASK;
  cause_save = cause = (cause &gt;&gt; NETA_CAUSE_RXQ_OCCUP_DESC_OFFS) &amp; ((1 &lt;&lt; ETH_MAX_RXQ_NUM) - 1);

  /* TODO Handle link control &amp; tx events */
  /* TODO Add budget */

  /* Recv packets */
  /* TODO: broken by seugene to compile without linux */
  while (cause) {
    register uint8_t rx_queue = adp_fls(cause) - 1;
    eth_rx(p, rx_queue);
    cause ^= (1 &lt;&lt; rx_queue);
  }

  /* Transmit pending packets on all ports */
  eth_tx_all();

  /* Return rx decriptors to GUnit */
  cause = cause_save;
  /* TODO: broken by seugene to compile without linux */
  while (cause) {
    register uint8_t rx_queue = adp_fls(cause) - 1;
    eth_rx_desc_return(p, rx_queue);
    cause ^= (1 &lt;&lt; rx_queue);
  }

  return ADP_IRQ_HANDLED;
}


/* Schedule packet for transmission */
int32_t eth_pkt_tx(eth_pkt_t* pkt) {
  register eth_port_t    *p = &amp;eth_ports[pkt-&gt;port];
  register uint8_t       txp = pkt-&gt;txp;
  register uint8_t       queue = pkt-&gt;queue;
  register eth_ring_t    *txq = p-&gt;txq[ETH_TXQ_INDEX(queue, txp)];
  register eth_tx_desc_t *tx_desc;

  tx_desc = eth_txq_next_desc(txq);

  if (tx_desc == NULL) {
     p-&gt;stats.tx_dropped++;

     return ADP_OUT_OF_TXQ_DESC;
  }

  tx_desc-&gt;command = pkt-&gt;tx_cmd;

#ifdef ETH_PON
  if (ETH_PORT_IS_PON(pkt-&gt;port)) {
      tx_desc-&gt;hw_cmd = pkt-&gt;hw_cmd;
      txq-&gt;pending_bytes += pkt-&gt;size;
  }
#endif /* ETH_PON */

  p-&gt;stats.tx_bytes += tx_desc-&gt;data_size = pkt-&gt;size;
  tx_desc-&gt;buf_phys_addr = pkt-&gt;phy_hdr_dma;

  /* Clean the Tx descriptor */
  dcache_clean_single_line((uint32_t)tx_desc);

  /* Clean &amp; invalidate the header. On Rx will not need to invalidate */
  /* FIXME Below will work not as expected if pkt-&gt;phy_hdr not cache aligned,
     temp fix using multi-line. Need to compare performance of multi-line
     to 3 cache line inv. */
  //dcache_clean_invalidate_double_line((uint32_t)pkt-&gt;phy_hdr);
  dcache_clean_invalidate_multi_line((uint32_t)pkt-&gt;phy_hdr,
                                     (uint32_t)pkt-&gt;phy_hdr + 2*CPU_D_CACHE_LINE_SIZE);
  txq-&gt;pending++;
  p-&gt;txq_pending |= (1 &lt;&lt; queue);
  /* NOTE pkt need not be returned to the pkt_pool:
   Case A. Fast Path origin: it's already owned by Rx descriptor;
   Case B. Slow Path origin: the owner is ADP Slow Path adapter being responsible
           for pkt dealloc */

  p-&gt;stats.tx_pkts++;
  p-&gt;stats.tx_bytes += pkt-&gt;size;

  return ADP_OK;
}


/* Redeem buffer back to pool.
   Normally, called from a slow path context */
int32_t eth_pkt_put(eth_pkt_t* pkt) {
  eth_port_t *p;

  if (ETH_IS_PKT_FAST_PATH(pkt)) {
      p = &amp;eth_ports[pkt-&gt;pool_id];
      if (p-&gt;init &amp;&amp; p-&gt;pkt_size == pkt-&gt;alloc_size) {
          return queue_put(p-&gt;pkt_pool, pkt);
      }
  }

  ETH_ERROR("Invalid port (%d) state, size (%d) or pool id  (%d) for redeemed pkt.",
             pkt-&gt;pool_id, pkt-&gt;alloc_size, pkt-&gt;pool_id);

  return ADP_INVALID_PARAM;
}


/* Get port statistics */
int32_t eth_stats_get(uint8_t port, adp_eth_stats_t *stats) {
  if (port &gt; (ADP_ETH_MAX_PORT_NUM - 1)) {
      ETH_DBG("Port (%d) out of range.", port);
      return ADP_INVALID_PARAM;
  }

  adp_memcpy(stats, &amp;eth_ports[port].stats, sizeof(adp_eth_stats_t));

  return ADP_OK;
}


/* Create packet buffers pool */
static inline int32_t eth_pkt_pool_create(eth_port_t *p) {
  uint32_t i;

  if (p-&gt;pkt_pool != NULL) {
      ETH_ERROR("Packet pool already created, port (%d).", p-&gt;port);
      return ADP_INTERNAL_ERROR;
  }

  p-&gt;pkt_pool = queue_create(p-&gt;pkt_pool_size);
  if (p-&gt;pkt_pool == NULL) {
      ETH_ERROR("Out of memory allocating packet pool, port (%d).", p-&gt;port);
      return ADP_OUT_OF_MEMORY;
  }

  for (i = 0; i &lt; p-&gt;pkt_pool_size; i++) {

       /* Custom alloc */
       eth_pkt_t *pkt = p-&gt;ops.pkt_alloc(p-&gt;pkt_size);
       if (pkt == NULL) {
           ETH_ERROR("Out of memory allocating packet pool buffers, port (%d).", p-&gt;port);
           return ADP_OUT_OF_MEMORY;
       }

       /* Consistency */
      if (pkt-&gt;buff_dma == 0 || pkt-&gt;buff == NULL || pkt-&gt;alloc_size &lt; p-&gt;pkt_size) {
          ETH_ERROR("Invalid allocated packet returned, port (%d).", p-&gt;port);
          p-&gt;ops.pkt_free(pkt);
          return ADP_INTERNAL_ERROR;
      }

      /* Invalidate cache for packet header and add to pool */
      dcache_invalidate_double_line(MEM_ALIGN_DOWN((uint32_t)pkt-&gt;buff, CPU_D_CACHE_LINE_SIZE));
      pkt-&gt;port = pkt-&gt;pool_id = p-&gt;port;
      if (!queue_put(p-&gt;pkt_pool, pkt)) { /* Theoretical, can't fail */
          ETH_ERROR("Can't add buffer to packet pool, port (%d).", p-&gt;port);
          p-&gt;ops.pkt_free(pkt);
          return ADP_INTERNAL_ERROR;
      }
  }

  return ADP_OK;
}

/* Destroy packet buffers pool */
static inline int32_t eth_pkt_pool_destroy(eth_port_t *p) {
  eth_pkt_t *pkt;

  if (p-&gt;pkt_pool == NULL) {
      ETH_ERROR("Packet pool already NULL, port (%d).", p-&gt;port);
      return ADP_INTERNAL_ERROR;
  }

  while ((pkt = (eth_pkt_t *)queue_get(p-&gt;pkt_pool)) != NULL) {
      p-&gt;ops.pkt_free(pkt);
  }

  queue_destroy(p-&gt;pkt_pool);
  p-&gt;pkt_pool = NULL;

  return ADP_OK;
}

/* Status of a port packet pool */
bool eth_pkt_pool_is_full(uint8_t port) {
  bool rc = true;

  if (ETH_PORT_VALID(port)) {
      eth_port_t *p = &amp;eth_ports[port];
      if (p-&gt;init) {
          rc = queue_is_full(p-&gt;pkt_pool);
      }
  }

  return rc;
}

/* Init Ethernet port. Allocate Rx &amp; Tx rings and packet buffer pool */
int32_t eth_init(eth_config_t *cfg) {
  eth_port_t *p;
  uint32_t err, queue, txp;

  ETH_DBG("Port (%d) init started.", cfg-&gt;port);

  /* Validate params */
  if (cfg-&gt;port &gt; (ADP_ETH_MAX_PORT_NUM - 1)) {
      ETH_ERROR("Port (%d) out of range.", cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  p = &amp;eth_ports[cfg-&gt;port];

  if (p-&gt;init) {
      ETH_DBG("Port (%d) already initialized.", cfg-&gt;port);
      goto ok;
  }

  if (cfg-&gt;rxq_num &gt; ETH_MAX_RXQ_NUM) {
      ETH_ERROR("Rx queue number invalid (%d), port (%d).", cfg-&gt;rxq_num, cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  if (cfg-&gt;txq_num &gt; ETH_MAX_TXQ_NUM) {
      ETH_ERROR("Tx queue number invalid (%d), port (%d).", cfg-&gt;txq_num, cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  if (cfg-&gt;txp_num &gt; ETH_MAX_TXP_NUM) {
      ETH_ERROR("TCont number invalid (%d), port (%d).", cfg-&gt;txp_num, cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  if (cfg-&gt;rx_desc_num &gt; ETH_MAX_RX_DESC_NUM) {
      ETH_ERROR("Rx descriptors number invalid(%d), port (%d).",
                cfg-&gt;rx_desc_num, cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  if (cfg-&gt;tx_desc_num &gt; ETH_MAX_TX_DESC_NUM) {
      ETH_ERROR("Tx descriptors number invalid (%d), port (%d).",
                cfg-&gt;tx_desc_num, cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  if (cfg-&gt;pkt_pool_size &gt; ETH_MAX_PKT_POOL_SIZE ||
      cfg-&gt;pkt_pool_size &lt; cfg-&gt;rx_desc_num) {
      ETH_ERROR("Packet pool size invalid (%d), port (%d).",
                cfg-&gt;pkt_pool_size, cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  if (cfg-&gt;mtu &gt; ETH_MAX_MTU) {
      ETH_ERROR("MTU (%d) invalid, port (%d).", cfg-&gt;mtu, cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  if (cfg-&gt;ops.pkt_alloc == NULL || cfg-&gt;ops.pkt_free == NULL ||
      cfg-&gt;ops.pkt_rx == NULL || cfg-&gt;ops.rx_done == NULL ||
      cfg-&gt;ops.tx_done == NULL) {
      ETH_ERROR("NULL callback(s), port (%d).", cfg-&gt;port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  p-&gt;port = cfg-&gt;port;
  p-&gt;rxq_num = cfg-&gt;rxq_num;
  p-&gt;txq_num = cfg-&gt;txq_num;
  p-&gt;txp_num = cfg-&gt;txp_num;
  p-&gt;rx_desc_num = cfg-&gt;rx_desc_num;
  p-&gt;tx_desc_num = cfg-&gt;tx_desc_num;
  p-&gt;txq_pending = 0;
  p-&gt;txp_pending = 0;
  p-&gt;mtu = cfg-&gt;mtu;
  p-&gt;pkt_size = ETH_PKT_SIZE(cfg-&gt;mtu);
  p-&gt;pkt_pool_size = cfg-&gt;pkt_pool_size;
  p-&gt;hw_cmd = 0;
  p-&gt;ops = cfg-&gt;ops;
  p-&gt;irq = NET_TH_RXTX_IRQ_NUM(cfg-&gt;port);

  ETH_DBG("Using IRQ %d for port %d", p-&gt;irq, cfg-&gt;port);

  /* Register IRQ handler */
  eth_irq_disable(cfg-&gt;port);
  err = adp_os_irq_alloc(p-&gt;irq, eth_poll, "adp_eth", p);
  if (err) {
      goto fail;
  }

  /* Create Rx packet pool */
  err = eth_pkt_pool_create(p);
  if (err) {
      goto fail;
  }

  /* Alloc Rx rings */
  for (queue = 0; queue &lt; p-&gt;rxq_num; queue++) {
      err = eth_rxq_init(p, queue);
      if (err) {
          goto fail;
      }
  }

  /* Alloc Tx rings */
  for (queue = 0; queue &lt; p-&gt;txq_num; queue++) {
       for (txp = 0; txp &lt; p-&gt;txp_num; txp++) {
            err = eth_txq_init(p, queue, txp);
            if (err) {
                goto fail;
            }
       }
  }

  /* Stats */
  eth_stats_reset(p-&gt;port);

  p-&gt;init = true;

ok:
  ETH_DBG("Eth port %d init Ok.", cfg-&gt;port);

  return ADP_OK;

fail:
  eth_shutdown(cfg-&gt;port);

  return err;
}


/* Uninitialize port. Deallocate Rx &amp; Tx rings and packet buffer pool */
int32_t eth_shutdown(uint8_t port) {

  eth_port_t *p;
  uint32_t err, queue, txp;
  bool is_init;

  if (port &gt; (ADP_ETH_MAX_PORT_NUM - 1)) {
      ETH_ERROR("Port (%d) out of range.", port);
      err = ADP_INVALID_PARAM;
      goto fail;
  }

  p = &amp;eth_ports[port];

  if (p-&gt;init) {
      eth_irq_disable(port);

      /* Release IRQ handler */
      adp_os_irq_free(p-&gt;irq, p);

      ETH_DBG("Port (%d) shutdown started.", port);
  }

  /* Dealloc Tx rings. Must be done prior to Tx rings as some Tx still pending */
  for (queue = 0; queue &lt; p-&gt;txq_num; queue++) {
       for (txp = 0; txp &lt; p-&gt;txp_num; txp++) {
            eth_txq_shutdown(p, queue, txp);
       }
  }

  /* Dealloc Rx rings */
  for (queue = 0; queue &lt; p-&gt;rxq_num; queue++) {
       eth_rxq_shutdown(p, queue);
  }

  /* Destroy Rx packet pool */
  if (p-&gt;pkt_pool != NULL) {
      eth_pkt_pool_destroy(p);
  }

  is_init = p-&gt;init;
  p-&gt;init = false;

  if (is_init) {
      ETH_DBG("Port (%d) shutdown Ok.", port);
  }

  return ADP_OK;

fail:
  return err;
}



/* Unmask port Rx interrupts */
void eth_irq_enable(uint8_t port) {
  ADP_REG_WRITE(NETA_INTR_NEW_MASK_REG(port), MV_ETH_RX_INTR_MASK);
  ETH_DBG("Port (%d) interrupts enabled", port);
}

/* Unmask all active ports Rx interrupts */
void eth_irq_enable_all(void) {
  uint8_t port;

  for (port = 0; port &lt; ADP_ETH_MAX_PORT_NUM; port++) {
       if (eth_ports[port].init) {
           eth_irq_enable(port);
       }
  }
}

/* Mask port Rx interrupts */
void eth_irq_disable(uint8_t port) {
  ADP_REG_WRITE(NETA_INTR_NEW_MASK_REG(port), 0);
  ETH_DBG("Port (%d) interrupts disabled", port);
}


/* Mask all active ports Rx interrupts */
void eth_irq_disable_all(void) {
  uint8_t port;

  for (port = 0; port &lt; ADP_ETH_MAX_PORT_NUM; port++) {
       if (eth_ports[port].init) {
           eth_irq_disable(port);
       }
  }
}


</Insert>
</MostRecent>
<Delta Version="0" Comment="" NL="\10" Encoding="text" Date="2013/07/07" Time="13:14:19000">
<Copy StartSeek="0" EndSeek="36386"/>
<Copy StartSeek="36436" EndSeek="40378"/>
</Delta>
<Delta Version="1" Comment="" NL="\10" Encoding="text" Date="2013/07/07" Time="13:18:36000">
<Copy StartSeek="0" EndSeek="36386"/>
<Insert>  printk("&lt;0&gt;""rx_desc %u\n", cfg-&gt;rx_desc_num);
</Insert>
<Copy StartSeek="36444" EndSeek="40387"/>
</Delta>
<Delta Version="2" Comment="" NL="\10" Encoding="text" Date="2013/07/07" Time="13:18:44000">
<Copy StartSeek="0" EndSeek="3776"/>
<Insert>
</Insert>
<Copy StartSeek="3817" EndSeek="40427"/>
</Delta>
<Delta Version="3" Comment="" NL="\10" Encoding="text" Date="2013/07/07" Time="13:19:30000">
<Copy StartSeek="0" EndSeek="2147"/>
<Insert>
</Insert>
<Copy StartSeek="2239" EndSeek="2437"/>
<Copy StartSeek="2463" EndSeek="2786"/>
<Copy StartSeek="2837" EndSeek="3213"/>
<Insert>  eth_ring_t      *txq[ETH_MAX_TXQ_NUM]; /* Tx packet buffer rings */
</Insert>
<Copy StartSeek="3301" EndSeek="3624"/>
<Copy StartSeek="3701" EndSeek="4039"/>
<Insert>extern int printk(const char *fmt, ...);
</Insert>
<Copy StartSeek="4040" EndSeek="6772"/>
<Insert>static inline void eth_neta_txq_enable(uint8_t port, uint8_t queue, bool enable) {
</Insert>
<Copy StartSeek="6868" EndSeek="6971"/>
<Insert>  ADP_REG_WRITE(ETH_TX_QUEUE_COMMAND_REG(port, 0), q_map);
</Insert>
<Copy StartSeek="7032" EndSeek="9942"/>
<Copy StartSeek="10249" EndSeek="16588"/>
<Insert>static void eth_txq_shutdown(eth_port_t *p, uint8_t queue) {
</Insert>
<Copy StartSeek="16662" EndSeek="16681"/>
<Insert>  uint32_t tx_pending, elapsed;
</Insert>
<Copy StartSeek="16716" EndSeek="16859"/>
<Insert>  txq = p-&gt;txq[queue];

</Insert>
<Copy StartSeek="17050" EndSeek="17228"/>
<Insert>           (tx_pending = eth_neta_txq_get_pending_desc_num(txq-&gt;port, 0 /* txp */, txq-&gt;queue))) {
</Insert>
<Copy StartSeek="17319" EndSeek="17589"/>
<Insert>    eth_neta_txq_enable(txq-&gt;port, txq-&gt;queue, false);
</Insert>
<Copy StartSeek="17649" EndSeek="17686"/>
<Insert>    eth_neta_txq_update_sent_desc(txq-&gt;port, 0 /* txp */, txq-&gt;queue);
</Insert>
<Copy StartSeek="17749" EndSeek="17782"/>
<Insert>    ADP_REG_WRITE(NETA_TXQ_TOKEN_CNTR_REG(txq-&gt;port, 0 /* txp */, txq-&gt;queue), 0);
</Insert>
<Copy StartSeek="17857" EndSeek="17921"/>
<Insert>    ADP_REG_WRITE(NETA_TXQ_BASE_ADDR_REG(txq-&gt;port, 0 /* txp */, txq-&gt;queue), 0);
    ADP_REG_WRITE(NETA_TXQ_SIZE_REG(txq-&gt;port, 0 /* txp */, txq-&gt;queue), 0);
</Insert>
<Copy StartSeek="18064" EndSeek="18142"/>
<Insert>  p-&gt;txq[queue] = NULL;
</Insert>
<Copy StartSeek="18162" EndSeek="18166"/>
<Insert>static uint32_t eth_txq_init(eth_port_t *p, uint8_t queue) {
</Insert>
<Copy StartSeek="18240" EndSeek="18497"/>
<Insert>  if (p-&gt;txq[queue] != NULL) {
</Insert>
<Copy StartSeek="18714" EndSeek="19100"/>
<Insert>  p-&gt;txq[queue] = txq;
</Insert>
<Copy StartSeek="19119" EndSeek="19726"/>
<Insert>  txq-&gt;next = txq-&gt;first + NETA_DESC_ALIGNED_SIZE * eth_neta_get_next_tx_desc_num(p-&gt;port, 0, queue);
</Insert>
<Copy StartSeek="19830" EndSeek="20039"/>
<Insert>          eth_neta_get_next_tx_desc_num(p-&gt;port, 0, queue), queue, p-&gt;port);
</Insert>
<Copy StartSeek="20118" EndSeek="20145"/>
<Insert>  ADP_REG_WRITE(NETA_TXQ_TOKEN_CNTR_REG(p-&gt;port, 0 /* txp */, queue), NETA_TXQ_TOKEN_CNTR_MAX);
</Insert>
<Copy StartSeek="20233" EndSeek="20365"/>
<Insert>  ADP_REG_WRITE(NETA_TXQ_BASE_ADDR_REG(p-&gt;port, 0 /* txp */, queue), dma);
</Insert>
<Copy StartSeek="20432" EndSeek="20434"/>
<Insert>  ADP_REG_WRITE(NETA_TXQ_SIZE_REG(p-&gt;port, 0 /* txp */, queue), NETA_TXQ_DESC_NUM_MASK(p-&gt;tx_desc_num));
</Insert>
<Copy StartSeek="20531" EndSeek="20560"/>
<Insert>  eth_neta_txq_enable(txq-&gt;port, txq-&gt;queue, true);
</Insert>
<Copy StartSeek="20617" EndSeek="20642"/>
<Insert>  eth_txq_shutdown(p, queue);
</Insert>
<Copy StartSeek="20677" EndSeek="21207"/>
<Copy StartSeek="21235" EndSeek="21388"/>
<Insert>
</Insert>
<Copy StartSeek="22598" EndSeek="24765"/>
<Insert>  } else if (txp &gt; ETH_MAX_TCONT) {
</Insert>
<Copy StartSeek="24803" EndSeek="32048"/>
<Insert>      eth_tx(eth2);
</Insert>
<Copy StartSeek="32072" EndSeek="33239"/>
<Copy StartSeek="33280" EndSeek="33325"/>
<Insert>  register eth_ring_t    *txq = p-&gt;txq[queue];
</Insert>
<Copy StartSeek="33392" EndSeek="33557"/>
<Insert>  tx_desc-&gt;command = pkt-&gt;hw_cmd;
</Insert>
<Copy StartSeek="33745" EndSeek="37807"/>
<Insert>  uint32_t err, queue;
</Insert>
<Copy StartSeek="37835" EndSeek="38551"/>
<Copy StartSeek="38727" EndSeek="38939"/>
<Insert>  printk("&lt;0&gt;""eth_init:rx_desc %u\n", cfg-&gt;rx_desc_num);

</Insert>
<Copy StartSeek="38939" EndSeek="39922"/>
<Copy StartSeek="39951" EndSeek="40047"/>
<Copy StartSeek="40069" EndSeek="40792"/>
<Insert>      err = eth_txq_init(p, queue);
      if (err) {
          goto fail;
      }
  }

</Insert>
<Copy StartSeek="40965" EndSeek="41276"/>
<Insert>  uint32_t err, queue;
</Insert>
<Copy StartSeek="41304" EndSeek="41794"/>
<Insert>       eth_txq_shutdown(p, queue);
  }

</Insert>
<Copy StartSeek="41901" EndSeek="43090"/>
</Delta>
<Delta Version="4" Comment="" NL="\10" Encoding="text" Date="2013/07/09" Time="18:26:41000">
<Copy StartSeek="0" EndSeek="21785"/>
<Insert>       printk(KERN_ERR "(%s:%d) txp_pending %#x\n", __func__, __LINE__, txp_pending);
</Insert>
<Copy StartSeek="21870" EndSeek="43089"/>
</Delta>
<Delta Version="5" Comment="" NL="\10" Encoding="text" Date="2013/07/09" Time="18:26:44000">
<Copy StartSeek="0" EndSeek="21785"/>
<Insert>       printk(KERN_ERR "(%s:%d) txp_pending %#x\n", __func__, __LINE__,txp_pending);
</Insert>
<Copy StartSeek="21871" EndSeek="43090"/>
</Delta>
</DeltaFile>
