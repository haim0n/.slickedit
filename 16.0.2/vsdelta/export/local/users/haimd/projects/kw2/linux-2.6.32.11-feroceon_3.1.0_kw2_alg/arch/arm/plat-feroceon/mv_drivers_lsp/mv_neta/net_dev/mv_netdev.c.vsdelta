<!DOCTYPE DeltaFile SYSTEM "http://www.slickedit.com/dtd/vse/vsdelta/9.0/vsdelta.dtd">
<DeltaFile FormatVersion="9.0.0">
<MostRecent Version="220" Comment="" Date="2012/08/23" Time="01:35:28000" NL="\10" Encoding="text">
<Insert>/*******************************************************************************
Copyright (C) Marvell International Ltd. and its affiliates

This software file (the "File") is owned and distributed by Marvell
International Ltd. and/or its affiliates ("Marvell") under the following
alternative licensing terms.  Once you have made an election to distribute the
File under one of the following license alternatives, please (i) delete this
introductory statement regarding license alternatives, (ii) delete the two
license alternatives that you have not elected to use and (iii) preserve the
Marvell copyright notice above.

********************************************************************************
Marvell GPL License Option

If you received this File from Marvell, you may opt to use, redistribute and/or
modify this File in accordance with the terms and conditions of the General
Public License Version 2, June 1991 (the "GPL License"), a copy of which is
available along with the File in the license.txt file or by writing to the Free
Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 or
on the worldwide web at http://www.gnu.org/licenses/gpl.txt.

THE FILE IS DISTRIBUTED AS-IS, WITHOUT WARRANTY OF ANY KIND, AND THE IMPLIED
WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
DISCLAIMED.  The GPL License provides additional details about this warranty
disclaimer.
*******************************************************************************/

#include "mvCommon.h"
#include &lt;linux/kernel.h&gt;
#include &lt;linux/version.h&gt;
#include &lt;linux/netdevice.h&gt;
#include &lt;linux/etherdevice.h&gt;
#include &lt;linux/platform_device.h&gt;
#include &lt;linux/skbuff.h&gt;
#include &lt;linux/inetdevice.h&gt;
#include &lt;linux/mv_neta.h&gt;
#include &lt;net/ip.h&gt;
#include &lt;net/ipv6.h&gt;

#include "mvOs.h"
#include "mvDebug.h"
#include "dbg-trace.h"
#include "mvSysHwConfig.h"
#include "boardEnv/mvBoardEnvLib.h"
#include "ctrlEnv/mvCtrlEnvLib.h"
#include "eth-phy/mvEthPhy.h"
#include "mvSysEthPhyApi.h"
#include "mvSysNetaApi.h"

#include "gbe/mvNeta.h"
#include "bm/mvBm.h"
#include "pnc/mvPnc.h"
#include "pnc/mvTcam.h"
#include "pmt/mvPmt.h"

#include "mv_switch.h"

#include "mv_netdev.h"
#include "mv_eth_tool.h"

#include "cpu/mvCpuCntrs.h"
/* DPA_HACK START */
enum dpa_entry_states {
	DPE_NEW = 0,
	DPE_LRN,
	DPE_FWD,
	DPE_LCL,
};

struct dpa_pri_key {
	uint32_t sip;
	uint32_t dip;
	/* HAIM FIXME : add IPV6 */
	uint16_t sport;
	uint16_t dport;
	uint32_t protocol; /* NOTE: u32 is for padding */
};

struct dpa_l2 {
	uint8_t dmac[6];
	uint8_t smac[6];
	uint16_t eth_type;
	uint16_t vlan_tag;
	uint16_t pppoe_sid;
};

struct dpa_counters {
	uint32_t rx_hits;
	uint32_t fwd_hits;
	uint32_t stable;
};

struct dpa_entry {
	struct {
		struct dpa_pri_key cls_tuple;
		struct dpa_l2 l2;
	} org;
	struct dpa_entry *next;
	uint32_t state;
	struct {
		struct dpa_pri_key mod_tuple;
		struct dpa_l2 l2;
		uint32_t tx_port;
	} mod;
	struct dpa_counters counters;
	uint32_t mod_ops;
};

enum dpa_mod_operations {
	DPE_MOD_L2 = 1,
	DPE_MOD_NAT = 2,
};

/* DPA_HACK_END */
#ifdef CONFIG_MV_CPU_PERF_CNTRS
MV_CPU_CNTRS_EVENT	*event0 = NULL;
MV_CPU_CNTRS_EVENT	*event1 = NULL;
MV_CPU_CNTRS_EVENT	*event2 = NULL;
MV_CPU_CNTRS_EVENT	*event3 = NULL;
MV_CPU_CNTRS_EVENT	*event4 = NULL;
MV_CPU_CNTRS_EVENT	*event5 = NULL;
#endif /* CONFIG_MV_CPU_PERF_CNTRS */

unsigned int ext_switch_port_mask = 0;

void handle_group_affinity(int port);
void set_rxq_affinity(struct eth_port *pp, MV_U32 rxqAffinity, int group);


/* uncomment if you want to debug the SKB recycle feature */
/* #define ETH_SKB_DEBUG */

#ifdef CONFIG_MV_ETH_PNC
unsigned int mv_eth_pnc_ctrl_en = 1;

int mv_eth_ctrl_pnc(int en)
{
	mv_eth_pnc_ctrl_en = en;
	return 0;
}
#endif /* CONFIG_MV_ETH_PNC */

#ifdef CONFIG_MV_ETH_NFP
extern int nfp_sysfs_init(void);
#endif /* CONFIG_MV_ETH_NFP */

#ifdef CONFIG_NET_SKB_RECYCLE
int mv_ctrl_recycle = CONFIG_NET_SKB_RECYCLE_DEF;

int mv_eth_ctrl_recycle(int en)
{
	mv_ctrl_recycle = en;
	return 0;
}
#else
int mv_eth_ctrl_recycle(int en)
{
	printk(KERN_ERR "SKB recycle is not supported\n");
	return 1;
}
#endif /* CONFIG_NET_SKB_RECYCLE */

extern u8 mvMacAddr[CONFIG_MV_ETH_PORTS_NUM][MV_MAC_ADDR_SIZE];
extern u16 mvMtu[CONFIG_MV_ETH_PORTS_NUM];

extern unsigned int switch_enabled_ports;

struct bm_pool mv_eth_pool[MV_ETH_BM_POOLS];
struct eth_port **mv_eth_ports;
struct net_device **mv_net_devs;

int mv_net_devs_num = 0;
int mv_ctrl_txdone = CONFIG_MV_ETH_TXDONE_COAL_PKTS;
/*
 * Static declarations
 */
static int mv_eth_ports_num = 0;
static int mv_net_devs_max = 0;

static int mv_eth_initialized = 0;

/* DPA hack start */
uint32_t (*dpa_rx_hook)(struct neta_rx_desc *rx_desc, uint8_t *pkt,
			    void **dpa_cookie);
uint32_t (*dpa_os_tx_hook)(uint8_t *pkt, void *dpa_cookie, struct iphdr *iph, 
			   void *tx_port);
/* DPA hack end */
/*
 * Local functions
 */
static void mv_eth_txq_delete(struct eth_port *pp, struct tx_queue *txq_ctrl);
static void mv_eth_tx_timeout(struct net_device *dev);
static int mv_eth_tx(struct sk_buff *skb, struct net_device *dev);
static void mv_eth_tx_frag_process(struct eth_port *pp, struct sk_buff *skb, struct tx_queue *txq_ctrl, u16 flags);

static void mv_eth_config_show(void);
static int  mv_eth_priv_init(struct eth_port *pp, int port);
static void mv_eth_priv_cleanup(struct eth_port *pp);
static int  mv_eth_config_get(struct eth_port *pp, u8 *mac);
static int  mv_eth_hal_init(struct eth_port *pp);
struct net_device *mv_eth_netdev_init(struct eth_port *pp, int mtu, u8 *mac);
static void mv_eth_netdev_set_features(struct net_device *dev);
static void mv_eth_netdev_update_features(struct net_device *dev);

static MV_STATUS mv_eth_pool_create(int pool, int capacity);
static int mv_eth_pool_add(int pool, int buf_num);
static int mv_eth_pool_free(int pool, int num);
static int mv_eth_pool_destroy(int pool);

#ifdef CONFIG_MV_ETH_TSO
int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev, struct mv_eth_tx_spec *tx_spec,
		struct tx_queue *txq_ctrl);
#endif

/* Get the configuration string from the Kernel Command Line */
static char *port0_config_str = NULL, *port1_config_str = NULL, *port2_config_str = NULL, *port3_config_str = NULL;
int mv_eth_cmdline_port0_config(char *s);
__setup("mv_port0_config=", mv_eth_cmdline_port0_config);
int mv_eth_cmdline_port1_config(char *s);
__setup("mv_port1_config=", mv_eth_cmdline_port1_config);
int mv_eth_cmdline_port2_config(char *s);
__setup("mv_port2_config=", mv_eth_cmdline_port2_config);
int mv_eth_cmdline_port3_config(char *s);
__setup("mv_port3_config=", mv_eth_cmdline_port3_config);

int mv_eth_cmdline_port0_config(char *s)
{
	port0_config_str = s;
	return 1;
}

int mv_eth_cmdline_port1_config(char *s)
{
	port1_config_str = s;
	return 1;
}

int mv_eth_cmdline_port2_config(char *s)
{
	port2_config_str = s;
	return 1;
}

int mv_eth_cmdline_port3_config(char *s)
{
	port3_config_str = s;
	return 1;
}

void set_cpu_affinity(struct eth_port *pp, MV_U32 cpuAffinity, int group)
{
	int cpu;
	MV_U32 rxqAffinity = 0;

	/* nothing to do when cpuAffinity == 0 */
	if (cpuAffinity == 0)
		return;

	/* First, read affinity of the target group, in case it contains CPUs */
	for (cpu = 0; cpu &lt; CONFIG_NR_CPUS; cpu++) {
		if (!(MV_BIT_CHECK(pp-&gt;cpuMask, cpu)))
			continue;
		if (pp-&gt;napiCpuGroup[cpu] == group) {
			rxqAffinity = MV_REG_READ(NETA_CPU_MAP_REG(pp-&gt;port, cpu)) &amp; 0xff;
			break;
		}
	}
	for (cpu = 0; cpu &lt; CONFIG_NR_CPUS; cpu++) {
		if (cpuAffinity &amp; 1) {
			pp-&gt;napi[cpu] = pp-&gt;napiGroup[group];
			pp-&gt;napiCpuGroup[cpu] = group;
			/* set rxq affinity of the target group */
			MV_REG_WRITE(NETA_CPU_MAP_REG(pp-&gt;port, cpu), rxqAffinity | NETA_CPU_TXQ_ACCESS_ALL_MASK);
		}
		cpuAffinity &gt;&gt;= 1;
	}
}

int group_has_cpus(struct eth_port *pp, int group)
{
	int cpu;

	for (cpu = 0; cpu &lt; CONFIG_NR_CPUS; cpu++) {
		if (!(MV_BIT_CHECK(pp-&gt;cpuMask, cpu)))
			continue;
		if (pp-&gt;napiCpuGroup[cpu] == group)
			return 1;
	}

	/* the group contains no CPU */
	return 0;
}

void set_rxq_affinity(struct eth_port *pp, MV_U32 rxqAffinity, int group)
{
	int rxq, cpu;
	MV_U32 regVal;
	MV_U32 tmpRxqAffinity;
	int groupHasCpus;
	int cpuInGroup;

	/* nothing to do when rxqAffinity == 0 */
	if (rxqAffinity == 0)
		return;

	groupHasCpus = group_has_cpus(pp, group);

	if (!groupHasCpus) {
		printk(KERN_ERR "%s: operation not performed; group %d has no cpu \n", __func__, group);
		return;
	}

   for (cpu = 0; cpu &lt; CONFIG_NR_CPUS; cpu++) {
		if (!(MV_BIT_CHECK(pp-&gt;cpuMask, cpu)))
			continue;
	   tmpRxqAffinity = rxqAffinity;

	   regVal = MV_REG_READ(NETA_CPU_MAP_REG(pp-&gt;port, cpu));

	   if (pp-&gt;napiCpuGroup[cpu] == group) {
		   cpuInGroup = 1;
		   /* init TXQ Access Enable bits */
		   regVal = regVal &amp; 0xff00;
	   } else {
		   cpuInGroup = 0;
		}

	   for (rxq = 0; rxq &lt; CONFIG_MV_ETH_RXQ; rxq++) {
		   /* set rxq affinity for this cpu */
		   if (tmpRxqAffinity &amp; 1) {
			   if (cpuInGroup)
				   regVal |= NETA_CPU_RXQ_ACCESS_MASK(rxq);
			   else
				   regVal &amp;= ~NETA_CPU_RXQ_ACCESS_MASK(rxq);
			}
			tmpRxqAffinity &gt;&gt;= 1;
	   }
	   MV_REG_WRITE(NETA_CPU_MAP_REG(pp-&gt;port, cpu), regVal);
   }
}

static int mv_eth_port_config_parse(struct eth_port *pp)
{
	char *str;

	printk(KERN_ERR "\n");
	if (pp == NULL) {
		printk(KERN_ERR "  o mv_eth_port_config_parse: got NULL pp\n");
		return -1;
	}

	switch (pp-&gt;port) {
	case 0:
		str = port0_config_str;
		break;
	case 1:
		str = port1_config_str;
		break;
	case 2:
		str = port2_config_str;
		break;
	case 3:
		str = port3_config_str;
		break;
	default:
		printk(KERN_ERR "  o mv_eth_port_config_parse: got unknown port %d\n", pp-&gt;port);
		return -1;
	}

	if (str != NULL) {
		if ((!strcmp(str, "disconnected")) || (!strcmp(str, "Disconnected"))) {
			printk(KERN_ERR "  o Port %d is disconnected from Linux netdevice\n", pp-&gt;port);
			clear_bit(MV_ETH_F_CONNECT_LINUX_BIT, &amp;(pp-&gt;flags));
			return 0;
		}
	}

	printk(KERN_ERR "  o Port %d is connected to Linux netdevice\n", pp-&gt;port);
	set_bit(MV_ETH_F_CONNECT_LINUX_BIT, &amp;(pp-&gt;flags));
	return 0;
}

#ifdef ETH_SKB_DEBUG
struct sk_buff *mv_eth_skb_debug[MV_BM_POOL_CAP_MAX * MV_ETH_BM_POOLS];
static spinlock_t skb_debug_lock;

void mv_eth_skb_check(struct sk_buff *skb)
{
	int i;
	struct sk_buff *temp;
	unsigned long flags;

	if (skb == NULL)
		printk(KERN_ERR "mv_eth_skb_check: got NULL SKB\n");

	spin_lock_irqsave(&amp;skb_debug_lock, flags);

	i = *((u32 *)&amp;skb-&gt;cb[0]);

	if ((i &gt;= 0) &amp;&amp; (i &lt; MV_BM_POOL_CAP_MAX * MV_ETH_BM_POOLS)) {
		temp = mv_eth_skb_debug[i];
		if (mv_eth_skb_debug[i] != skb) {
			printk(KERN_ERR "mv_eth_skb_check: Unexpected skb: %p (%d) != %p (%d)\n",
			       skb, i, temp, *((u32 *)&amp;temp-&gt;cb[0]));
		}
		mv_eth_skb_debug[i] = NULL;
	} else {
		printk(KERN_ERR "mv_eth_skb_check: skb-&gt;cb=%d is out of range\n", i);
	}

	spin_unlock_irqrestore(&amp;skb_debug_lock, flags);
}

void mv_eth_skb_save(struct sk_buff *skb, const char *s)
{
	int i;
	int saved = 0;
	unsigned long flags;

	spin_lock_irqsave(&amp;skb_debug_lock, flags);

	for (i = 0; i &lt; MV_BM_POOL_CAP_MAX * MV_ETH_BM_POOLS; i++) {
		if (mv_eth_skb_debug[i] == skb) {
			printk(KERN_ERR "%s: mv_eth_skb_debug Duplicate: i=%d, skb=%p\n", s, i, skb);
			mv_eth_skb_print(skb);
		}

		if ((!saved) &amp;&amp; (mv_eth_skb_debug[i] == NULL)) {
			mv_eth_skb_debug[i] = skb;
			*((u32 *)&amp;skb-&gt;cb[0]) = i;
			saved = 1;
		}
	}

	spin_unlock_irqrestore(&amp;skb_debug_lock, flags);

	if ((i == MV_BM_POOL_CAP_MAX * MV_ETH_BM_POOLS) &amp;&amp; (!saved))
		printk(KERN_ERR "mv_eth_skb_debug is FULL, skb=%p\n", skb);
}
#endif /* ETH_SKB_DEBUG */

struct eth_port *mv_eth_port_by_id(unsigned int port)
{
	if (port &lt; mv_eth_ports_num)
		return mv_eth_ports[port];

	return NULL;
}

struct net_device *mv_eth_netdev_by_id(unsigned int idx)
{
	if (idx &lt; mv_net_devs_num)
		return mv_net_devs[idx];

	return NULL;
}

static inline int mv_eth_skb_mh_add(struct sk_buff *skb, u16 mh)
{
	/* sanity: Check that there is place for MH in the buffer */
	if (skb_headroom(skb) &lt; MV_ETH_MH_SIZE) {
		printk(KERN_ERR "%s: skb (%p) doesn't have place for MH, head=%p, data=%p\n",
		       __func__, skb, skb-&gt;head, skb-&gt;data);
		return 1;
	}

	/* Prepare place for MH header */
	skb-&gt;len += MV_ETH_MH_SIZE;
	skb-&gt;data -= MV_ETH_MH_SIZE;
	*((u16 *) skb-&gt;data) = mh;

	return 0;
}

void mv_eth_ctrl_txdone(int num)
{
	mv_ctrl_txdone = num;
}

int mv_eth_ctrl_flag(int port, u32 flag, u32 val)
{
	struct eth_port *pp = mv_eth_port_by_id(port);
	u32 bit_flag = (fls(flag) - 1);

	if (!pp)
		return -ENODEV;

	if ((flag == MV_ETH_F_MH) &amp;&amp; (pp-&gt;flags &amp; MV_ETH_F_SWITCH)) {
		printk(KERN_ERR "Error: cannot change Marvell Header on a port used by the Gateway driver\n");
		return -EPERM;
	}

	if (val)
		set_bit(bit_flag, &amp;(pp-&gt;flags));
	else
		clear_bit(bit_flag, &amp;(pp-&gt;flags));

	if (flag == MV_ETH_F_MH)
		mvNetaMhSet(pp-&gt;port, val ? MV_NETA_MH : MV_NETA_MH_NONE);

	return 0;
}

int mv_eth_ctrl_port_buf_num_set(int port, int long_num, int short_num)
{
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "Port %d must be stopped before\n", port);
		return -EINVAL;
	}
	if (pp-&gt;pool_long != NULL) {
		/* Update number of buffers in existing pool (allocate or free) */
		if (pp-&gt;pool_long_num &gt; long_num)
			mv_eth_pool_free(pp-&gt;pool_long-&gt;pool, pp-&gt;pool_long_num - long_num);
		else if (long_num &gt; pp-&gt;pool_long_num)
			mv_eth_pool_add(pp-&gt;pool_long-&gt;pool, long_num - pp-&gt;pool_long_num);
	}
	pp-&gt;pool_long_num = long_num;

#ifdef CONFIG_MV_ETH_BM_CPU
	if (pp-&gt;pool_short != NULL) {
		/* Update number of buffers in existing pool (allocate or free) */
		if (pp-&gt;pool_short_num &gt; short_num)
			mv_eth_pool_free(pp-&gt;pool_short-&gt;pool, pp-&gt;pool_short_num - short_num);
		else if (short_num &gt; pp-&gt;pool_short_num)
			mv_eth_pool_add(pp-&gt;pool_short-&gt;pool, short_num - pp-&gt;pool_short_num);
	}
	pp-&gt;pool_short_num = short_num;
#endif /* CONFIG_MV_ETH_BM_CPU */

	return 0;
}

#ifdef CONFIG_MV_ETH_BM
/* Set pkt_size for the pool. Check that pool not in use (all ports are stopped) */
/* Free all buffers from the pool */
/* Detach the pool from all ports */
int mv_eth_ctrl_pool_size_set(int pool, int pkt_size)
{
#ifdef CONFIG_MV_ETH_BM_CPU
	int port;
	struct bm_pool *ppool;
	struct eth_port *pp;

	if (mvNetaMaxCheck(pool, MV_ETH_BM_POOLS))
		return -EINVAL;

	ppool = &amp;mv_eth_pool[pool];

	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		/* Check that all ports using this pool are stopped */
		if (ppool-&gt;port_map &amp; (1 &lt;&lt; port)) {
			pp = mv_eth_port_by_id(port);

			if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
				printk(KERN_ERR "Port %d use pool #%d and must be stopped before change pkt_size\n",
					port, pool);
				return -EINVAL;
			}
		}
	}
	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		/* Free all buffers and detach pool */
		if (ppool-&gt;port_map &amp; (1 &lt;&lt; port)) {
			pp = mv_eth_port_by_id(port);

			if (ppool == pp-&gt;pool_long) {
				mv_eth_pool_free(pool, pp-&gt;pool_long_num);
				ppool-&gt;port_map &amp;= ~(1 &lt;&lt; pp-&gt;port);
				pp-&gt;pool_long = NULL;
			}
			if (ppool == pp-&gt;pool_short) {
				mv_eth_pool_free(pool, pp-&gt;pool_short_num);
				ppool-&gt;port_map &amp;= ~(1 &lt;&lt; pp-&gt;port);
				pp-&gt;pool_short = NULL;
			}
		}
	}
	ppool-&gt;pkt_size = pkt_size;
#endif /* CONFIG_MV_ETH_BM_CPU */

	mv_eth_bm_config_pkt_size_set(pool, pkt_size);
	if (pkt_size == 0)
		mvBmPoolBufSizeSet(pool, 0);
	else
		mvBmPoolBufSizeSet(pool, RX_BUF_SIZE(pkt_size));

	return 0;
}
#endif /* CONFIG_MV_ETH_BM */

int mv_eth_ctrl_set_poll_rx_weight(int port, u32 weight)
{
	struct eth_port *pp = mv_eth_port_by_id(port);
	int cpu;

	if (pp == NULL) {
		printk(KERN_INFO "port doens not exist (%d) in %s\n" , port, __func__);
		return -EINVAL;
	}

	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "Port %d must be stopped before\n", port);
		return -EINVAL;
	}

	if (weight &gt; 255)
		weight = 255;
	pp-&gt;weight = weight;

	for_each_possible_cpu(cpu) {
		if (pp-&gt;napi[cpu])
			pp-&gt;napi[cpu]-&gt;weight = pp-&gt;weight;
	}

	return 0;
}

int mv_eth_ctrl_rxq_size_set(int port, int rxq, int value)
{
	struct eth_port *pp = mv_eth_port_by_id(port);
	struct rx_queue	*rxq_ctrl;

	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "Port %d must be stopped before\n", port);
		return -EINVAL;
	}
	rxq_ctrl = &amp;pp-&gt;rxq_ctrl[rxq];
	if ((rxq_ctrl-&gt;q) &amp;&amp; (rxq_ctrl-&gt;rxq_size != value)) {
		/* Reset is required when RXQ ring size is changed */
		mv_eth_rx_reset(pp-&gt;port);

		mvNetaRxqDelete(pp-&gt;port, rxq);
		rxq_ctrl-&gt;q = NULL;
	}
	pp-&gt;rxq_ctrl[rxq].rxq_size = value;

	/* New RXQ will be created during mv_eth_start_internals */
	return 0;
}

int mv_eth_ctrl_txq_size_set(int port, int txp, int txq, int value)
{
	struct tx_queue *txq_ctrl;
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "Port %d must be stopped before\n", port);
		return -EINVAL;
	}
	txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
	if ((txq_ctrl-&gt;q) &amp;&amp; (txq_ctrl-&gt;txq_size != value)) {
		mv_eth_txq_delete(pp, txq_ctrl);
		/* Reset of port/txp is required when TXQ ring size is changed */
		/* Reset done before as part of stop_internals function */
	}
	txq_ctrl-&gt;txq_size = value;

	/* New TXQ will be created during mv_eth_start_internals */
	return 0;
}

int mv_eth_ctrl_txq_mode_get(int port, int txp, int txq, int *value)
{
	int mode = MV_ETH_TXQ_FREE, val = 0;
	struct tx_queue *txq_ctrl;
	struct eth_port *pp = mv_eth_port_by_id(port);

	txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
	if (txq_ctrl-&gt;cpu_owner) {
		mode = MV_ETH_TXQ_CPU;
		val = txq_ctrl-&gt;cpu_owner;
	} else if (txq_ctrl-&gt;hwf_rxp &lt; (MV_U8) mv_eth_ports_num) {
		mode = MV_ETH_TXQ_HWF;
		val = txq_ctrl-&gt;hwf_rxp;
	}
	if (value)
		*value = val;

	return mode;
}

/* Increment/Decrement CPU ownership for this TXQ */
int mv_eth_ctrl_txq_cpu_own(int port, int txp, int txq, int add)
{
	int mode;
	struct tx_queue *txq_ctrl;
	struct eth_port *pp = mv_eth_port_by_id(port);

	if ((pp == NULL) || (pp-&gt;txq_ctrl == NULL))
		return -ENODEV;

	/* Check that new txp/txq can be allocated for CPU */
	mode = mv_eth_ctrl_txq_mode_get(port, txp, txq, NULL);

	txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
	if (add) {
		if ((mode != MV_ETH_TXQ_CPU) &amp;&amp; (mode != MV_ETH_TXQ_FREE))
			return -EINVAL;

		txq_ctrl-&gt;cpu_owner++;
	} else {
		if (mode != MV_ETH_TXQ_CPU)
			return -EINVAL;

		txq_ctrl-&gt;cpu_owner--;
	}
	return 0;
}

/* Set TXQ ownership to HWF from the RX port.  rxp=-1 - free TXQ ownership */
int mv_eth_ctrl_txq_hwf_own(int port, int txp, int txq, int rxp)
{
	int mode;
	struct tx_queue *txq_ctrl;
	struct eth_port *pp = mv_eth_port_by_id(port);

	if ((pp == NULL) || (pp-&gt;txq_ctrl == NULL))
		return -ENODEV;

	/* Check that new txp/txq can be allocated for HWF */
	mode = mv_eth_ctrl_txq_mode_get(port, txp, txq, NULL);

	txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];

	if (rxp == -1) {
		if (mode != MV_ETH_TXQ_HWF)
			return -EINVAL;
	} else {
		if ((mode != MV_ETH_TXQ_HWF) &amp;&amp; (mode != MV_ETH_TXQ_FREE))
			return -EINVAL;
	}

	txq_ctrl-&gt;hwf_rxp = (MV_U8) rxp;

	return 0;
}

/* Set TXQ for CPU originated packets */
int mv_eth_ctrl_txq_cpu_def(int port, int txp, int txq, int cpu)
{
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (cpu &gt;= CONFIG_NR_CPUS) {
		printk(KERN_ERR "cpu #%d is out of range: from 0 to %d\n",
			cpu, CONFIG_NR_CPUS - 1);
		return -EINVAL;
	}

	if (mvNetaTxpCheck(port, txp))
		return -EINVAL;

	if ((pp == NULL) || (pp-&gt;txq_ctrl == NULL))
		return -ENODEV;

	/* Decrement CPU ownership for old txq */
	mv_eth_ctrl_txq_cpu_own(port, pp-&gt;txp, pp-&gt;txq[cpu], 0);

	if (txq != -1) {
		if (mvNetaMaxCheck(txq, CONFIG_MV_ETH_TXQ))
			return -EINVAL;

		/* Increment CPU ownership for new txq */
		if (mv_eth_ctrl_txq_cpu_own(port, txp, txq, 1))
			return -EINVAL;
	}
	pp-&gt;txp = txp;
	pp-&gt;txq[cpu] = txq;

	return 0;
}

int mv_eth_ctrl_tx_cmd(int port, u32 tx_cmd)
{
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (!pp)
		return -ENODEV;

	pp-&gt;hw_cmd = tx_cmd;

	return 0;
}

int mv_eth_ctrl_tx_mh(int port, u16 mh)
{
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (!pp)
		return -ENODEV;

	pp-&gt;tx_mh = mh;

	return 0;
}

#ifdef CONFIG_MV_ETH_TX_SPECIAL
/* Register special transmit check function */
void mv_eth_tx_special_check_func(int port,
					int (*func)(int port, struct net_device *dev, struct sk_buff *skb,
								struct mv_eth_tx_spec *tx_spec_out))
{
	struct eth_port *pp = mv_eth_port_by_id(port);

	pp-&gt;tx_special_check = func;
}
#endif /* CONFIG_MV_ETH_TX_SPECIAL */

#ifdef CONFIG_MV_ETH_RX_SPECIAL
/* Register special transmit check function */
void mv_eth_rx_special_proc_func(int port, void (*func)(int port, int rxq, struct net_device *dev,
							struct sk_buff *skb, struct neta_rx_desc *rx_desc))
{
	struct eth_port *pp = mv_eth_port_by_id(port);

	pp-&gt;rx_special_proc = func;
}
#endif /* CONFIG_MV_ETH_RX_SPECIAL */

static const struct net_device_ops mv_eth_netdev_ops = {
	.ndo_open = mv_eth_open,
	.ndo_stop = mv_eth_stop,
	.ndo_start_xmit = mv_eth_tx,
	.ndo_set_multicast_list = mv_eth_set_multicast_list,
	.ndo_set_mac_address = mv_eth_set_mac_addr,
	.ndo_change_mtu = mv_eth_change_mtu,
	.ndo_tx_timeout = mv_eth_tx_timeout,
};

#ifdef CONFIG_MV_ETH_SWITCH

static const struct net_device_ops mv_switch_netdev_ops = {
	.ndo_open = mv_eth_switch_start,
	.ndo_stop = mv_eth_switch_stop,
	.ndo_start_xmit = mv_eth_tx,
	.ndo_set_multicast_list = mv_eth_switch_set_multicast_list,
	.ndo_set_mac_address = mv_eth_switch_set_mac_addr,
	.ndo_change_mtu = mv_eth_switch_change_mtu,
	.ndo_tx_timeout = mv_eth_tx_timeout,
};

int mv_eth_switch_netdev_first = 0;
int mv_eth_switch_netdev_last = 0;

static inline struct net_device *mv_eth_switch_netdev_get(struct eth_port *pp, struct eth_pbuf *pkt)
{
	MV_U8 *data;
	int db_num;

	if (pp-&gt;flags &amp; MV_ETH_F_SWITCH) {
		data = pkt-&gt;pBuf + pkt-&gt;offset;

		/* bits[4-7] of MSB in Marvell header */
		db_num = ((*data) &gt;&gt; 4);

		return mv_net_devs[mv_eth_switch_netdev_first + db_num];
	}
	return pp-&gt;dev;
}


void mv_eth_switch_priv_update(struct net_device *netdev, int i)
{
	struct eth_netdev *dev_priv;
	struct eth_port *pp = MV_ETH_PRIV(netdev);
	int print_flag, port, switch_port;

	/* Update dev_priv structure */
	dev_priv = MV_DEV_PRIV(netdev);
	dev_priv-&gt;port_map = 0;
	dev_priv-&gt;link_map = 0;

	print_flag = 1;
	for (port = 0; port &lt; BOARD_ETH_SWITCH_PORT_NUM; port++) {
		if (switch_net_config[pp-&gt;port].board_port_map[i] &amp; (1 &lt;&lt; port)) {
			if (print_flag) {
				printk(KERN_CONT ". Interface ports: ");
				print_flag = 0;
			}
			printk(KERN_CONT "%d ", port);
			switch_port = mvBoardSwitchPortGet(MV_SWITCH_ID_0, port);
			if (switch_port &gt;= 0) {
				dev_priv-&gt;port_map |= (1 &lt;&lt; switch_port);
				switch_enabled_ports |= (1 &lt;&lt; switch_port);
			}
		}
	}
	printk(KERN_CONT "\n");
	dev_priv-&gt;group = i;
	dev_priv-&gt;vlan_grp_id = MV_SWITCH_GROUP_VLAN_ID(i);	/* e.g. 0x100, 0x200... */
	dev_priv-&gt;tx_vlan_mh = cpu_to_be16((i &lt;&lt; 12) | dev_priv-&gt;port_map);
	dev_priv-&gt;cpu_port = mvBoardSwitchCpuPortGet(MV_SWITCH_ID_0);

	mv_eth_switch_vlan_set(dev_priv-&gt;vlan_grp_id, dev_priv-&gt;port_map, dev_priv-&gt;cpu_port);
}


int mv_eth_switch_netdev_init(struct eth_port *pp, int dev_i)
{
	int i;
	struct net_device *netdev;

	switch_enabled_ports = 0;

	for (i = 0; i &lt; switch_net_config[pp-&gt;port].netdev_max; i++) {
		netdev = mv_eth_netdev_init(pp, switch_net_config[pp-&gt;port].mtu, switch_net_config[pp-&gt;port].mac_addr[i]);
		if (netdev == NULL) {
			printk(KERN_ERR "mv_eth_switch_netdev_init: can't create netdevice\n");
			break;
		}
		mv_net_devs[dev_i++] = netdev;

		mv_eth_switch_priv_update(netdev, i);

	}
	return dev_i;
}

#endif /* CONFIG_MV_ETH_SWITCH */

void mv_eth_link_status_print(int port)
{
	MV_ETH_PORT_STATUS link;

	mvNetaLinkStatus(port, &amp;link);
#ifdef CONFIG_MV_PON
	if (MV_PON_PORT(port))
		link.linkup = mv_pon_link_status();
#endif /* CONFIG_MV_PON */

	if (link.linkup) {
		printk(KERN_CONT "link up");
		printk(KERN_CONT ", %s duplex", (link.duplex == MV_ETH_DUPLEX_FULL) ? "full" : "half");
		printk(KERN_CONT ", speed ");

		if (link.speed == MV_ETH_SPEED_1000)
			printk(KERN_CONT "1 Gbps\n");
		else if (link.speed == MV_ETH_SPEED_100)
			printk(KERN_CONT "100 Mbps\n");
		else
			printk(KERN_CONT "10 Mbps\n");
	} else
		printk(KERN_CONT "link down\n");
}

static void mv_eth_rx_error(struct eth_port *pp, struct neta_rx_desc *rx_desc)
{
	STAT_ERR(pp-&gt;stats.rx_error++);

	if (pp-&gt;dev)
		pp-&gt;dev-&gt;stats.rx_errors++;

#ifdef CONFIG_MV_ETH_DEBUG_CODE
	if ((pp-&gt;flags &amp; MV_ETH_F_DBG_RX) == 0)
		return;

	if (!printk_ratelimit())
		return;

	if ((rx_desc-&gt;status &amp; NETA_RX_FL_DESC_MASK) != NETA_RX_FL_DESC_MASK) {
		printk(KERN_ERR "giga #%d: bad rx status %08x (buffer oversize), size=%d\n",
				pp-&gt;port, rx_desc-&gt;status, rx_desc-&gt;dataSize);
		return;
	}

	switch (rx_desc-&gt;status &amp; NETA_RX_ERR_CODE_MASK) {
	case NETA_RX_ERR_CRC:
		printk(KERN_ERR "giga #%d: bad rx status %08x (crc error), size=%d\n",
				pp-&gt;port, rx_desc-&gt;status, rx_desc-&gt;dataSize);
		break;
	case NETA_RX_ERR_OVERRUN:
		printk(KERN_ERR "giga #%d: bad rx status %08x (overrun error), size=%d\n",
				pp-&gt;port, rx_desc-&gt;status, rx_desc-&gt;dataSize);
		break;
	case NETA_RX_ERR_LEN:
		printk(KERN_ERR "giga #%d: bad rx status %08x (max frame length error), size=%d\n",
				pp-&gt;port, rx_desc-&gt;status, rx_desc-&gt;dataSize);
		break;
	case NETA_RX_ERR_RESOURCE:
		printk(KERN_ERR "giga #%d: bad rx status %08x (resource error), size=%d\n",
				pp-&gt;port, rx_desc-&gt;status, rx_desc-&gt;dataSize);
		break;
	}
	mv_eth_rx_desc_print(rx_desc);
#endif /* CONFIG_MV_ETH_DEBUG_CODE */
}

void mv_eth_skb_print(struct sk_buff *skb)
{
	printk(KERN_ERR "skb=%p: head=%p, data=%p, tail=%p, end=%p\n", skb, skb-&gt;head, skb-&gt;data, skb-&gt;tail, skb-&gt;end);
	printk(KERN_ERR "\t mac=%p, network=%p, transport=%p\n",
			skb-&gt;mac_header, skb-&gt;network_header, skb-&gt;transport_header);
	printk(KERN_ERR "\t truesize=%d, len=%d, data_len=%d, mac_len=%d\n",
		skb-&gt;truesize, skb-&gt;len, skb-&gt;data_len, skb-&gt;mac_len);
	printk(KERN_ERR "\t users=%d, dataref=%d, nr_frags=%d, gso_size=%d, gso_segs=%d\n",
	       atomic_read(&amp;skb-&gt;users), atomic_read(&amp;skb_shinfo(skb)-&gt;dataref),
	       skb_shinfo(skb)-&gt;nr_frags, skb_shinfo(skb)-&gt;gso_size, skb_shinfo(skb)-&gt;gso_segs);
	printk(KERN_ERR "\t proto=%d, ip_summed=%d, priority=%d\n", ntohs(skb-&gt;protocol), skb-&gt;ip_summed, skb-&gt;priority);
#ifdef CONFIG_NET_SKB_RECYCLE
	printk(KERN_ERR "\t skb_recycle=%p, hw_cookie=%p\n", skb-&gt;skb_recycle, skb-&gt;hw_cookie);
#endif /* CONFIG_NET_SKB_RECYCLE */
}

void mv_eth_rx_desc_print(struct neta_rx_desc *desc)
{
	int i;
	u32 *words = (u32 *) desc;

	printk(KERN_ERR "RX desc - %p: ", desc);
	for (i = 0; i &lt; 8; i++)
		printk(KERN_CONT "%8.8x ", *words++);
	printk(KERN_CONT "\n");

	if (desc-&gt;status &amp; NETA_RX_IP4_FRAG_MASK)
		printk(KERN_ERR "Frag, ");

	printk(KERN_CONT "size=%d, L3_offs=%d, IP_hlen=%d, L4_csum=%s, L3=",
	       desc-&gt;dataSize,
	       (desc-&gt;status &amp; NETA_RX_L3_OFFSET_MASK) &gt;&gt; NETA_RX_L3_OFFSET_OFFS,
	       (desc-&gt;status &amp; NETA_RX_IP_HLEN_MASK) &gt;&gt; NETA_RX_IP_HLEN_OFFS,
	       (desc-&gt;status &amp; NETA_RX_L4_CSUM_OK_MASK) ? "Ok" : "Bad");

	if (NETA_RX_L3_IS_IP4(desc-&gt;status))
		printk(KERN_CONT "IPv4, ");
	else if (NETA_RX_L3_IS_IP4_ERR(desc-&gt;status))
		printk(KERN_CONT "IPv4 bad, ");
	else if (NETA_RX_L3_IS_IP6(desc-&gt;status))
		printk(KERN_CONT "IPv6, ");
	else
		printk(KERN_CONT "Unknown, ");

	printk(KERN_CONT "L4=");
	if (NETA_RX_L4_IS_TCP(desc-&gt;status))
		printk(KERN_CONT "TCP");
	else if (NETA_RX_L4_IS_UDP(desc-&gt;status))
		printk(KERN_CONT "UDP");
	else
		printk(KERN_CONT "Unknown");
	printk(KERN_CONT "\n");

#ifdef CONFIG_MV_ETH_PNC
	printk(KERN_ERR "RINFO: ");
	if (desc-&gt;pncInfo &amp; NETA_PNC_DA_MC)
		printk(KERN_CONT "DA_MC, ");
	if (desc-&gt;pncInfo &amp; NETA_PNC_DA_BC)
		printk(KERN_CONT "DA_BC, ");
	if (desc-&gt;pncInfo &amp; NETA_PNC_DA_UC)
		printk(KERN_CONT "DA_UC, ");
	if (desc-&gt;pncInfo &amp; NETA_PNC_VLAN)
		printk(KERN_CONT "VLAN, ");
	if (desc-&gt;pncInfo &amp; NETA_PNC_PPPOE)
		printk(KERN_CONT "PPPOE, ");
	if (desc-&gt;pncInfo &amp; NETA_PNC_RX_SPECIAL)
		printk(KERN_CONT "RX_SPEC, ");
#endif /* CONFIG_MV_ETH_PNC */

	printk(KERN_CONT "\n");
}

void mv_eth_tx_desc_print(struct neta_tx_desc *desc)
{
	int i;
	u32 *words = (u32 *) desc;

	printk(KERN_ERR "TX desc - %p: ", desc);
	for (i = 0; i &lt; 8; i++)
		printk(KERN_CONT "%8.8x ", *words++);
	printk(KERN_CONT "\n");
}

void mv_eth_pkt_print(struct eth_pbuf *pkt)
{
	printk(KERN_ERR "pkt: len=%d off=%d pool=%d "
	       "skb=%p pa=%lx buf=%p\n",
	       pkt-&gt;bytes, pkt-&gt;offset, pkt-&gt;pool,
	       pkt-&gt;osInfo, pkt-&gt;physAddr, pkt-&gt;pBuf);

	mvDebugMemDump(pkt-&gt;pBuf + pkt-&gt;offset, 64, 1);
	mvOsCacheInvalidate(NULL, pkt-&gt;pBuf + pkt-&gt;offset, 64);
}

static inline void mv_eth_rx_csum(struct eth_port *pp, struct neta_rx_desc *rx_desc, struct sk_buff *skb)
{
#if defined(CONFIG_MV_ETH_RX_CSUM_OFFLOAD)
	if (pp-&gt;rx_csum_offload &amp;&amp;
	    ((NETA_RX_L3_IS_IP4(rx_desc-&gt;status) ||
	      NETA_RX_L3_IS_IP6(rx_desc-&gt;status)) &amp;&amp; (rx_desc-&gt;status &amp; NETA_RX_L4_CSUM_OK_MASK))) {
		skb-&gt;csum = 0;
		skb-&gt;ip_summed = CHECKSUM_UNNECESSARY;
		STAT_DBG(pp-&gt;stats.rx_csum_hw++);
		return;
	}
#endif /* CONFIG_MV_ETH_RX_CSUM_OFFLOAD */

	skb-&gt;ip_summed = CHECKSUM_NONE;
	STAT_DBG(pp-&gt;stats.rx_csum_sw++);
}

static inline int mv_eth_tx_done_policy(u32 cause)
{
	return fls(cause &gt;&gt; NETA_CAUSE_TXQ_SENT_DESC_OFFS) - 1;
}

inline int mv_eth_rx_policy(u32 cause)
{
	return fls(cause &gt;&gt; NETA_CAUSE_RXQ_OCCUP_DESC_OFFS) - 1;
}

static inline int mv_eth_txq_tos_map_get(struct eth_port *pp, MV_U8 tos)
{
	MV_U8 q = pp-&gt;txq_tos_map[tos];

	if (q == MV_ETH_TXQ_INVALID)
		return pp-&gt;txq[smp_processor_id()];

	return q;
}

static inline int mv_eth_tx_policy(struct eth_port *pp, struct sk_buff *skb)
{
	int txq = pp-&gt;txq[smp_processor_id()];

	if (ip_hdr(skb)) {
		MV_U8 tos;

		tos = ip_hdr(skb)-&gt;tos;
		txq = mv_eth_txq_tos_map_get(pp, tos);
	}
	return txq;
}

#ifdef CONFIG_NET_SKB_RECYCLE
int mv_eth_skb_recycle(struct sk_buff *skb)
{
	struct eth_pbuf *pkt = skb-&gt;hw_cookie;
	struct bm_pool *pool = &amp;mv_eth_pool[pkt-&gt;pool];
	int status = 0;

	if (skb_recycle_check(skb, pool-&gt;pkt_size)) {

#ifdef CONFIG_MV_ETH_DEBUG_CODE
		/* Sanity check */
		if (skb-&gt;truesize != ((skb-&gt;end - skb-&gt;head) + sizeof(struct sk_buff)))
			mv_eth_skb_print(skb);
#endif /* CONFIG_MV_ETH_DEBUG_CODE */

		STAT_DBG(pool-&gt;stats.skb_recycled_ok++);
		mvOsCacheInvalidate(NULL, skb-&gt;head, RX_BUF_SIZE(pool-&gt;pkt_size));

		status = mv_eth_pool_put(pool, pkt);

#ifdef ETH_SKB_DEBUG
		if (status == 0)
			mv_eth_skb_save(skb, "recycle");
#endif /* ETH_SKB_DEBUG */

		return 0;
	}

	/* printk(KERN_ERR "mv_eth_skb_recycle failed: pool=%d, pkt=%p, skb=%p\n", pkt-&gt;pool, pkt, skb); */

	mvOsFree(pkt);
	skb-&gt;hw_cookie = NULL;

	STAT_DBG(pool-&gt;stats.skb_recycled_err++);

	return 1;
}
#endif /* CONFIG_NET_SKB_RECYCLE */

static struct sk_buff *mv_eth_skb_alloc(struct bm_pool *pool, struct eth_pbuf *pkt)
{
	struct sk_buff *skb;

	skb = dev_alloc_skb(pool-&gt;pkt_size);
	if (!skb) {
		STAT_ERR(pool-&gt;stats.skb_alloc_oom++);
		return NULL;
	}
	STAT_DBG(pool-&gt;stats.skb_alloc_ok++);

#ifdef ETH_SKB_DEBUG
	mv_eth_skb_save(skb, "alloc");
#endif /* ETH_SKB_DEBUG */

#ifdef CONFIG_MV_ETH_BM_CPU
	/* Save pkt as first 4 bytes in the buffer */
#if !defined(CONFIG_MV_ETH_BE_WA)
	*((MV_U32 *) skb-&gt;head) = MV_32BIT_LE((MV_U32)pkt);
#else
	*((MV_U32 *) skb-&gt;head) = (MV_U32)pkt;
#endif /* !CONFIG_MV_ETH_BE_WA */
	mvOsCacheLineFlush(NULL, skb-&gt;head);
#endif /* CONFIG_MV_ETH_BM_CPU */

	pkt-&gt;osInfo = (void *)skb;
	pkt-&gt;pBuf = skb-&gt;head;
	pkt-&gt;physAddr = mvOsCacheInvalidate(NULL, skb-&gt;head, RX_BUF_SIZE(pool-&gt;pkt_size));
	pkt-&gt;offset = NET_SKB_PAD;
	pkt-&gt;pool = pool-&gt;pool;

	return skb;
}

static inline void mv_eth_txq_bufs_free(struct eth_port *pp, struct tx_queue *txq_ctrl, int num)
{
	u32 shadow;
	int i;

	/* Free buffers that was not freed automatically by BM */
	for (i = 0; i &lt; num; i++) {
		shadow = txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_get_i];
		mv_eth_shadow_inc_get(txq_ctrl);

		if (!shadow)
			continue;

		if (shadow &amp; MV_ETH_SHADOW_SKB) {
			shadow &amp;= ~MV_ETH_SHADOW_SKB;
			dev_kfree_skb_any((struct sk_buff *)shadow);
			STAT_DBG(pp-&gt;stats.tx_skb_free++);
		} else {
			if (shadow &amp; MV_ETH_SHADOW_EXT) {
				shadow &amp;= ~MV_ETH_SHADOW_EXT;
				mv_eth_extra_pool_put(pp, (void *)shadow);
			} else {
				/* packet from NFP without BM */
				struct eth_pbuf *pkt = (struct eth_pbuf *)shadow;
				struct bm_pool *pool = &amp;mv_eth_pool[pkt-&gt;pool];

				if (mv_eth_pool_bm(pool)) {
					/* Refill BM pool */
					STAT_DBG(pool-&gt;stats.bm_put++);
					mvBmPoolPut(pkt-&gt;pool, (MV_ULONG) pkt-&gt;physAddr);
				} else {
					mv_eth_pool_put(pool, pkt);
				}
			}
		}
	}
}

inline u32 mv_eth_txq_done(struct eth_port *pp, struct tx_queue *txq_ctrl)
{
	int tx_done;

	tx_done = mvNetaTxqSentDescProc(pp-&gt;port, txq_ctrl-&gt;txp, txq_ctrl-&gt;txq);
	if (!tx_done)
		return tx_done;
/*
	printk(KERN_ERR "tx_done: txq_count=%d, port=%d, txp=%d, txq=%d, tx_done=%d\n",
			txq_ctrl-&gt;txq_count, pp-&gt;port, txq_ctrl-&gt;txp, txq_ctrl-&gt;txq, tx_done);
*/
	if (!mv_eth_txq_bm(txq_ctrl))
		mv_eth_txq_bufs_free(pp, txq_ctrl, tx_done);

	txq_ctrl-&gt;txq_count -= tx_done;
	STAT_DBG(txq_ctrl-&gt;stats.txq_txdone += tx_done);

	return tx_done;
}

inline struct eth_pbuf *mv_eth_pool_get(struct bm_pool *pool)
{
	struct eth_pbuf *pkt = NULL;
	struct sk_buff *skb;
	unsigned long flags = 0;

	MV_ETH_LOCK(&amp;pool-&gt;lock, flags);

	if (mvStackIndex(pool-&gt;stack) &gt; 0) {
		STAT_DBG(pool-&gt;stats.stack_get++);
		pkt = (struct eth_pbuf *)mvStackPop(pool-&gt;stack);
	} else
		STAT_ERR(pool-&gt;stats.stack_empty++);

	MV_ETH_UNLOCK(&amp;pool-&gt;lock, flags);
	if (pkt)
		return pkt;

	/* Try to allocate new pkt + skb */
	pkt = mvOsMalloc(sizeof(struct eth_pbuf));
	if (pkt) {
		skb = mv_eth_skb_alloc(pool, pkt);
		if (!skb) {
			mvOsFree(pkt);
			pkt = NULL;
		}
	}
	return pkt;
}

/* Reuse pkt if possible, allocate new skb and move BM pool or RXQ ring */
inline int mv_eth_refill(struct eth_port *pp, int rxq,
				struct eth_pbuf *pkt, struct bm_pool *pool, struct neta_rx_desc *rx_desc)
{
	if (pkt == NULL) {
		pkt = mv_eth_pool_get(pool);
		if (pkt == NULL)
			return 1;
	} else {
		struct sk_buff *skb;

		/* No recycle -  alloc new skb */
		skb = mv_eth_skb_alloc(pool, pkt);
		if (!skb) {
			mvOsFree(pkt);
			pool-&gt;missed++;
			mv_eth_add_cleanup_timer(pp);
			return 1;
		}
	}
	mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);

	return 0;
}


static inline MV_U32 mv_eth_skb_tx_csum(struct eth_port *pp, struct sk_buff *skb)
{
#ifdef CONFIG_MV_ETH_TX_CSUM_OFFLOAD
	if (skb-&gt;ip_summed == CHECKSUM_PARTIAL) {
		int   ip_hdr_len = 0;
		MV_U8 l4_proto;

		if (skb-&gt;protocol == htons(ETH_P_IP)) {
			struct iphdr *ip4h = ip_hdr(skb);

			/* Calculate IPv4 checksum and L4 checksum */
			ip_hdr_len = ip4h-&gt;ihl;
			l4_proto = ip4h-&gt;protocol;
		} else if (skb-&gt;protocol == htons(ETH_P_IPV6)) {
			/* If not IPv4 - must be ETH_P_IPV6 - Calculate only L4 checksum */
			struct ipv6hdr *ip6h = ipv6_hdr(skb);

			/* Read l4_protocol from one of IPv6 extra headers ?????? */
			if (skb_network_header_len(skb) &gt; 0)
				ip_hdr_len = (skb_network_header_len(skb) &gt;&gt; 2);
			l4_proto = ip6h-&gt;nexthdr;
		} else {
			STAT_DBG(pp-&gt;stats.tx_csum_sw++);
			return NETA_TX_L4_CSUM_NOT;
		}
		STAT_DBG(pp-&gt;stats.tx_csum_hw++);

		return mvNetaTxqDescCsum(skb_network_offset(skb), skb-&gt;protocol, ip_hdr_len, l4_proto);
	}
#endif /* CONFIG_MV_ETH_TX_CSUM_OFFLOAD */

	STAT_DBG(pp-&gt;stats.tx_csum_sw++);
	return NETA_TX_L4_CSUM_NOT;
}

#ifdef CONFIG_MV_ETH_RX_DESC_PREFETCH
inline struct neta_rx_desc *mv_eth_rx_prefetch(struct eth_port *pp, MV_NETA_RXQ_CTRL *rx_ctrl,
									  int rx_done, int rx_todo)
{
	struct neta_rx_desc	*rx_desc, *next_desc;

	rx_desc = mvNetaRxqNextDescGet(rx_ctrl);
	if (rx_done == 0) {
		/* First descriptor in the NAPI loop */
		mvOsCacheLineInv(NULL, rx_desc);
		prefetch(rx_desc);
	}
	if ((rx_done + 1) == rx_todo) {
		/* Last descriptor in the NAPI loop - prefetch are not needed */
		return rx_desc;
	}
	/* Prefetch next descriptor */
	next_desc = mvNetaRxqDescGet(rx_ctrl);
	mvOsCacheLineInv(NULL, next_desc);
	prefetch(next_desc);

	return rx_desc;
}
#endif /* CONFIG_MV_ETH_RX_DESC_PREFETCH */

static void dpa_wrapper_fwd(struct eth_pbuf *pkt, struct dpa_entry *dpe, 
			    struct neta_rx_desc *rx_desc, int rxq)
{
	struct net_device *dev = (struct net_device *)dpe-&gt;mod.tx_port;
	struct eth_port *pp = MV_ETH_PRIV(dev);
	struct neta_tx_desc *tx_desc;
	u32 tx_cmd = 0, physAddr, txq, txp;
	struct tx_queue *txq_ctrl;
	struct bm_pool *pool = &amp;mv_eth_pool[pkt-&gt;pool];
	int pkt_offset;

	printk(KERN_ERR "(%s:%d) FASTFWD port %s!\n", __func__, __LINE__,
	       dev-&gt;name);
	read_lock(&amp;pp-&gt;rwlock);

	/* Get TxQ to send packet */
	/* Check TXQ classification */
	txq = 0/* pp-&gt;txq[smp_processor_id()] */;
	txp = 0/*pp-&gt;txp*/;

	txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
	spin_lock(&amp;txq_ctrl-&gt;queue_lock);

	/* Get next descriptor for tx, single buffer, so FIRST &amp; LAST */
	tx_desc = mv_eth_tx_desc_get(txq_ctrl, 1);
	if (tx_desc == NULL) {
		/* No resources: Drop */
		printk(KERN_ERR "(%s:%d) no tx_descriptors !\n", __func__, __LINE__);
		dev-&gt;stats.tx_dropped++;
		goto out;
	}

	/* pkt stays the same only if bridged */
	if (dpe-&gt;mod_ops) {
		MV_U8 *pData = pkt-&gt;pBuf + pkt-&gt;offset;
		mvOsCacheMultiLineFlushInv(NULL, pData, 64);
	}
	txq_ctrl-&gt;txq_count++;

	/* this means at least routing, hence ttl modified */
	if (dpe-&gt;mod_ops &amp; DPE_MOD_L2) {
		tx_cmd |= NETA_TX_L3_IP4 | NETA_TX_IP_CSUM_MASK |
			  ((14) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
			  ((NETA_RX_GET_IPHDR_HDRLEN(rx_desc)) &lt;&lt; NETA_TX_IP_HLEN_OFFS);
	}
	if (dpe-&gt;mod_ops &amp; DPE_MOD_NAT) {
		if (dpe-&gt;org.cls_tuple.protocol == MV_IP_PROTO_UDP)
			tx_cmd |= NETA_TX_L4_UDP;
		else 
			tx_cmd |= (NETA_TX_L4_TCP | NETA_TX_L4_CSUM_FULL);

		tx_cmd |= NETA_TX_L4_CSUM_FULL;
	}
	pkt_offset = pkt-&gt;offset;
	physAddr = pkt-&gt;physAddr;
	if (pkt_offset &gt; NETA_TX_PKT_OFFSET_MAX) {
		physAddr += pkt_offset;
		pkt_offset = 0;
	}

	if ((pkt-&gt;pool &gt;= 0) &amp;&amp; (pkt-&gt;pool &lt; MV_ETH_BM_POOLS)) {
		txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = (u32) pkt;
	} else {
		/* skb from external interface */
		txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = ((u32)pkt-&gt;osInfo | MV_ETH_SHADOW_SKB);
	}

	mv_eth_shadow_inc_put(txq_ctrl);

	tx_cmd |= NETA_TX_PKT_OFFSET_MASK(pkt_offset);

	tx_desc-&gt;command = tx_cmd | NETA_TX_FLZ_DESC_MASK;
	tx_desc-&gt;dataSize = pkt-&gt;bytes;
	tx_desc-&gt;bufPhysAddr = physAddr;

	tx_desc-&gt;hw_cmd = 0;

	mv_eth_tx_desc_flush(tx_desc);

	/* Enable transmit by update PENDING counter */
	mvNetaTxqPendDescAdd(pp-&gt;port, txp, txq, 1);

	/* FIXME: stats includes MH --BK */
	dev-&gt;stats.tx_packets++;
	dev-&gt;stats.tx_bytes += pkt-&gt;bytes;
	dpe-&gt;counters.fwd_hits++;

	mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
out:
	spin_unlock(&amp;txq_ctrl-&gt;queue_lock);
	read_unlock(&amp;pp-&gt;rwlock);

}


static inline int mv_eth_rx(struct eth_port *pp, int rx_todo, int rxq)
{
	struct net_device *dev;
	MV_NETA_RXQ_CTRL *rx_ctrl = pp-&gt;rxq_ctrl[rxq].q;
	int rx_done, rx_filled, err;
	struct neta_rx_desc *rx_desc;
	u32 rx_status;
	int rx_bytes;
	struct eth_pbuf *pkt;
	struct sk_buff *skb;
	struct bm_pool *pool;

	/* Get number of received packets */
	rx_done = mvNetaRxqBusyDescNumGet(pp-&gt;port, rxq);
	mvOsCacheIoSync();

	if (rx_todo &gt; rx_done)
		rx_todo = rx_done;

	rx_done = 0;
	rx_filled = 0;

	/* Fairness NAPI loop */
	while (rx_done &lt; rx_todo) {

#ifdef CONFIG_MV_ETH_RX_DESC_PREFETCH
		rx_desc = mv_eth_rx_prefetch(pp, rx_ctrl, rx_done, rx_todo);
#else
		rx_desc = mvNetaRxqNextDescGet(rx_ctrl);
		mvOsCacheLineInv(NULL, rx_desc);
		prefetch(rx_desc);
#endif /* CONFIG_MV_ETH_RX_DESC_PREFETCH */

		rx_done++;
		rx_filled++;

#if defined(MV_CPU_BE)
		mvNetaRxqDescSwap(rx_desc);
#endif /* MV_CPU_BE */

#ifdef CONFIG_MV_ETH_DEBUG_CODE
		if (pp-&gt;flags &amp; MV_ETH_F_DBG_RX) {
			printk(KERN_ERR "\n%s: port=%d, cpu=%d\n", __func__, pp-&gt;port, smp_processor_id());
			mv_eth_rx_desc_print(rx_desc);
		}
#endif /* CONFIG_MV_ETH_DEBUG_CODE */

/* HAIM DEBUG START */{
//      		printk(KERN_ERR "(%s:%d) HAIM\n", __func__, __LINE__);
//      		mv_eth_rx_desc_print(rx_desc);
		}
/*HAIM_END*/
		rx_status = rx_desc-&gt;status;
		pkt = (struct eth_pbuf *)rx_desc-&gt;bufCookie;
		pool = &amp;mv_eth_pool[pkt-&gt;pool];

		if (((rx_status &amp; NETA_RX_FL_DESC_MASK) != NETA_RX_FL_DESC_MASK) ||
			(rx_status &amp; NETA_RX_ES_MASK)) {

			mv_eth_rx_error(pp, rx_desc);

			mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
			continue;
		}

		/* Speculative ICache prefetch WA: should be replaced with dma_unmap_single (invalidate l2) */
		mvOsCacheMultiLineInv(NULL, pkt-&gt;pBuf + pkt-&gt;offset, rx_desc-&gt;dataSize);

#ifdef CONFIG_MV_ETH_RX_PKT_PREFETCH
		prefetch(pkt-&gt;pBuf + pkt-&gt;offset);
		prefetch(pkt-&gt;pBuf + pkt-&gt;offset + CPU_D_CACHE_LINE_SIZE);
#endif /* CONFIG_MV_ETH_RX_PKT_PREFETCH */

#ifdef CONFIG_MV_ETH_SWITCH
		dev = mv_eth_switch_netdev_get(pp, pkt);
#else
		dev = pp-&gt;dev;
#endif /* CONFIG_MV_ETH_SWITCH */

		STAT_DBG(pp-&gt;stats.rxq[rxq]++);
		dev-&gt;stats.rx_packets++;

		rx_bytes = rx_desc-&gt;dataSize - (MV_ETH_CRC_SIZE + MV_ETH_MH_SIZE);
		dev-&gt;stats.rx_bytes += rx_bytes;

#ifndef CONFIG_MV_ETH_PNC
	/* Update IP offset and IP header len in RX descriptor */
	if (NETA_RX_L3_IS_IP4(rx_desc-&gt;status)) {
		int ip_offset;

		if ((rx_desc-&gt;status &amp; ETH_RX_VLAN_TAGGED_FRAME_MASK))
			ip_offset = MV_ETH_MH_SIZE + sizeof(MV_802_3_HEADER) + MV_VLAN_HLEN;
		else
			ip_offset = MV_ETH_MH_SIZE + sizeof(MV_802_3_HEADER);

		NETA_RX_SET_IPHDR_OFFSET(rx_desc, ip_offset);
		NETA_RX_SET_IPHDR_HDRLEN(rx_desc, 5);
	}
#endif /* !CONFIG_MV_ETH_PNC */

#ifdef CONFIG_MV_ETH_DEBUG_CODE
		if (pp-&gt;flags &amp; MV_ETH_F_DBG_RX)
			mvDebugMemDump(pkt-&gt;pBuf + pkt-&gt;offset, 64, 1);
#endif /* CONFIG_MV_ETH_DEBUG_CODE */

#if defined(CONFIG_MV_ETH_PNC) &amp;&amp; defined(CONFIG_MV_ETH_RX_SPECIAL)
		/* Special RX processing */
		if (rx_desc-&gt;pncInfo &amp; NETA_PNC_RX_SPECIAL) {
			if (pp-&gt;rx_special_proc) {
				pp-&gt;rx_special_proc(pp-&gt;port, rxq, dev, (struct sk_buff *)(pkt-&gt;osInfo), rx_desc);
				STAT_INFO(pp-&gt;stats.rx_special++);

				/* Refill processing */
				err = mv_eth_refill(pp, rxq, pkt, pool, rx_desc);
				if (err) {
					printk(KERN_ERR "Linux processing - Can't refill\n");
					pp-&gt;rxq_ctrl[rxq].missed++;
					rx_filled--;
				}
				continue;
			}
		}
#endif /* CONFIG_MV_ETH_PNC &amp;&amp; CONFIG_MV_ETH_RX_SPECIAL */

#ifdef CONFIG_MV_ETH_NFP
		if (pp-&gt;flags &amp; MV_ETH_F_NFP_EN) {
			MV_STATUS status;

			pkt-&gt;bytes = rx_bytes + MV_ETH_MH_SIZE;
			pkt-&gt;offset = NET_SKB_PAD;

			status = mv_eth_nfp(pp, rxq, rx_desc, pkt, pool);
			if (status == MV_OK)
				continue;
			if (status == MV_FAIL) {
				rx_filled--;
				continue;
			}
			/* MV_TERMINATE - packet returned to slow path */
		}
#endif /* CONFIG_MV_ETH_NFP */

		/* Linux processing */
		skb = (struct sk_buff *)(pkt-&gt;osInfo);

		if (dpa_rx_hook) {
			uint32_t ret = dpa_rx_hook(rx_desc, 
						       skb-&gt;data, 
						       &amp;skb-&gt;dpa_cookie);
			if (ret == DPE_FWD) {
				pkt-&gt;bytes = rx_bytes;
				pkt-&gt;offset = NET_SKB_PAD + MV_ETH_MH_SIZE;

				dpa_wrapper_fwd(pkt, (struct dpa_entry *)skb-&gt;dpa_cookie,
						rx_desc, rxq);
				continue;
			}
		}
		skb-&gt;data += MV_ETH_MH_SIZE;
		skb-&gt;tail += (rx_bytes + MV_ETH_MH_SIZE);
		skb-&gt;len = rx_bytes;

#ifdef ETH_SKB_DEBUG
		mv_eth_skb_check(skb);
#endif /* ETH_SKB_DEBUG */

		skb-&gt;protocol = eth_type_trans(skb, dev);

#ifdef CONFIG_NET_SKB_RECYCLE
		if (mv_eth_is_recycle()) {
			skb-&gt;skb_recycle = mv_eth_skb_recycle;
			skb-&gt;hw_cookie = pkt;
			pkt = NULL;
		}
#endif /* CONFIG_NET_SKB_RECYCLE */

		if (skb)
			mv_eth_rx_csum(pp, rx_desc, skb);

#ifdef CONFIG_MV_ETH_GRO
		if (skb &amp;&amp; (dev-&gt;features &amp; NETIF_F_GRO)) {
			STAT_DBG(pp-&gt;stats.rx_gro++);
			STAT_DBG(pp-&gt;stats.rx_gro_bytes += skb-&gt;len);

			rx_status = napi_gro_receive(pp-&gt;napi[smp_processor_id()], skb);
			skb = NULL;
		}
#endif /* CONFIG_MV_ETH_GRO */

		if (skb) {
			STAT_DBG(pp-&gt;stats.rx_netif++);
			rx_status = netif_receive_skb(skb);
			STAT_DBG(if (rx_status)	(pp-&gt;stats.rx_drop_sw++));
		}

		/* Refill processing: */
		err = mv_eth_refill(pp, rxq, pkt, pool, rx_desc);
		if (err) {
			printk(KERN_ERR "Linux processing - Can't refill\n");
			pp-&gt;rxq_ctrl[rxq].missed++;
			mv_eth_add_cleanup_timer(pp);
			rx_filled--;
		}
	}

	/* Update RxQ management counters */
	mvOsCacheIoSync();
	mvNetaRxqDescNumUpdate(pp-&gt;port, rxq, rx_done, rx_filled);

	return rx_done;
}

static int mv_eth_tx(struct sk_buff *skb, struct net_device *dev)
{
	struct eth_port *pp = MV_ETH_PRIV(dev);
	struct eth_netdev *dev_priv = MV_DEV_PRIV(dev);
	int frags = 0;
	bool tx_spec_ready = false;
	struct mv_eth_tx_spec tx_spec;
	u32 tx_cmd;
	u16 mh;
	struct tx_queue *txq_ctrl = NULL;
	struct neta_tx_desc *tx_desc;

	read_lock(&amp;pp-&gt;rwlock);

	if (!(netif_running(dev))) {
		printk(KERN_ERR "!netif_running() in %s\n", __func__);
		goto out;
	}

#if defined(CONFIG_MV_ETH_TX_SPECIAL)
	if (pp-&gt;tx_special_check) {

		if (pp-&gt;tx_special_check(pp-&gt;port, dev, skb, &amp;tx_spec)) {
			STAT_INFO(pp-&gt;stats.tx_special++);
			if (tx_spec.tx_func) {
				tx_spec.tx_func(skb-&gt;data, skb-&gt;len, &amp;tx_spec);
				goto out;
			} else {
				/* Check validity of tx_spec txp/txq must be CPU owned */
				tx_spec_ready = true;
			}
		}
	}
#endif /* CONFIG_MV_ETH_TX_SPECIAL */

	/* Get TXQ (without BM) to send packet generated by Linux */
	if (tx_spec_ready == false) {
		tx_spec.txp = pp-&gt;txp;
		tx_spec.txq = mv_eth_tx_policy(pp, skb);
		tx_spec.hw_cmd = pp-&gt;hw_cmd;
		tx_spec.flags = pp-&gt;flags;
	}

	txq_ctrl = &amp;pp-&gt;txq_ctrl[tx_spec.txp * CONFIG_MV_ETH_TXQ + tx_spec.txq];
	if (txq_ctrl == NULL) {
		printk(KERN_ERR "%s: invalidate txp/txq (%d/%d)\n", __func__, tx_spec.txp, tx_spec.txq);
		goto out;
	}
	spin_lock(&amp;txq_ctrl-&gt;queue_lock);

#ifdef CONFIG_MV_ETH_TSO
	/* GSO/TSO */
	if (skb_is_gso(skb)) {
		frags = mv_eth_tx_tso(skb, dev, &amp;tx_spec, txq_ctrl);
		goto out;
	}
#endif /* CONFIG_MV_ETH_TSO */

	frags = skb_shinfo(skb)-&gt;nr_frags + 1;

	if (tx_spec.flags &amp; MV_ETH_F_MH) {
		if (tx_spec.flags &amp; MV_ETH_F_SWITCH)
			mh = dev_priv-&gt;tx_vlan_mh;
		else
			mh = pp-&gt;tx_mh;

		if (mv_eth_skb_mh_add(skb, mh)) {
			frags = 0;
			goto out;
		}
	}

	tx_desc = mv_eth_tx_desc_get(txq_ctrl, frags);
	if (tx_desc == NULL) {
		frags = 0;
		goto out;
	}

	/* Don't use BM for Linux packets: NETA_TX_BM_ENABLE_MASK = 0 */
	/* NETA_TX_PKT_OFFSET_MASK = 0 - for all descriptors */
	tx_cmd = mv_eth_skb_tx_csum(pp, skb);

#ifdef CONFIG_MV_PON
	tx_desc-&gt;hw_cmd = tx_spec.hw_cmd;
#endif

	/* FIXME: beware of nonlinear --BK */
	tx_desc-&gt;dataSize = skb_headlen(skb);

	tx_desc-&gt;bufPhysAddr = mvOsCacheFlush(NULL, skb-&gt;data, tx_desc-&gt;dataSize);
	if (dpa_os_tx_hook) {
		dpa_os_tx_hook(skb-&gt;data, skb-&gt;dpa_cookie,
				(struct iphdr *)skb_network_header(skb), dev);
	}

	if (frags == 1) {
		/*
		 * First and Last descriptor
		 */
		if (tx_spec.flags &amp; MV_ETH_F_NO_PAD)
			tx_cmd |= NETA_TX_F_DESC_MASK | NETA_TX_L_DESC_MASK;
		else
			tx_cmd |= NETA_TX_FLZ_DESC_MASK;

		tx_desc-&gt;command = tx_cmd;
		mv_eth_tx_desc_flush(tx_desc);

		txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = ((MV_ULONG) skb | MV_ETH_SHADOW_SKB);
		mv_eth_shadow_inc_put(txq_ctrl);
	} else {

		/* First but not Last */
		tx_cmd |= NETA_TX_F_DESC_MASK;

		txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = 0;
		mv_eth_shadow_inc_put(txq_ctrl);

		tx_desc-&gt;command = tx_cmd;
		mv_eth_tx_desc_flush(tx_desc);

		/* Continue with other skb fragments */
		mv_eth_tx_frag_process(pp, skb, txq_ctrl, tx_spec.flags);
		STAT_DBG(pp-&gt;stats.tx_sg++);
	}
/*
	printk(KERN_ERR "tx: frags=%d, tx_desc[0x0]=%x [0xc]=%x, wr_id=%d, rd_id=%d, skb=%p\n",
			frags, tx_desc-&gt;command,tx_desc-&gt;hw_cmd,
			txq_ctrl-&gt;shadow_txq_put_i, txq_ctrl-&gt;shadow_txq_get_i, skb);
*/
	txq_ctrl-&gt;txq_count += frags;

#ifdef CONFIG_MV_ETH_DEBUG_CODE
	if (pp-&gt;flags &amp; MV_ETH_F_DBG_TX) {
		printk(KERN_ERR "\n");
		printk(KERN_ERR "%s - eth_tx_%lu: port=%d, txp=%d, txq=%d, skb=%p, head=%p, data=%p, size=%d\n",
		       dev-&gt;name, dev-&gt;stats.tx_packets, pp-&gt;port, tx_spec.txp, tx_spec.txq, skb,
			   skb-&gt;head, skb-&gt;data, skb-&gt;len);
		mv_eth_tx_desc_print(tx_desc);
		/*mv_eth_skb_print(skb);*/
		mvDebugMemDump(skb-&gt;data, 64, 1);
	}
#endif /* CONFIG_MV_ETH_DEBUG_CODE */

#ifdef CONFIG_MV_PON
	if (MV_PON_PORT(pp-&gt;port))
		mvNetaPonTxqBytesAdd(pp-&gt;port, tx_spec.txp, tx_spec.txq, skb-&gt;len);
#endif /* CONFIG_MV_PON */

	/* Enable transmit */
	mvNetaTxqPendDescAdd(pp-&gt;port, tx_spec.txp, tx_spec.txq, frags);

	STAT_DBG(txq_ctrl-&gt;stats.txq_tx += frags);

out:
	if (frags &gt; 0) {
		dev-&gt;stats.tx_packets++;
		dev-&gt;stats.tx_bytes += skb-&gt;len;
	} else {
		dev-&gt;stats.tx_dropped++;
		dev_kfree_skb_any(skb);
	}

#ifndef CONFIG_MV_ETH_TXDONE_ISR
	if (txq_ctrl) {
		if (txq_ctrl-&gt;txq_count &gt;= mv_ctrl_txdone) {
			STAT_DIST(u32 tx_done =) mv_eth_txq_done(pp, txq_ctrl);

			STAT_DIST(if (tx_done &lt; pp-&gt;dist_stats.tx_done_dist_size)
					pp-&gt;dist_stats.tx_done_dist[tx_done]++);
		}
		/* If after calling mv_eth_txq_done, txq_ctrl-&gt;txq_count equals frags, we need to set the timer */
		if ((txq_ctrl-&gt;txq_count == frags) &amp;&amp; (frags &gt; 0))
			mv_eth_add_tx_done_timer(pp);
	}
#endif /* CONFIG_MV_ETH_TXDONE_ISR */

	if (txq_ctrl)
		spin_unlock(&amp;txq_ctrl-&gt;queue_lock);

	read_unlock(&amp;pp-&gt;rwlock);
	return NETDEV_TX_OK;
}

#ifdef CONFIG_MV_ETH_TSO
/* Validate TSO */
static inline int mv_eth_tso_validate(struct sk_buff *skb, struct net_device *dev)
{
	if (!(dev-&gt;features &amp; NETIF_F_TSO)) {
		printk(KERN_ERR "error: (skb_is_gso(skb) returns true but features is not NETIF_F_TSO\n");
		return 1;
	}

	if (skb_shinfo(skb)-&gt;frag_list != NULL) {
		printk(KERN_ERR "***** ERROR: frag_list is not null\n");
		return 1;
	}

	if (skb_shinfo(skb)-&gt;gso_segs == 1) {
		printk(KERN_ERR "***** ERROR: only one TSO segment\n");
		return 1;
	}

	if (skb-&gt;len &lt;= skb_shinfo(skb)-&gt;gso_size) {
		printk(KERN_ERR "***** ERROR: total_len (%d) less than gso_size (%d)\n", skb-&gt;len, skb_shinfo(skb)-&gt;gso_size);
		return 1;
	}
	if ((htons(ETH_P_IP) != skb-&gt;protocol) || (ip_hdr(skb)-&gt;protocol != IPPROTO_TCP) || (tcp_hdr(skb) == NULL)) {
		printk(KERN_ERR "***** ERROR: Protocol is not TCP over IP\n");
		return 1;
	}
	return 0;
}

static inline int mv_eth_tso_build_hdr_desc(struct neta_tx_desc *tx_desc, struct eth_port *priv, struct sk_buff *skb,
					     struct tx_queue *txq_ctrl, u16 *mh, int hdr_len, int size,
					     MV_U32 tcp_seq, MV_U16 ip_id, int left_len)
{
	struct iphdr *iph;
	struct tcphdr *tcph;
	MV_U8 *data, *mac;
	int mac_hdr_len = skb_network_offset(skb);

	data = mv_eth_extra_pool_get(priv);
	if (!data)
		return 0;

	txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = ((MV_ULONG)data | MV_ETH_SHADOW_EXT);

	/* Reserve 2 bytes for IP header alignment */
	mac = data + MV_ETH_MH_SIZE;
	iph = (struct iphdr *)(mac + mac_hdr_len);

	memcpy(mac, skb-&gt;data, hdr_len);

	if (iph) {
		iph-&gt;id = htons(ip_id);
		iph-&gt;tot_len = htons(size + hdr_len - mac_hdr_len);
	}

	tcph = (struct tcphdr *)(mac + skb_transport_offset(skb));
	tcph-&gt;seq = htonl(tcp_seq);

	if (left_len) {
		/* Clear all special flags for not last packet */
		tcph-&gt;psh = 0;
		tcph-&gt;fin = 0;
		tcph-&gt;rst = 0;
	}

	if (mh) {
		/* Start tarnsmit from MH - add 2 bytes to size */
		*((MV_U16 *)data) = *mh;
		/* increment ip_offset field in TX descriptor by 2 bytes */
		mac_hdr_len += MV_ETH_MH_SIZE;
		hdr_len += MV_ETH_MH_SIZE;
	} else {
		/* Start transmit from MAC */
		data = mac;
	}

	tx_desc-&gt;dataSize = hdr_len;
	tx_desc-&gt;command = mvNetaTxqDescCsum(mac_hdr_len, skb-&gt;protocol, ((u8 *)tcph - (u8 *)iph) &gt;&gt; 2, IPPROTO_TCP);
	tx_desc-&gt;command |= NETA_TX_F_DESC_MASK;

	tx_desc-&gt;bufPhysAddr = mvOsCacheFlush(NULL, data, tx_desc-&gt;dataSize);
	mv_eth_shadow_inc_put(txq_ctrl);

	mv_eth_tx_desc_flush(tx_desc);

	return hdr_len;
}

static inline int mv_eth_tso_build_data_desc(struct neta_tx_desc *tx_desc, struct sk_buff *skb,
					     struct tx_queue *txq_ctrl, char *frag_ptr,
					     int frag_size, int data_left, int total_left)
{
	int size;

	size = MV_MIN(frag_size, data_left);

	tx_desc-&gt;dataSize = size;
	tx_desc-&gt;bufPhysAddr = mvOsCacheFlush(NULL, frag_ptr, size);
	tx_desc-&gt;command = 0;
	txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = 0;

	if (size == data_left) {
		/* last descriptor in the TCP packet */
		tx_desc-&gt;command = NETA_TX_L_DESC_MASK;

		if (total_left == 0) {
			/* last descriptor in SKB */
			txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = ((MV_ULONG) skb | MV_ETH_SHADOW_SKB);
		}
	}
	mv_eth_shadow_inc_put(txq_ctrl);
	mv_eth_tx_desc_flush(tx_desc);

	return size;
}

/***********************************************************
 * mv_eth_tx_tso --                                        *
 *   send a packet.                                        *
 ***********************************************************/
int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev,
		struct mv_eth_tx_spec *tx_spec, struct tx_queue *txq_ctrl)
{
	int frag = 0;
	int total_len, hdr_len, size, frag_size, data_left;
	char *frag_ptr;
	int totalDescNum, totalBytes = 0;
	struct neta_tx_desc *tx_desc;
	MV_U16 ip_id;
	MV_U32 tcp_seq = 0;
	skb_frag_t *skb_frag_ptr;
	const struct tcphdr *th = tcp_hdr(skb);
	struct eth_port *priv = MV_ETH_PRIV(dev);
	struct eth_netdev *dev_priv = MV_DEV_PRIV(dev);
	MV_U16 *mh = NULL;
	int i;

	STAT_DBG(priv-&gt;stats.tx_tso++);
/*
	printk(KERN_ERR "mv_eth_tx_tso_%d ENTER: skb=%p, total_len=%d\n", priv-&gt;stats.tx_tso, skb, skb-&gt;len);
*/
	if (mv_eth_tso_validate(skb, dev))
		return 0;

	/* Calculate expected number of TX descriptors */
	totalDescNum = skb_shinfo(skb)-&gt;gso_segs * 2 + skb_shinfo(skb)-&gt;nr_frags;

	if ((txq_ctrl-&gt;txq_count + totalDescNum) &gt;= txq_ctrl-&gt;txq_size) {
/*
		printk(KERN_ERR "%s: no TX descriptors - txq_count=%d, len=%d, nr_frags=%d, gso_segs=%d\n",
					__func__, txq_ctrl-&gt;txq_count, skb-&gt;len, skb_shinfo(skb)-&gt;nr_frags,
					skb_shinfo(skb)-&gt;gso_segs);
*/
		STAT_ERR(txq_ctrl-&gt;stats.txq_err++);
		return 0;
	}

	total_len = skb-&gt;len;
	hdr_len = (skb_transport_offset(skb) + tcp_hdrlen(skb));

	total_len -= hdr_len;
	ip_id = ntohs(ip_hdr(skb)-&gt;id);
	tcp_seq = ntohl(th-&gt;seq);

	frag_size = skb_headlen(skb);
	frag_ptr = skb-&gt;data;

	if (frag_size &lt; hdr_len) {
		printk(KERN_ERR "***** ERROR: frag_size=%d, hdr_len=%d\n", frag_size, hdr_len);
		return 0;
	}

	frag_size -= hdr_len;
	frag_ptr += hdr_len;
	if (frag_size == 0) {
		skb_frag_ptr = &amp;skb_shinfo(skb)-&gt;frags[frag];

		/* Move to next segment */
		frag_size = skb_frag_ptr-&gt;size;
		frag_ptr = page_address(skb_frag_ptr-&gt;page) + skb_frag_ptr-&gt;page_offset;
		frag++;
	}
	totalDescNum = 0;

	while (total_len &gt; 0) {
		data_left = MV_MIN(skb_shinfo(skb)-&gt;gso_size, total_len);

		tx_desc = mv_eth_tx_desc_get(txq_ctrl, 1);
		if (tx_desc == NULL)
			goto outNoTxDesc;

		totalDescNum++;
		total_len -= data_left;
		txq_ctrl-&gt;txq_count++;

		if (tx_spec-&gt;flags &amp; MV_ETH_F_MH) {
			if (tx_spec-&gt;flags &amp; MV_ETH_F_SWITCH)
				mh = &amp;dev_priv-&gt;tx_vlan_mh;
			else
				mh = &amp;priv-&gt;tx_mh;
		}

		/* prepare packet headers: MAC + IP + TCP */
		size = mv_eth_tso_build_hdr_desc(tx_desc, priv, skb, txq_ctrl, mh,
					hdr_len, data_left, tcp_seq, ip_id, total_len);
		if (size == 0)
			goto outNoTxDesc;

		totalBytes += size;
/*
		printk(KERN_ERR "Header desc: tx_desc=%p, skb=%p, hdr_len=%d, data_left=%d\n",
						tx_desc, skb, hdr_len, data_left);
*/
		ip_id++;

		while (data_left &gt; 0) {
			tx_desc = mv_eth_tx_desc_get(txq_ctrl, 1);
			if (tx_desc == NULL)
				goto outNoTxDesc;

			totalDescNum++;
			txq_ctrl-&gt;txq_count++;

			size = mv_eth_tso_build_data_desc(tx_desc, skb, txq_ctrl,
							  frag_ptr, frag_size, data_left, total_len);
			totalBytes += size;
/*
			printk(KERN_ERR "Data desc: tx_desc=%p, skb=%p, size=%d, frag_size=%d, data_left=%d\n",
							tx_desc, skb, size, frag_size, data_left);
 */
			data_left -= size;
			tcp_seq += size;

			frag_size -= size;
			frag_ptr += size;

			if ((frag_size == 0) &amp;&amp; (frag &lt; skb_shinfo(skb)-&gt;nr_frags)) {
				skb_frag_ptr = &amp;skb_shinfo(skb)-&gt;frags[frag];

				/* Move to next segment */
				frag_size = skb_frag_ptr-&gt;size;
				frag_ptr = page_address(skb_frag_ptr-&gt;page) + skb_frag_ptr-&gt;page_offset;
				frag++;
			}
		}		/* of while data_left &gt; 0 */
	}			/* of while (total_len &gt; 0) */

#ifdef CONFIG_MV_PON
	if (MV_PON_PORT(priv-&gt;port))
		mvNetaPonTxqBytesAdd(priv-&gt;port, txq_ctrl-&gt;txp, txq_ctrl-&gt;txq, totalBytes);
#endif /* CONFIG_MV_PON */

	STAT_DBG(priv-&gt;stats.tx_tso_bytes += totalBytes);
	STAT_DBG(txq_ctrl-&gt;stats.txq_tx += totalDescNum);

	mvNetaTxqPendDescAdd(priv-&gt;port, txq_ctrl-&gt;txp, txq_ctrl-&gt;txq, totalDescNum);
/*
	printk(KERN_ERR "mv_eth_tx_tso EXIT: totalDescNum=%d\n", totalDescNum);
*/
	return totalDescNum;

outNoTxDesc:
	/* No enough TX descriptors for the whole skb - rollback */
	printk(KERN_ERR "%s: No TX descriptors - rollback %d, txq_count=%d, nr_frags=%d, skb=%p, len=%d, gso_segs=%d\n",
			__func__, totalDescNum, txq_ctrl-&gt;txq_count, skb_shinfo(skb)-&gt;nr_frags,
			skb, skb-&gt;len, skb_shinfo(skb)-&gt;gso_segs);

	for (i = 0; i &lt; totalDescNum; i++) {
		txq_ctrl-&gt;txq_count--;
		mv_eth_shadow_dec_put(txq_ctrl);
		mvNetaTxqPrevDescGet(txq_ctrl-&gt;q);
	}
	return 0;
}
#endif /* CONFIG_MV_ETH_TSO */

/* Drop packets received by the RXQ and free buffers */
static void mv_eth_rxq_drop_pkts(struct eth_port *pp, int rxq)
{
	struct neta_rx_desc *rx_desc;
	struct eth_pbuf     *pkt;
	struct bm_pool      *pool;
	int	                rx_done, i;
	MV_NETA_RXQ_CTRL    *rx_ctrl = pp-&gt;rxq_ctrl[rxq].q;

	if (rx_ctrl == NULL)
		return;

	rx_done = mvNetaRxqBusyDescNumGet(pp-&gt;port, rxq);
	mvOsCacheIoSync();

	for (i = 0; i &lt; rx_done; i++) {
		rx_desc = mvNetaRxqNextDescGet(rx_ctrl);
		mvOsCacheLineInv(NULL, rx_desc);

#if defined(MV_CPU_BE)
		mvNetaRxqDescSwap(rx_desc);
#endif /* MV_CPU_BE */

		pkt = (struct eth_pbuf *)rx_desc-&gt;bufCookie;
		pool = &amp;mv_eth_pool[pkt-&gt;pool];
		mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
	}
	if (rx_done) {
		mvOsCacheIoSync();
		mvNetaRxqDescNumUpdate(pp-&gt;port, rxq, rx_done, rx_done);
	}
}

static void mv_eth_txq_done_force(struct eth_port *pp, struct tx_queue *txq_ctrl)
{
	int tx_done = txq_ctrl-&gt;txq_count;

	mv_eth_txq_bufs_free(pp, txq_ctrl, tx_done);

	STAT_DBG(txq_ctrl-&gt;stats.txq_txdone += tx_done);

	/* reset txq */
	txq_ctrl-&gt;txq_count = 0;
	txq_ctrl-&gt;shadow_txq_put_i = 0;
	txq_ctrl-&gt;shadow_txq_get_i = 0;
}

inline u32 mv_eth_tx_done_pon(struct eth_port *pp, int *tx_todo)
{
	int txp, txq;
	struct tx_queue *txq_ctrl;
	u32 tx_done = 0;

	*tx_todo = 0;

	STAT_INFO(pp-&gt;stats.tx_done++);

	/* simply go over all TX ports and TX queues */
	txp = pp-&gt;txp_num;
	while (txp--) {
		txq = CONFIG_MV_ETH_TXQ;

		while (txq--) {
			txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
			spin_lock(&amp;txq_ctrl-&gt;queue_lock);
			if ((txq_ctrl) &amp;&amp; (txq_ctrl-&gt;txq_count)) {
				tx_done += mv_eth_txq_done(pp, txq_ctrl);
				*tx_todo += txq_ctrl-&gt;txq_count;
			}
			spin_unlock(&amp;txq_ctrl-&gt;queue_lock);
		}
	}

	STAT_DIST(if (tx_done &lt; pp-&gt;dist_stats.tx_done_dist_size)
			pp-&gt;dist_stats.tx_done_dist[tx_done]++);

	return tx_done;
}


inline u32 mv_eth_tx_done_gbe(struct eth_port *pp, u32 cause_tx_done, int *tx_todo)
{
	int txp, txq;
	struct tx_queue *txq_ctrl;
	u32 tx_done = 0;

	*tx_todo = 0;

	STAT_INFO(pp-&gt;stats.tx_done++);

	while (cause_tx_done != 0) {
		/* For GbE ports we get TX Buffers Threshold Cross per queue in bits [7:0] */
		txp = pp-&gt;txp_num; /* 1 for GbE ports */
		while (txp--) {
			txq = mv_eth_tx_done_policy(cause_tx_done);
			if (txq == -1)
				break;

			txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
			spin_lock(&amp;txq_ctrl-&gt;queue_lock);
			if ((txq_ctrl) &amp;&amp; (txq_ctrl-&gt;txq_count)) {
				tx_done += mv_eth_txq_done(pp, txq_ctrl);
				*tx_todo += txq_ctrl-&gt;txq_count;
			}
			spin_unlock(&amp;txq_ctrl-&gt;queue_lock);

			cause_tx_done &amp;= ~((1 &lt;&lt; txq) &lt;&lt; NETA_CAUSE_TXQ_SENT_DESC_OFFS);
		}
	}

	STAT_DIST(if (tx_done &lt; pp-&gt;dist_stats.tx_done_dist_size)
			pp-&gt;dist_stats.tx_done_dist[tx_done]++);

	return tx_done;
}


static void mv_eth_tx_frag_process(struct eth_port *pp, struct sk_buff *skb, struct tx_queue *txq_ctrl,	u16 flags)
{
	int i;
	struct neta_tx_desc *tx_desc;

	for (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++) {
		skb_frag_t *frag = &amp;skb_shinfo(skb)-&gt;frags[i];

		tx_desc = mvNetaTxqNextDescGet(txq_ctrl-&gt;q);

		/* NETA_TX_BM_ENABLE_MASK = 0 */
		/* NETA_TX_PKT_OFFSET_MASK = 0 */
		tx_desc-&gt;dataSize = frag-&gt;size;
		tx_desc-&gt;bufPhysAddr = mvOsCacheFlush(NULL, page_address(frag-&gt;page) + frag-&gt;page_offset,
						      tx_desc-&gt;dataSize);

		if (i == (skb_shinfo(skb)-&gt;nr_frags - 1)) {
			/* Last descriptor */
			if (flags &amp; MV_ETH_F_NO_PAD)
				tx_desc-&gt;command = NETA_TX_L_DESC_MASK;
			else
				tx_desc-&gt;command = (NETA_TX_L_DESC_MASK | NETA_TX_Z_PAD_MASK);

			txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = ((MV_ULONG) skb | MV_ETH_SHADOW_SKB);
			mv_eth_shadow_inc_put(txq_ctrl);
		} else {
			/* Descriptor in the middle: Not First, Not Last */
			tx_desc-&gt;command = 0;

			txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = 0;
			mv_eth_shadow_inc_put(txq_ctrl);
		}

		mv_eth_tx_desc_flush(tx_desc);
	}
}


/* Free "num" buffers from the pool */
static int mv_eth_pool_free(int pool, int num)
{
	struct eth_pbuf *pkt;
	int i = 0;
	struct bm_pool *ppool = &amp;mv_eth_pool[pool];
	unsigned long flags = 0;
	bool free_all = false;

	MV_ETH_LOCK(&amp;ppool-&gt;lock, flags);

	if (num &gt;= ppool-&gt;buf_num) {
		/* Free all buffers from the pool */
		free_all = true;
		num = ppool-&gt;buf_num;
	}

#ifdef CONFIG_MV_ETH_BM_CPU
	if (mv_eth_pool_bm(ppool)) {

		if (free_all)
			mvBmConfigSet(MV_BM_EMPTY_LIMIT_MASK);

		while (i &lt; num) {
			MV_U32 *va;
			MV_U32 pa = mvBmPoolGet(pool);

			if (pa == 0)
				break;

			va = phys_to_virt(pa);
			pkt = (struct eth_pbuf *)*va;
#if !defined(CONFIG_MV_ETH_BE_WA)
			pkt = (struct eth_pbuf *)MV_32BIT_LE((MV_U32)pkt);
#endif /* !CONFIG_MV_ETH_BE_WA */

			if (pkt) {
				mv_eth_pkt_free(pkt);
#ifdef ETH_SKB_DEBUG
				mv_eth_skb_check((struct sk_buff *)pkt-&gt;osInfo);
#endif /* ETH_SKB_DEBUG */
			}
			i++;
		}
		printk(KERN_ERR "bm pool #%d: pkt_size=%d, buf_size=%d - %d of %d buffers free\n",
			pool, ppool-&gt;pkt_size, RX_BUF_SIZE(ppool-&gt;pkt_size), i, num);

		if (free_all)
			mvBmConfigClear(MV_BM_EMPTY_LIMIT_MASK);
	}
#endif /* CONFIG_MV_ETH_BM_CPU */

	ppool-&gt;buf_num -= num;

	/* Free buffers from the pool stack too */
	if (free_all)
		num = mvStackIndex(ppool-&gt;stack);
	else if (mv_eth_pool_bm(ppool))
		num = 0;

	i = 0;
	while (i &lt; num) {
		/* sanity check */
		if (mvStackIndex(ppool-&gt;stack) == 0) {
			printk(KERN_ERR "%s: No more buffers in the stack\n", __func__);
			break;
		}
		pkt = (struct eth_pbuf *)mvStackPop(ppool-&gt;stack);
		if (pkt) {
			mv_eth_pkt_free(pkt);
#ifdef ETH_SKB_DEBUG
			mv_eth_skb_check((struct sk_buff *)pkt-&gt;osInfo);
#endif /* ETH_SKB_DEBUG */
		}
		i++;
	}
	if (i &gt; 0)
		printk(KERN_ERR "stack pool #%d: pkt_size=%d, buf_size=%d - %d of %d buffers free\n",
			pool, ppool-&gt;pkt_size, RX_BUF_SIZE(ppool-&gt;pkt_size), i, num);

	MV_ETH_UNLOCK(&amp;ppool-&gt;lock, flags);

	return i;
}


static int mv_eth_pool_destroy(int pool)
{
	int num, status = 0;
	struct bm_pool *ppool = &amp;mv_eth_pool[pool];

	num = mv_eth_pool_free(pool, ppool-&gt;buf_num);
	if (num != ppool-&gt;buf_num) {
		printk(KERN_ERR "Warning: could not free all buffers in pool %d while destroying pool\n", pool);
		return MV_ERROR;
	}

	status = mvStackDelete(ppool-&gt;stack);

#ifdef CONFIG_MV_ETH_BM_CPU
	mvBmPoolDisable(pool);

	/* Note: we don't free the bm_pool here ! */
	if (ppool-&gt;bm_pool)
		mvOsFree(ppool-&gt;bm_pool);
#endif /* CONFIG_MV_ETH_BM_CPU */

	memset(ppool, 0, sizeof(struct bm_pool));

	return status;
}


static int mv_eth_pool_add(int pool, int buf_num)
{
	struct bm_pool *bm_pool;
	struct sk_buff *skb;
	struct eth_pbuf *pkt;
	int i;
	unsigned long flags = 0;

	if ((pool &lt; 0) || (pool &gt;= MV_ETH_BM_POOLS)) {
		printk(KERN_ERR "%s: invalid pool number %d\n", __func__, pool);
		return 0;
	}

	bm_pool = &amp;mv_eth_pool[pool];

	/* Check buffer size */
	if (bm_pool-&gt;pkt_size == 0) {
		printk(KERN_ERR "%s: invalid pool #%d state: pkt_size=%d, buf_size=%d, buf_num=%d\n",
		       __func__, pool, bm_pool-&gt;pkt_size, RX_BUF_SIZE(bm_pool-&gt;pkt_size), bm_pool-&gt;buf_num);
		return 0;
	}

	/* Insure buf_num is smaller than capacity */
	if ((buf_num &lt; 0) || ((buf_num + bm_pool-&gt;buf_num) &gt; (bm_pool-&gt;capacity))) {

		printk(KERN_ERR "%s: can't add %d buffers into bm_pool=%d: capacity=%d, buf_num=%d\n",
		       __func__, buf_num, pool, bm_pool-&gt;capacity, bm_pool-&gt;buf_num);
		return 0;
	}

	MV_ETH_LOCK(&amp;bm_pool-&gt;lock, flags);

	for (i = 0; i &lt; buf_num; i++) {
		pkt = mvOsMalloc(sizeof(struct eth_pbuf));
		if (!pkt) {
			printk(KERN_ERR "%s: can't allocate %d bytes\n", __func__, sizeof(struct eth_pbuf));
			break;
		}

		skb = mv_eth_skb_alloc(bm_pool, pkt);
		if (!skb) {
			kfree(pkt);
			break;
		}
/*
	printk(KERN_ERR "skb_alloc_%d: pool=%d, skb=%p, pkt=%p, head=%p (%lx), skb-&gt;truesize=%d\n",
				i, bm_pool-&gt;pool, skb, pkt, pkt-&gt;pBuf, pkt-&gt;physAddr, skb-&gt;truesize);
*/

#ifdef CONFIG_MV_ETH_BM_CPU
		mvBmPoolPut(pool, (MV_ULONG) pkt-&gt;physAddr);
		STAT_DBG(bm_pool-&gt;stats.bm_put++);
#else
		mvStackPush(bm_pool-&gt;stack, (MV_U32) pkt);
		STAT_DBG(bm_pool-&gt;stats.stack_put++);
#endif /* CONFIG_MV_ETH_BM_CPU */
	}
	bm_pool-&gt;buf_num += i;

	printk(KERN_ERR "pool #%d: pkt_size=%d, buf_size=%d - %d of %d buffers added\n",
	       pool, bm_pool-&gt;pkt_size, RX_BUF_SIZE(bm_pool-&gt;pkt_size), i, buf_num);

	MV_ETH_UNLOCK(&amp;bm_pool-&gt;lock, flags);

	return i;
}

#ifdef CONFIG_MV_ETH_BM
void	*mv_eth_bm_pool_create(int pool, int capacity, MV_ULONG *pPhysAddr)
{
		MV_ULONG			physAddr;
		MV_UNIT_WIN_INFO	winInfo;
		void				*pVirt;
		MV_STATUS			status;

		pVirt = mvOsIoUncachedMalloc(NULL, sizeof(MV_U32) * capacity, &amp;physAddr, NULL);
		if (pVirt == NULL) {
			mvOsPrintf("%s: Can't allocate %d bytes for Long pool #%d\n",
					__func__, MV_BM_POOL_CAP_MAX * sizeof(MV_U32), pool);
			return NULL;
		}

		/* Pool address must be MV_BM_POOL_PTR_ALIGN bytes aligned */
		if (MV_IS_NOT_ALIGN((unsigned)pVirt, MV_BM_POOL_PTR_ALIGN)) {
			mvOsPrintf("memory allocated for BM pool #%d is not %d bytes aligned\n",
						pool, MV_BM_POOL_PTR_ALIGN);
			mvOsIoCachedFree(NULL, sizeof(MV_U32) * capacity, physAddr, pVirt, 0);
			return NULL;
		}
		status = mvBmPoolInit(pool, pVirt, physAddr, capacity);
		if (status != MV_OK) {
			mvOsPrintf("%s: Can't init #%d BM pool. status=%d\n", __func__, pool, status);
			mvOsIoCachedFree(NULL, sizeof(MV_U32) * capacity, physAddr, pVirt, 0);
			return NULL;
		}
		status = mvCtrlAddrWinInfoGet(&amp;winInfo, physAddr);
		if (status != MV_OK) {
			printk(KERN_ERR "%s: Can't map BM pool #%d. phys_addr=0x%x, status=%d\n",
			       __func__, pool, (unsigned)physAddr, status);
			mvOsIoCachedFree(NULL, sizeof(MV_U32) * capacity, physAddr, pVirt, 0);
			return NULL;
		}
		mvBmPoolTargetSet(pool, winInfo.targetId, winInfo.attrib);
		mvBmPoolEnable(pool);

		if (pPhysAddr != NULL)
			*pPhysAddr = physAddr;

		return pVirt;
}
#endif /* CONFIG_MV_ETH_BM */

static MV_STATUS mv_eth_pool_create(int pool, int capacity)
{
	struct bm_pool *bm_pool;

	if ((pool &lt; 0) || (pool &gt;= MV_ETH_BM_POOLS)) {
		printk(KERN_ERR "%s: pool=%d is out of range\n", __func__, pool);
		return MV_BAD_VALUE;
	}

	bm_pool = &amp;mv_eth_pool[pool];
	memset(bm_pool, 0, sizeof(struct bm_pool));

#ifdef CONFIG_MV_ETH_BM_CPU
	bm_pool-&gt;bm_pool = mv_eth_bm_pool_create(pool, capacity, NULL);
	if (bm_pool-&gt;bm_pool == NULL)
		return MV_FAIL;
#endif /* CONFIG_MV_ETH_BM_CPU */

	/* Create Stack as container of alloacted skbs for SKB_RECYCLE and for RXQs working without BM support */
	bm_pool-&gt;stack = mvStackCreate(capacity);

	if (bm_pool-&gt;stack == NULL) {
		printk(KERN_ERR "Can't create MV_STACK structure for %d elements\n", capacity);
		return MV_OUT_OF_CPU_MEM;
	}

	bm_pool-&gt;pool = pool;
	bm_pool-&gt;capacity = capacity;
	bm_pool-&gt;pkt_size = 0;
	bm_pool-&gt;buf_num = 0;
	spin_lock_init(&amp;bm_pool-&gt;lock);

	return MV_OK;
}

/* Interrupt handling */
irqreturn_t mv_eth_isr(int irq, void *dev_id)
{
	struct eth_port *pp = (struct eth_port *)dev_id;
	struct napi_struct *napi = pp-&gt;napi[smp_processor_id()];

#ifdef CONFIG_MV_ETH_DEBUG_CODE
	if (pp-&gt;flags &amp; MV_ETH_F_DBG_ISR) {
		printk(KERN_ERR "%s: port=%d, cpu=%d, mask=0x%x, cause=0x%x\n",
			__func__, pp-&gt;port, smp_processor_id(),
			MV_REG_READ(NETA_INTR_NEW_MASK_REG(pp-&gt;port)), MV_REG_READ(NETA_INTR_NEW_CAUSE_REG(pp-&gt;port)));
	}
#endif /* CONFIG_MV_ETH_DEBUG_CODE */

	STAT_INFO(pp-&gt;stats.irq++);

	/* Mask all interrupts */
	MV_REG_WRITE(NETA_INTR_NEW_MASK_REG(pp-&gt;port), 0);
	/* To be sure that itterrupt already masked Dummy read is required */
	/* MV_REG_READ(NETA_INTR_NEW_MASK_REG(pp-&gt;port));*/

	/* Verify that the device not already on the polling list */
	if (napi_schedule_prep(napi)) {
		/* schedule the work (rx+txdone+link) out of interrupt contxet */
		__napi_schedule(napi);
	} else {
		STAT_INFO(pp-&gt;stats.irq_err++);
#ifdef CONFIG_MV_ETH_DEBUG_CODE
		printk(KERN_ERR "mv_eth_isr ERROR: port=%d, cpu=%d\n", pp-&gt;port, smp_processor_id());
#endif /* CONFIG_MV_ETH_DEBUG_CODE */
	}
	return IRQ_HANDLED;
}

void mv_eth_link_event(struct eth_port *pp, int print)
{
	struct net_device *dev = pp-&gt;dev;
	bool              link_is_up;

	STAT_INFO(pp-&gt;stats.link++);

	/* Check Link status on ethernet port */
#ifdef CONFIG_MV_PON
	if (MV_PON_PORT(pp-&gt;port))
		link_is_up = mv_pon_link_status();
	else
#endif /* CONFIG_MV_PON */
		link_is_up = mvNetaLinkIsUp(pp-&gt;port);

	if (link_is_up) {
		mvNetaPortUp(pp-&gt;port);
		set_bit(MV_ETH_F_LINK_UP_BIT, &amp;(pp-&gt;flags));

		if (mv_eth_ctrl_is_tx_enabled(pp)) {
			if (dev) {
				netif_carrier_on(dev);
				netif_wake_queue(dev);
			}
		}
	} else {
		if (dev) {
			netif_carrier_off(dev);
			netif_stop_queue(dev);
		}
		mvNetaPortDown(pp-&gt;port);
		clear_bit(MV_ETH_F_LINK_UP_BIT, &amp;(pp-&gt;flags));
	}

	if (print) {
		if (dev)
			printk(KERN_ERR "%s: ", dev-&gt;name);
		else
			printk(KERN_ERR "%s: ", "none");

		mv_eth_link_status_print(pp-&gt;port);
	}
}

/***********************************************************************************************/
int mv_eth_poll(struct napi_struct *napi, int budget)
{
	int rx_done = 0;
	MV_U32 causeRxTx;
	struct eth_port *pp = MV_ETH_PRIV(napi-&gt;dev);

#ifdef CONFIG_MV_ETH_DEBUG_CODE
	if (pp-&gt;flags &amp; MV_ETH_F_DBG_POLL) {
		printk(KERN_ERR "%s ENTER: port=%d, cpu=%d, mask=0x%x, cause=0x%x\n",
			__func__, pp-&gt;port, smp_processor_id(),
			MV_REG_READ(NETA_INTR_NEW_MASK_REG(pp-&gt;port)), MV_REG_READ(NETA_INTR_NEW_CAUSE_REG(pp-&gt;port)));
	}
#endif /* CONFIG_MV_ETH_DEBUG_CODE */

	read_lock(&amp;pp-&gt;rwlock);

	STAT_INFO(pp-&gt;stats.poll[smp_processor_id()]++);

	/* Read cause register */
	causeRxTx = MV_REG_READ(NETA_INTR_NEW_CAUSE_REG(pp-&gt;port)) &amp;
	    (MV_ETH_MISC_SUM_INTR_MASK | MV_ETH_TXDONE_INTR_MASK | MV_ETH_RX_INTR_MASK);

	if (causeRxTx &amp; MV_ETH_MISC_SUM_INTR_MASK) {
		MV_U32 causeMisc;

		/* Process MISC events - Link, etc ??? */
		causeRxTx &amp;= ~MV_ETH_MISC_SUM_INTR_MASK;
		causeMisc = MV_REG_READ(NETA_INTR_MISC_CAUSE_REG(pp-&gt;port));

		if (causeMisc &amp; NETA_CAUSE_LINK_CHANGE_MASK)
			mv_eth_link_event(pp, 1);

		MV_REG_WRITE(NETA_INTR_MISC_CAUSE_REG(pp-&gt;port), 0);
	}
	causeRxTx |= pp-&gt;causeRxTx[smp_processor_id()];

#ifdef CONFIG_MV_ETH_TXDONE_ISR
	if (causeRxTx &amp; MV_ETH_TXDONE_INTR_MASK) {
		int tx_todo = 0;
		/* TX_DONE process */

		if (MV_PON_PORT(pp-&gt;port))
			mv_eth_tx_done_pon(pp, &amp;tx_todo);
		else
			mv_eth_tx_done_gbe(pp, (causeRxTx &amp; MV_ETH_TXDONE_INTR_MASK), &amp;tx_todo);

		causeRxTx &amp;= ~MV_ETH_TXDONE_INTR_MASK;
	}
#endif /* CONFIG_MV_ETH_TXDONE_ISR */

#if (CONFIG_MV_ETH_RXQ &gt; 1)
	while ((causeRxTx != 0) &amp;&amp; (budget &gt; 0)) {
		int count, rx_queue;

		rx_queue = mv_eth_rx_policy(causeRxTx);
		if (rx_queue == -1)
			break;

		count = mv_eth_rx(pp, budget, rx_queue);
		rx_done += count;
		budget -= count;
		if (budget &gt; 0)
			causeRxTx &amp;= ~((1 &lt;&lt; rx_queue) &lt;&lt; NETA_CAUSE_RXQ_OCCUP_DESC_OFFS);
	}
#else
	rx_done = mv_eth_rx(pp, budget, CONFIG_MV_ETH_RXQ_DEF);
	budget -= rx_done;
#endif /* (CONFIG_MV_ETH_RXQ &gt; 1) */

	STAT_DIST(if (rx_done &lt; pp-&gt;dist_stats.rx_dist_size)
			pp-&gt;dist_stats.rx_dist[rx_done]++);

#ifdef CONFIG_MV_ETH_DEBUG_CODE
	if (pp-&gt;flags &amp; MV_ETH_F_DBG_POLL) {
		printk(KERN_ERR "%s  EXIT: port=%d, cpu=%d, budget=%d, rx_done=%d\n",
			__func__, pp-&gt;port, smp_processor_id(), budget, rx_done);
	}
#endif /* CONFIG_MV_ETH_DEBUG_CODE */

	if (budget &gt; 0) {
		unsigned long flags;

		causeRxTx = 0;

		napi_complete(napi);
		STAT_INFO(pp-&gt;stats.poll_exit[smp_processor_id()]++);

		local_irq_save(flags);
		MV_REG_WRITE(NETA_INTR_NEW_MASK_REG(pp-&gt;port),
			     (MV_ETH_MISC_SUM_INTR_MASK | MV_ETH_TXDONE_INTR_MASK | MV_ETH_RX_INTR_MASK));

		local_irq_restore(flags);
	}
	pp-&gt;causeRxTx[smp_processor_id()] = causeRxTx;
	read_unlock(&amp;pp-&gt;rwlock);
	return rx_done;
}

static void mv_eth_cpu_counters_init(void)
{
#ifdef CONFIG_MV_CPU_PERF_CNTRS

	mvCpuCntrsInitialize();

#ifdef CONFIG_PLAT_ARMADA
	/*  cycles counter via special CCNT counter */
	mvCpuCntrsProgram(0, MV_CPU_CNTRS_CYCLES, "Cycles", 13);

	/* instruction counters */
	mvCpuCntrsProgram(1, MV_CPU_CNTRS_INSTRUCTIONS, "Instr", 13);
	/* mvCpuCntrsProgram(0, MV_CPU_CNTRS_DCACHE_READ_HIT, "DcRdHit", 0); */

	/* ICache misses counter */
	mvCpuCntrsProgram(2, MV_CPU_CNTRS_ICACHE_READ_MISS, "IcMiss", 0);

	/* DCache read misses counter */
	mvCpuCntrsProgram(3, MV_CPU_CNTRS_DCACHE_READ_MISS, "DcRdMiss", 0);

	/* DCache write misses counter */
	mvCpuCntrsProgram(4, MV_CPU_CNTRS_DCACHE_WRITE_MISS, "DcWrMiss", 0);

	/* DTLB Miss counter */
	mvCpuCntrsProgram(5, MV_CPU_CNTRS_DTLB_MISS, "dTlbMiss", 0);

	/* mvCpuCntrsProgram(3, MV_CPU_CNTRS_TLB_MISS, "TlbMiss", 0); */
#else /* CONFIG_FEROCEON */
	/* 0 - instruction counters */
	mvCpuCntrsProgram(0, MV_CPU_CNTRS_INSTRUCTIONS, "Instr", 16);
	/* mvCpuCntrsProgram(0, MV_CPU_CNTRS_DCACHE_READ_HIT, "DcRdHit", 0); */

	/* 1 - ICache misses counter */
	mvCpuCntrsProgram(1, MV_CPU_CNTRS_ICACHE_READ_MISS, "IcMiss", 0);

	/* 2 - cycles counter */
	mvCpuCntrsProgram(2, MV_CPU_CNTRS_CYCLES, "Cycles", 18);

	/* 3 - DCache read misses counter */
	mvCpuCntrsProgram(3, MV_CPU_CNTRS_DCACHE_READ_MISS, "DcRdMiss", 0);
	/* mvCpuCntrsProgram(3, MV_CPU_CNTRS_TLB_MISS, "TlbMiss", 0); */
#endif /* CONFIG_PLAT_ARMADA */

	event0 = mvCpuCntrsEventCreate("RX_DESC_PREF", 100000);
	event1 = mvCpuCntrsEventCreate("RX_DESC_READ", 100000);
	event2 = mvCpuCntrsEventCreate("RX_BUF_INV", 100000);
	event3 = mvCpuCntrsEventCreate("RX_DESC_FILL", 100000);
	event4 = mvCpuCntrsEventCreate("TX_START", 100000);
	event5 = mvCpuCntrsEventCreate("RX_BUF_INV", 100000);
	if ((event0 == NULL) || (event1 == NULL) || (event2 == NULL) ||
		(event3 == NULL) || (event4 == NULL) || (event5 == NULL))
		printk(KERN_ERR "Can't create cpu counter events\n");
#endif /* CONFIG_MV_CPU_PERF_CNTRS */
}

static void mv_eth_port_promisc_set(int port, int queue)
{
#ifdef CONFIG_MV_ETH_PNC
	/* Accept all */
	if (mv_eth_pnc_ctrl_en) {
		pnc_mac_me(port, NULL, queue);
		pnc_mcast_all(port, 1);
	} else {
		printk(KERN_ERR "%s: PNC control is disabled\n", __func__);
	}
#else /* Legacy parser */
	mvNetaRxUnicastPromiscSet(port, MV_TRUE);
	mvNetaSetUcastTable(port, queue);
	mvNetaSetSpecialMcastTable(port, queue);
	mvNetaSetOtherMcastTable(port, queue);
#endif /* CONFIG_MV_ETH_PNC */
}

void mv_eth_port_filtering_cleanup(int port)
{
#ifdef CONFIG_MV_ETH_PNC
	static bool is_first = true;

	/* clean TCAM only one, no need to do this per port. */
	if (is_first) {
		tcam_hw_init();
		is_first = false;
	}
#else
	mvNetaRxUnicastPromiscSet(port, MV_FALSE);
	mvNetaSetUcastTable(port, -1);
	mvNetaSetSpecialMcastTable(port, -1);
	mvNetaSetOtherMcastTable(port, -1);
#endif /* CONFIG_MV_ETH_PNC */
}


static MV_STATUS mv_eth_bm_pools_init(void)
{
	int i, j;
	MV_STATUS status;

	/* Get compile time configuration */
#ifdef CONFIG_MV_ETH_BM
	mvBmControl(MV_START);
	mv_eth_bm_config_get();
#endif /* CONFIG_MV_ETH_BM */

	/* Create all pools with maximum capacity */
	for (i = 0; i &lt; MV_ETH_BM_POOLS; i++) {
		status = mv_eth_pool_create(i, MV_BM_POOL_CAP_MAX);
		if (status != MV_OK) {
			printk(KERN_ERR "%s: can't create bm_pool=%d - capacity=%d\n", __func__, i, MV_BM_POOL_CAP_MAX);
			for (j = 0; j &lt; i; j++)
				mv_eth_pool_destroy(j);
			return status;
		}
#ifdef CONFIG_MV_ETH_BM_CPU
		mv_eth_pool[i].pkt_size = mv_eth_bm_config_pkt_size_get(i);
		if (mv_eth_pool[i].pkt_size == 0)
			mvBmPoolBufSizeSet(i, 0);
		else
			mvBmPoolBufSizeSet(i, RX_BUF_SIZE(mv_eth_pool[i].pkt_size));
#else
		mv_eth_pool[i].pkt_size = 0;
#endif /* CONFIG_MV_ETH_BM */
	}
	return 0;
}

/* Note: call this function only after mv_eth_ports_num is initialized */
static int mv_eth_load_network_interfaces(MV_U32 portMask, MV_U32 cpuMask)
{
	u32 port, dev_i = 0;
	struct eth_port *pp;
	int mtu, err;
	u8 mac[MV_MAC_ADDR_SIZE];

	printk(KERN_ERR "  o Loading network interface(s)\n");

	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		if (!(MV_BIT_CHECK(portMask, port)))
			continue;

		if (!mvCtrlPwrClckGet(ETH_GIG_UNIT_ID, port)) {
			printk(KERN_ERR "\n  o Warning: GbE port %d is powered off\n\n", port);
			continue;
		}
		if (!MV_PON_PORT(port) &amp;&amp; !mvBoardIsGbEPortConnected(port)) {
			printk(KERN_ERR "\n  o Warning: GbE port %d is not connected to PHY/RGMII/Switch, skip initialization\n\n",
					port);
			continue;
		}

		pp = mv_eth_ports[port] = mvOsMalloc(sizeof(struct eth_port));
		if (!pp) {
			printk(KERN_ERR "Error: failed to allocate memory for port %d\n", port);
			return -ENOMEM;
		}

		err = mv_eth_priv_init(pp, port);
		if (err)
			return err;

		pp-&gt;cpuMask = cpuMask;

#ifdef CONFIG_MV_ETH_PMT
		if (MV_PON_PORT(port))
			mvNetaPmtInit(port, (MV_NETA_PMT *)ioremap(PMT_PON_PHYS_BASE, PMT_MEM_SIZE));
		else
			mvNetaPmtInit(port, (MV_NETA_PMT *)ioremap(PMT_GIGA_PHYS_BASE + port * 0x40000, PMT_MEM_SIZE));
#endif /* CONFIG_MV_ETH_PMT */

#ifdef CONFIG_MV_ETH_SWITCH
		if (pp-&gt;flags &amp; (MV_ETH_F_SWITCH | MV_ETH_F_EXT_SWITCH)) {
			int status = mv_eth_switch_config_get(mv_eth_initialized, port);

			if (status &lt; 0) {
				printk(KERN_ERR "\nWarning: port %d - Invalid netconfig string\n", port);
				mv_eth_priv_cleanup(pp);
				continue;
			} else if (status == 0) {	/* User selected to work with Gateway driver    */
				clear_bit(MV_ETH_F_EXT_SWITCH_BIT, &amp;(pp-&gt;flags));
			} else if (status == 1) {
				/* User selected to work without Gateway driver */
				clear_bit(MV_ETH_F_SWITCH_BIT, &amp;(pp-&gt;flags));
				printk(KERN_ERR "  o Working in External Switch mode\n");
				ext_switch_port_mask = mv_switch_link_detection_init();
			}
		}

		if (pp-&gt;flags &amp; MV_ETH_F_SWITCH) {
			set_bit(MV_ETH_F_MH_BIT, &amp;(pp-&gt;flags));
			mtu = switch_net_config[port].mtu;
			if (mv_switch_init(RX_PKT_SIZE(mtu), SWITCH_CONNECTED_PORTS_MASK)) {
				printk(KERN_ERR "\nWarning: port %d - Switch initialization failed\n", port);
				mv_eth_priv_cleanup(pp);
				continue;
			}
		} else
#endif /* CONFIG_MV_ETH_SWITCH */
			mtu = mv_eth_config_get(pp, mac);

		printk(KERN_ERR "\t%s p=%d: mtu=%d, mac=%p\n", MV_PON_PORT(port) ? "pon" : "giga", port, mtu, mac);

		if (mv_eth_hal_init(pp)) {
			printk(KERN_ERR "%s: can't init eth hal\n", __func__);
			mv_eth_priv_cleanup(pp);
			return -EIO;
		}
#ifdef CONFIG_MV_ETH_SWITCH
		if (pp-&gt;flags &amp; MV_ETH_F_SWITCH) {
			int queue = CONFIG_MV_ETH_RXQ_DEF;

			mv_eth_switch_netdev_first = dev_i;
			dev_i = mv_eth_switch_netdev_init(pp, dev_i);
			if (dev_i &lt; (mv_eth_switch_netdev_first + switch_net_config[port].netdev_max)) {
				printk(KERN_ERR "%s: can't create netdevice for switch\n", __func__);
				mv_eth_priv_cleanup(pp);
				return -EIO;
			}
			mv_eth_switch_netdev_last = dev_i - 1;

			/* set this port to be in promiscuous mode. MAC filtering is performed by the Switch */
			mv_eth_port_promisc_set(pp-&gt;port, queue);
			handle_group_affinity(port);
			continue;
		}
#endif /* CONFIG_MV_ETH_SWITCH */

		mv_net_devs[dev_i] = mv_eth_netdev_init(pp, mtu, mac);
		if (!mv_net_devs[dev_i]) {
			printk(KERN_ERR "%s: can't create netdevice\n", __func__);
			mv_eth_priv_cleanup(pp);
			return -EIO;
		}
		pp-&gt;dev = mv_net_devs[dev_i];
		dev_i++;
		handle_group_affinity(port);
	}

	mv_net_devs_num = dev_i;

	return 0;
}


/***********************************************************
 * mv_eth_probe --                                         *
 *   main driver initialization. loading the interfaces.   *
 ***********************************************************/
static int mv_eth_probe(struct platform_device *pdev)
{
	u32 port;
	struct eth_port *pp;
	int size;
	MV_U32 port_mask, cpu_mask;

	if (pdev-&gt;dev.platform_data) {
		port_mask = ((struct netaSmpGroupStruct *)pdev-&gt;dev.platform_data)-&gt;portMask;
		cpu_mask =  ((struct netaSmpGroupStruct *)pdev-&gt;dev.platform_data)-&gt;cpuMask;
	} else {
		port_mask = (1 &lt;&lt; CONFIG_MV_ETH_PORTS_NUM) - 1;
		cpu_mask = (1 &lt;&lt; CONFIG_NR_CPUS) - 1;
	}
	printk(KERN_INFO "%s: port_mask=0x%x, cpu_mask=0x%x \n", __func__, port_mask, cpu_mask);

#ifdef ETH_SKB_DEBUG
	memset(mv_eth_skb_debug, 0, sizeof(mv_eth_skb_debug));
	spin_lock_init(&amp;skb_debug_lock);
#endif

	if (!mv_eth_initialized) {
		mvSysNetaInit(port_mask, cpu_mask); /* init MAC Unit */

		mv_eth_ports_num = mvCtrlEthMaxPortGet();
		if (mv_eth_ports_num &gt; CONFIG_MV_ETH_PORTS_NUM)
			mv_eth_ports_num = CONFIG_MV_ETH_PORTS_NUM;

		mv_net_devs_max = mv_eth_ports_num;

#ifdef CONFIG_MV_ETH_SWITCH
		mv_net_devs_max += (CONFIG_MV_ETH_SWITCH_NETDEV_NUM - 1);
#endif /* CONFIG_MV_ETH_SWITCH */

		mv_eth_config_show();

		size = mv_eth_ports_num * sizeof(struct eth_port *);
		mv_eth_ports = mvOsMalloc(size);
		if (!mv_eth_ports)
			goto oom;

		memset(mv_eth_ports, 0, size);

		/* Allocate array of pointers to struct net_device */
		size = mv_net_devs_max * sizeof(struct net_device *);
		mv_net_devs = mvOsMalloc(size);
		if (!mv_net_devs)
			goto oom;

		memset(mv_net_devs, 0, size);
	}

#ifdef CONFIG_MV_ETH_PNC
	if (mv_eth_pnc_ctrl_en) {
		if (pnc_default_init())
			printk(KERN_ERR "%s: Warning PNC init failed\n", __func__);
	} else
		printk(KERN_ERR "%s: PNC control is disabled\n", __func__);
#endif /* CONFIG_MV_ETH_PNC */

	if (mv_eth_bm_pools_init())
		goto oom;

#ifdef CONFIG_MV_INCLUDE_SWITCH
	if ((mvBoardSwitchConnectedPortGet(0) != -1) || (mvBoardSwitchConnectedPortGet(1) != -1)) {
		if (mv_switch_load(SWITCH_CONNECTED_PORTS_MASK))
			printk(KERN_ERR "\nWarning: Switch load failed\n");
	}
#endif /* CONFIG_MV_INCLUDE_SWITCH */

	if (!mv_eth_initialized) {
		if (mv_eth_load_network_interfaces(port_mask, cpu_mask))
			goto oom;
	}

#ifdef CONFIG_MV_ETH_HWF
	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		if (mv_eth_ports[port])
			mvNetaHwfInit(port);
	}
#endif /* CONFIG_MV_ETH_HWF */

	/* Call mv_eth_open specifically for ports not connected to Linux netdevice */
	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		pp = mv_eth_port_by_id(port);
		if (pp) {
			if (!(pp-&gt;flags &amp; MV_ETH_F_CONNECT_LINUX)) {
				if (pp-&gt;flags &amp; MV_ETH_F_SWITCH)
					printk(KERN_ERR "%s: a GbE port using the Gateway driver cannot be disconnected from Linux\n",
							__func__);
				else
					mv_eth_open(pp-&gt;dev);
			}
		}
	}

	if (!mv_eth_initialized) {
#ifdef CONFIG_MV_ETH_NFP
#ifdef CONFIG_MV_ETH_NFP_LIB
	printk(KERN_INFO "Using NFP lib\n");
#endif /* CONFIG_MV_ETH_NFP_LIB */
	nfp_sysfs_init();
#endif /* CONFIG_MV_ETH_NFP */
		mv_eth_cpu_counters_init();
	}

	printk(KERN_ERR "\n");

	mv_eth_initialized = 1;

	return 0;
oom:
	if (mv_eth_ports)
		mvOsFree(mv_eth_ports);

	if (mv_net_devs)
		mvOsFree(mv_net_devs);

	printk(KERN_ERR "%s: out of memory\n", __func__);
	return -ENOMEM;
}


static int mv_eth_config_get(struct eth_port *pp, MV_U8 *mac_addr)
{
	char *mac_str = NULL;
	u8 zero_mac[MV_MAC_ADDR_SIZE] = { 0x00, 0x00, 0x00, 0x00, 0x00, 0x00 };
	int mtu;

	switch (pp-&gt;port) {
	case 0:
		if (mvMtu[0] != 0)
			mtu = mvMtu[0];
		else
			mtu = CONFIG_MV_ETH_0_MTU;

		/* use default MAC address from Kconfig only if the MAC address we got is all 0 */
		if (memcmp(mvMacAddr[0], zero_mac, MV_MAC_ADDR_SIZE) == 0)
			mac_str = CONFIG_MV_ETH_0_MACADDR;
		else
			memcpy(mac_addr, mvMacAddr[0], MV_MAC_ADDR_SIZE);

		break;

#if (CONFIG_MV_ETH_PORTS_NUM &gt; 1)
	case 1:
		if (mvMtu[1] != 0)
			mtu = mvMtu[1];
		else
			mtu = CONFIG_MV_ETH_1_MTU;

		/* use default MAC address from Kconfig only if the MAC address we got is all 0 */
		if (memcmp(mvMacAddr[1], zero_mac, MV_MAC_ADDR_SIZE) == 0)
			mac_str = CONFIG_MV_ETH_1_MACADDR;
		else
			memcpy(mac_addr, mvMacAddr[1], MV_MAC_ADDR_SIZE);

		break;
#endif /* CONFIG_MV_ETH_PORTS_NUM &gt; 1 */

#if (CONFIG_MV_ETH_PORTS_NUM &gt; 2)
	case 2:
		if (mvMtu[2] != 0)
			mtu = mvMtu[2];
		else
			mtu = CONFIG_MV_ETH_2_MTU;

		/* use default MAC address from Kconfig only if the MAC address we got is all 0 */
		if (memcmp(mvMacAddr[2], zero_mac, MV_MAC_ADDR_SIZE) == 0)
			mac_str = CONFIG_MV_ETH_2_MACADDR;
		else
			memcpy(mac_addr, mvMacAddr[2], MV_MAC_ADDR_SIZE);
		break;
#endif /* CONFIG_MV_ETH_PORTS_NUM &gt; 2 */

#if (CONFIG_MV_ETH_PORTS_NUM &gt; 3)
	case 3:
		if (mvMtu[3] != 0)
			mtu = mvMtu[3];
		else
			mtu = CONFIG_MV_ETH_3_MTU;

		/* use default MAC address from Kconfig only if the MAC address we got is all 0 */
		if (memcmp(mvMacAddr[3], zero_mac, MV_MAC_ADDR_SIZE) == 0)
			mac_str = CONFIG_MV_ETH_3_MACADDR;
		else
			memcpy(mac_addr, mvMacAddr[3], MV_MAC_ADDR_SIZE);

		break;
#endif /* CONFIG_MV_ETH_PORTS_NUM &gt; 3 */

	default:
		printk(KERN_ERR "eth_get_config: Unexpected port number %d\n", pp-&gt;port);
		return -1;
	}
	if ((mac_str != NULL) &amp;&amp; (mac_addr != NULL))
		mvMacStrToHex(mac_str, mac_addr);

	return mtu;
}

/***********************************************************
 * mv_eth_tx_timeout --                                    *
 *   nothing to be done (?)                                *
 ***********************************************************/
static void mv_eth_tx_timeout(struct net_device *dev)
{
#ifdef CONFIG_MV_ETH_STAT_ERR
	struct eth_port *pp = MV_ETH_PRIV(dev);

	pp-&gt;stats.tx_timeout++;
#endif /* #ifdef CONFIG_MV_ETH_STAT_ERR */

	printk(KERN_INFO "%s: tx timeout\n", dev-&gt;name);
}

/***************************************************************
 * mv_eth_netdev_init -- Allocate and initialize net_device    *
 *                   structure                                 *
 ***************************************************************/
struct net_device *mv_eth_netdev_init(struct eth_port *pp, int mtu, u8 *mac)
{
	int cpu, i;
	struct net_device *dev;
	struct eth_dev_priv *dev_priv;

	dev = alloc_etherdev(sizeof(struct eth_dev_priv));
	if (!dev)
		return NULL;

	dev_priv = (struct eth_dev_priv *)netdev_priv(dev);
	if (!dev_priv)
		return NULL;

	memset(dev_priv, 0, sizeof(struct eth_dev_priv));
	dev_priv-&gt;port_p = pp;

	dev-&gt;irq = NET_TH_RXTX_IRQ_NUM(pp-&gt;port);

	dev-&gt;mtu = mtu;
	memcpy(dev-&gt;dev_addr, mac, MV_MAC_ADDR_SIZE);
	dev-&gt;tx_queue_len = CONFIG_MV_ETH_TXQ_DESC;
	dev-&gt;watchdog_timeo = 5 * HZ;

#ifdef CONFIG_MV_ETH_SWITCH
	if (pp-&gt;flags &amp; (MV_ETH_F_SWITCH | MV_ETH_F_EXT_SWITCH)) {

		if (pp-&gt;flags &amp; MV_ETH_F_SWITCH)
			dev-&gt;netdev_ops = &amp;mv_switch_netdev_ops;
		else
			dev-&gt;netdev_ops = &amp;mv_eth_netdev_ops;

		dev_priv-&gt;netdev_p = mvOsMalloc(sizeof(struct eth_netdev));
		if (!dev_priv-&gt;netdev_p) {
			printk(KERN_ERR "failed to allocate eth_netdev\n");
			free_netdev(dev);
			return NULL;
		}
		memset(dev_priv-&gt;netdev_p, 0, sizeof(struct eth_netdev));
		/* For correct link information of Linux interface: */
		if (pp-&gt;flags &amp; MV_ETH_F_EXT_SWITCH) {
			dev_priv-&gt;netdev_p-&gt;port_map = ext_switch_port_mask;
			dev_priv-&gt;netdev_p-&gt;link_map = 0;
		}
	} else
#endif /* CONFIG_MV_ETH_SWITCH */
		dev-&gt;netdev_ops = &amp;mv_eth_netdev_ops;

#ifdef CONFIG_MV_ETH_TOOL
	SET_ETHTOOL_OPS(dev, &amp;mv_eth_tool_ops);
#endif

	/* Default NAPI initialization */
	for (i = 0; i &lt; CONFIG_MV_ETH_NAPI_GROUPS; i++) {
		pp-&gt;napiGroup[i] = kmalloc(sizeof(struct napi_struct), GFP_KERNEL);
		memset(pp-&gt;napiGroup[i], 0, sizeof(struct napi_struct));
	}

	for (cpu = 0; cpu &lt; CONFIG_NR_CPUS; cpu++) {
		pp-&gt;napiCpuGroup[cpu] = 0;
		pp-&gt;napi[cpu]         = NULL;
		}

	/* Add NAPI default group */
	if (pp-&gt;flags &amp; MV_ETH_F_CONNECT_LINUX) {
		for (i = 0; i &lt; CONFIG_MV_ETH_NAPI_GROUPS; i++)
			netif_napi_add(dev, pp-&gt;napiGroup[i], mv_eth_poll, pp-&gt;weight);
	}

	pp-&gt;tx_done_timer.data = (unsigned long)dev;
	pp-&gt;cleanup_timer.data = (unsigned long)dev;

	if (pp-&gt;flags &amp; MV_ETH_F_CONNECT_LINUX) {
		if (register_netdev(dev)) {
			printk(KERN_ERR "failed to register %s\n", dev-&gt;name);
			free_netdev(dev);
			return NULL;
		} else {
			mv_eth_netdev_set_features(dev);

			printk(KERN_ERR "    o %s, ifindex = %d, GbE port = %d", dev-&gt;name, dev-&gt;ifindex, pp-&gt;port);
#ifdef CONFIG_MV_ETH_SWITCH
			if (!(pp-&gt;flags &amp; MV_ETH_F_SWITCH))
				printk(KERN_CONT "\n");
#else
			printk(KERN_CONT "\n");
#endif
		}
	}
	return dev;
}

bool mv_eth_netdev_find(unsigned int dev_idx)
{
	int i;

	for (i = 0; i &lt; mv_net_devs_num; i++) {
		if (mv_net_devs &amp;&amp; mv_net_devs[i] &amp;&amp; (mv_net_devs[i]-&gt;ifindex == dev_idx))
			return true;
	}
	return false;
}

void mv_eth_netdev_update(int dev_index, struct eth_port *pp)
{
	int i;
	struct eth_dev_priv *dev_priv;
#ifdef CONFIG_MV_ETH_SWITCH
	struct eth_netdev *eth_netdev_priv;
#endif /* CONFIG_MV_ETH_SWITCH */
	struct net_device *dev = mv_net_devs[dev_index];

	dev_priv = (struct eth_dev_priv *)netdev_priv(dev); /* assuming dev_priv has to be valid here */

	dev_priv-&gt;port_p = pp;

	dev-&gt;irq = NET_TH_RXTX_IRQ_NUM(pp-&gt;port);

	if (pp-&gt;flags &amp; MV_ETH_F_CONNECT_LINUX) {
		for (i = 0; i &lt; CONFIG_MV_ETH_NAPI_GROUPS; i++)
			netif_napi_add(dev, pp-&gt;napiGroup[i], mv_eth_poll, pp-&gt;weight);
	}

	pp-&gt;tx_done_timer.data = (unsigned long)dev;
	pp-&gt;cleanup_timer.data = (unsigned long)dev;

	printk(KERN_ERR "    o %s, ifindex = %d, GbE port = %d", dev-&gt;name, dev-&gt;ifindex, pp-&gt;port);

#ifdef CONFIG_MV_ETH_SWITCH
	if (pp-&gt;flags &amp; MV_ETH_F_SWITCH) {
		eth_netdev_priv = MV_DEV_PRIV(dev);
		mv_eth_switch_priv_update(dev, MV_SWITCH_VLAN_TO_GROUP(eth_netdev_priv-&gt;vlan_grp_id));
	} else {
		printk(KERN_CONT "\n");
	}
#else
	printk(KERN_CONT "\n");
#endif /* CONFIG_MV_ETH_SWITCH */
}


int mv_eth_hal_init(struct eth_port *pp)
{
	int rxq, txp, txq, size;
	struct tx_queue *txq_ctrl;
	struct rx_queue *rxq_ctrl;

	if (!MV_PON_PORT(pp-&gt;port)) {
		int phyAddr;

		/* Set the board information regarding PHY address */
		phyAddr = mvBoardPhyAddrGet(pp-&gt;port);
		mvNetaPhyAddrSet(pp-&gt;port, phyAddr);
	}

	/* Init port */
	pp-&gt;port_ctrl = mvNetaPortInit(pp-&gt;port, NULL);
	if (!pp-&gt;port_ctrl) {
		printk(KERN_ERR "%s: failed to load port=%d\n", __func__, pp-&gt;port);
		return -ENODEV;
	}

	size = pp-&gt;txp_num * CONFIG_MV_ETH_TXQ * sizeof(struct tx_queue);
	pp-&gt;txq_ctrl = mvOsMalloc(size);
	if (!pp-&gt;txq_ctrl)
		goto oom;

	memset(pp-&gt;txq_ctrl, 0, size);

	/* Create TX descriptor rings */
	for (txp = 0; txp &lt; pp-&gt;txp_num; txp++) {
		for (txq = 0; txq &lt; CONFIG_MV_ETH_TXQ; txq++) {
			txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];

			txq_ctrl-&gt;q = NULL;
			txq_ctrl-&gt;cpu_owner = 0;
			txq_ctrl-&gt;hwf_rxp = 0xFF;
			txq_ctrl-&gt;txp = txp;
			txq_ctrl-&gt;txq = txq;
			txq_ctrl-&gt;txq_size = CONFIG_MV_ETH_TXQ_DESC;
			txq_ctrl-&gt;txq_count = 0;
			txq_ctrl-&gt;bm_only = MV_FALSE;

			txq_ctrl-&gt;shadow_txq_put_i = 0;
			txq_ctrl-&gt;shadow_txq_get_i = 0;
			txq_ctrl-&gt;txq_done_pkts_coal = mv_ctrl_txdone;
		}
	}

	pp-&gt;rxq_ctrl = mvOsMalloc(CONFIG_MV_ETH_RXQ * sizeof(struct rx_queue));
	if (!pp-&gt;rxq_ctrl)
		goto oom;

	memset(pp-&gt;rxq_ctrl, 0, CONFIG_MV_ETH_RXQ * sizeof(struct rx_queue));

	/* Create Rx descriptor rings */
	for (rxq = 0; rxq &lt; CONFIG_MV_ETH_RXQ; rxq++) {
		rxq_ctrl = &amp;pp-&gt;rxq_ctrl[rxq];
		rxq_ctrl-&gt;rxq_size = CONFIG_MV_ETH_RXQ_DESC;
		rxq_ctrl-&gt;rxq_pkts_coal = CONFIG_MV_ETH_RX_COAL_PKTS;
		rxq_ctrl-&gt;rxq_time_coal = CONFIG_MV_ETH_RX_COAL_USEC;
	}

	if (pp-&gt;flags &amp; MV_ETH_F_MH)
		mvNetaMhSet(pp-&gt;port, MV_NETA_MH);

#ifdef CONFIG_MV_ETH_TOOL
	/* Configure defaults */
	pp-&gt;autoneg_cfg  = AUTONEG_ENABLE;
	pp-&gt;speed_cfg    = SPEED_1000;
	pp-&gt;duplex_cfg  = DUPLEX_FULL;
	pp-&gt;advertise_cfg = 0x2f;
#endif /* CONFIG_MV_ETH_TOOL */

	return 0;
oom:
	printk(KERN_ERR "%s: port=%d: out of memory\n", __func__, pp-&gt;port);
	return -ENODEV;
}

/* Show network driver configuration */
void mv_eth_config_show(void)
{
	/* Check restrictions */
#if (CONFIG_MV_ETH_PORTS_NUM &gt; MV_ETH_MAX_PORTS)
#   error "CONFIG_MV_ETH_PORTS_NUM is large than MV_ETH_MAX_PORTS"
#endif

#if (CONFIG_MV_ETH_RXQ &gt; MV_ETH_MAX_RXQ)
#   error "CONFIG_MV_ETH_RXQ is large than MV_ETH_MAX_RXQ"
#endif

#if CONFIG_MV_ETH_TXQ &gt; MV_ETH_MAX_TXQ
#   error "CONFIG_MV_ETH_TXQ is large than MV_ETH_MAX_TXQ"
#endif

#if defined(CONFIG_MV_ETH_TSO) &amp;&amp; !defined(CONFIG_MV_ETH_TX_CSUM_OFFLOAD)
#   error "If GSO enabled - TX checksum offload must be enabled too"
#endif

	printk(KERN_ERR "  o %d Giga ports supported\n", CONFIG_MV_ETH_PORTS_NUM);

#ifdef CONFIG_MV_PON
	printk(KERN_ERR "  o Giga PON port is #%d: - %d TCONTs supported\n", MV_PON_PORT_ID, MV_ETH_MAX_TCONT());
#endif

#ifdef CONFIG_NET_SKB_RECYCLE
	printk(KERN_ERR "  o SKB recycle supported (%s)\n", mv_ctrl_recycle ? "Enabled" : "Disabled");
#endif

#ifdef CONFIG_MV_ETH_NETA
	printk(KERN_ERR "  o NETA acceleration mode %d\n", mvNetaAccMode());
#endif

#ifdef CONFIG_MV_ETH_BM_CPU
	printk(KERN_ERR "  o BM supported for CPU: %d BM pools\n", MV_ETH_BM_POOLS);
#endif /* CONFIG_MV_ETH_BM_CPU */

#ifdef CONFIG_MV_ETH_PNC
	printk(KERN_ERR "  o PnC supported (%s)\n", mv_eth_pnc_ctrl_en ? "Enabled" : "Disabled");
#endif

#ifdef CONFIG_MV_ETH_HWF
	printk(KERN_ERR "  o HWF supported\n");
#endif

#ifdef CONFIG_MV_ETH_PMT
	printk(KERN_ERR "  o PMT supported\n");
#endif

	printk(KERN_ERR "  o RX Queue support: %d Queues * %d Descriptors\n", CONFIG_MV_ETH_RXQ, CONFIG_MV_ETH_RXQ_DESC);

	printk(KERN_ERR "  o TX Queue support: %d Queues * %d Descriptors\n", CONFIG_MV_ETH_TXQ, CONFIG_MV_ETH_TXQ_DESC);

#if defined(CONFIG_MV_ETH_TSO)
	printk(KERN_ERR "  o GSO supported\n");
#endif /* CONFIG_MV_ETH_TSO */

#if defined(CONFIG_MV_ETH_GRO)
	printk(KERN_ERR "  o GRO supported\n");
#endif /* CONFIG_MV_ETH_GRO */

#if defined(CONFIG_MV_ETH_RX_CSUM_OFFLOAD)
	printk(KERN_ERR "  o Receive checksum offload supported\n");
#endif
#if defined(CONFIG_MV_ETH_TX_CSUM_OFFLOAD)
	printk(KERN_ERR "  o Transmit checksum offload supported\n");
#endif

#ifdef CONFIG_MV_ETH_NFP
	printk(KERN_ERR "  o Network Fast Processing (NFP) supported\n");

#ifdef NFP_BRIDGE
	printk(KERN_ERR "     o NFP Bridging supported\n");
#endif /* NFP_BRIDGE */

#ifdef NFP_VLAN
	printk(KERN_ERR "     o NFP VLAN Processing supported\n");
#endif /* NFP_VLAN */

#ifdef NFP_FIB
	printk(KERN_ERR "     o NFP Routing (FIB) supported\n");
#endif /* NFP_FIB */

#ifdef NFP_NAT
	printk(KERN_ERR "     o NFP NAT supported\n");
#endif /* NFP_NAT */

#ifdef NFP_LIMIT
	printk(KERN_ERR "     o NFP Rate Limiting supported\n");
#endif /* NFP_LIMIT */

#ifdef NFP_PPP
	printk(KERN_ERR "     o NFP PPPoE supported\n");
#endif /* NFP_PPP */

#endif /* CONFIG_MV_ETH_NFP */

#ifdef CONFIG_MV_ETH_STAT_ERR
	printk(KERN_ERR "  o Driver ERROR statistics enabled\n");
#endif

#ifdef CONFIG_MV_ETH_STAT_INF
	printk(KERN_ERR "  o Driver INFO statistics enabled\n");
#endif

#ifdef CONFIG_MV_ETH_STAT_DBG
	printk(KERN_ERR "  o Driver DEBUG statistics enabled\n");
#endif

#ifdef ETH_DEBUG
	printk(KERN_ERR "  o Driver debug messages enabled\n");
#endif

#if defined(CONFIG_MV_ETH_SWITCH)
	printk(KERN_ERR "  o Switch support enabled\n");

#endif /* CONFIG_MV_ETH_SWITCH */

	printk(KERN_ERR "\n");
}

/* Set network device features on initialization. Take into account default compile time configuration. */
static void mv_eth_netdev_set_features(struct net_device *dev)
{
	dev-&gt;features = NETIF_F_SG | NETIF_F_LLTX;

#ifdef CONFIG_MV_ETH_TX_CSUM_OFFLOAD_DEF
	if (dev-&gt;mtu &lt;= MV_ETH_TX_CSUM_MAX_SIZE)
		dev-&gt;features |= NETIF_F_IP_CSUM;
#endif /* CONFIG_MV_ETH_TX_CSUM_OFFLOAD_DEF */

#ifdef CONFIG_MV_ETH_TSO_DEF
	if (dev-&gt;features &amp; NETIF_F_IP_CSUM)
		dev-&gt;features |= NETIF_F_TSO;
#endif /* CONFIG_MV_ETH_TSO_DEF */

#ifdef CONFIG_MV_ETH_GRO_DEF
	dev-&gt;features |= NETIF_F_GRO;
#endif /* CONFIG_MV_ETH_GRO_DEF */
}

/* Update network device features after changing MTU.	*/
static void mv_eth_netdev_update_features(struct net_device *dev)
{
#ifdef CONFIG_MV_ETH_TX_CSUM_OFFLOAD
	if (dev-&gt;mtu &gt; MV_ETH_TX_CSUM_MAX_SIZE) {
		dev-&gt;features &amp;= ~NETIF_F_IP_CSUM;
		printk(KERN_ERR "Removing NETIF_F_IP_CSUM in device %s features\n", dev-&gt;name);
	}
#endif /* CONFIG_MV_ETH_TX_CSUM_OFFLOAD */

#ifdef CONFIG_MV_ETH_TSO
	if (!(dev-&gt;features &amp; NETIF_F_IP_CSUM)) {
		dev-&gt;features &amp;= ~NETIF_F_TSO;
		printk(KERN_ERR "Removing NETIF_F_TSO in device %s features\n", dev-&gt;name);
	}
#endif /* CONFIG_MV_ETH_TSO */
}

int mv_eth_napi_set_cpu_affinity(int port, int group, int affinity)
{
	struct eth_port *pp = mv_eth_port_by_id(port);
	if (pp == NULL) {
		printk(KERN_ERR "%s: pp == NULL, port=%d\n", __func__, port);
		return -1;
	}

	if (group &gt;= CONFIG_MV_ETH_NAPI_GROUPS) {
		printk(KERN_ERR "%s: group number is higher than %d\n", __func__, CONFIG_MV_ETH_NAPI_GROUPS-1);
		return -1;
		}
	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "Port %d must be stopped before\n", port);
		return -EINVAL;
	}
	set_cpu_affinity(pp, affinity, group);
	return 0;

}
void handle_group_affinity(int port)
{
	int group;
	struct eth_port *pp;
	MV_U32 group_cpu_affinity[CONFIG_MV_ETH_NAPI_GROUPS];
	MV_U32 rxq_affinity[CONFIG_MV_ETH_NAPI_GROUPS];

	group_cpu_affinity[0] = CONFIG_MV_ETH_GROUP0_CPU;
	rxq_affinity[0] 	  = CONFIG_MV_ETH_GROUP0_RXQ;

#ifdef CONFIG_MV_ETH_GROUP1_CPU
		group_cpu_affinity[1] = CONFIG_MV_ETH_GROUP1_CPU;
		rxq_affinity[1] 	  = CONFIG_MV_ETH_GROUP1_RXQ;
#endif

#ifdef CONFIG_MV_ETH_GROUP2_CPU
		group_cpu_affinity[2] = CONFIG_MV_ETH_GROUP2_CPU;
		rxq_affinity[2] 	  = CONFIG_MV_ETH_GROUP2_RXQ;
#endif

#ifdef CONFIG_MV_ETH_GROUP3_CPU
		group_cpu_affinity[3] = CONFIG_MV_ETH_GROUP3_CPU;
		rxq_affinity[3] 	  = CONFIG_MV_ETH_GROUP3_RXQ;
#endif

	pp = mv_eth_port_by_id(port);
	if (pp == NULL)
		return;

	for (group = 0; group &lt; CONFIG_MV_ETH_NAPI_GROUPS; group++)
		set_cpu_affinity(pp, group_cpu_affinity[group], group);
	for (group = 0; group &lt; CONFIG_MV_ETH_NAPI_GROUPS; group++)
		set_rxq_affinity(pp, rxq_affinity[group], group);

}

int	mv_eth_napi_set_rxq_affinity(int port, int group, int rxqAffinity)
{
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (pp == NULL) {
		printk(KERN_ERR "%s: pp is null \n", __func__);
		return MV_FAIL;
	}
	if (group &gt;= CONFIG_MV_ETH_NAPI_GROUPS) {
		printk(KERN_ERR "%s: group number is higher than %d\n", __func__, CONFIG_MV_ETH_NAPI_GROUPS-1);
		return -1;
		}
	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "Port %d must be stopped before\n", port);
		return -EINVAL;
	}

	set_rxq_affinity(pp, rxqAffinity, group);
	return MV_OK;
}


void mv_eth_napi_group_show(int port)
{
	int cpu, group;
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (pp == NULL) {
		printk(KERN_ERR "%s: pp == NULL\n", __func__);
		return;
	}
	for (group = 0; group &lt; CONFIG_MV_ETH_NAPI_GROUPS; group++) {
		printk(KERN_INFO "group=%d:\n", group);
		for (cpu = 0; cpu &lt; CONFIG_NR_CPUS; cpu++) {
			if (!(MV_BIT_CHECK(pp-&gt;cpuMask, cpu)))
				continue;
			if (pp-&gt;napiCpuGroup[cpu] == group) {
				printk(KERN_INFO "   CPU%d ", cpu);
				mvNetaRxqCpuDump(port, cpu);
				printk(KERN_INFO "\n");
			}
		}
		printk(KERN_INFO "\n");
	}
}

void mv_eth_priv_cleanup(struct eth_port *pp)
{
	/* TODO */
}

#ifdef CONFIG_MV_ETH_BM_CPU
static struct bm_pool *mv_eth_long_pool_get(struct eth_port *pp, int pkt_size)
{
	int             pool, i;
	struct bm_pool	*bm_pool, *temp_pool = NULL;
	unsigned long   flags = 0;

	pool = mv_eth_bm_config_long_pool_get(pp-&gt;port);
	if (pool != -1) /* constant long pool for the port */
		return &amp;mv_eth_pool[pool];

	/* look for free pool pkt_size == 0. First check pool == pp-&gt;port */
	/* if no free pool choose larger than required */
	for (i = 0; i &lt; MV_ETH_BM_POOLS; i++) {
		pool = (pp-&gt;port + i) % MV_ETH_BM_POOLS;
		bm_pool = &amp;mv_eth_pool[pool];

		MV_ETH_LOCK(&amp;bm_pool-&gt;lock, flags);

		if (bm_pool-&gt;pkt_size == 0) {
			/* found free pool */

			MV_ETH_UNLOCK(&amp;bm_pool-&gt;lock, flags);
			return bm_pool;
		}
		if (bm_pool-&gt;pkt_size &gt;= pkt_size) {
			if (temp_pool == NULL)
				temp_pool = bm_pool;
			else if (bm_pool-&gt;pkt_size &lt; temp_pool-&gt;pkt_size)
				temp_pool = bm_pool;
		}
		MV_ETH_UNLOCK(&amp;bm_pool-&gt;lock, flags);
	}
	return temp_pool;
}
#else
static struct bm_pool *mv_eth_long_pool_get(struct eth_port *pp, int pkt_size)
{
	return &amp;mv_eth_pool[pp-&gt;port];
}
#endif /* CONFIG_MV_ETH_BM_CPU */

static int mv_eth_rxq_fill(struct eth_port *pp, int rxq, int num)
{
	int i;

#ifndef CONFIG_MV_ETH_BM_CPU
	struct eth_pbuf *pkt;
	struct bm_pool *bm_pool;
	MV_NETA_RXQ_CTRL *rx_ctrl;
	struct neta_rx_desc *rx_desc;

	bm_pool = pp-&gt;pool_long;

	rx_ctrl = pp-&gt;rxq_ctrl[rxq].q;
	if (!rx_ctrl) {
		printk(KERN_ERR "%s: rxq %d is not initialized\n", __func__, rxq);
		return 0;
	}

	for (i = 0; i &lt; num; i++) {
		pkt = mv_eth_pool_get(bm_pool);
		if (pkt) {
			rx_desc = (struct neta_rx_desc *)MV_NETA_QUEUE_DESC_PTR(&amp;rx_ctrl-&gt;queueCtrl, i);
			memset(rx_desc, 0, sizeof(struct neta_rx_desc));

			mvNetaRxDescFill(rx_desc, pkt-&gt;physAddr, (MV_U32)pkt);
		} else {
			printk(KERN_ERR "%s: rxq %d, %d of %d buffers are filled\n", __func__, rxq, i, num);
			break;
		}
	}
#else
	i = num;
#endif /* CONFIG_MV_ETH_BM_CPU */

	mvNetaRxqNonOccupDescAdd(pp-&gt;port, rxq, i);

	return i;
}

static int mv_eth_txq_create(struct eth_port *pp, struct tx_queue *txq_ctrl)
{
	txq_ctrl-&gt;q = mvNetaTxqInit(pp-&gt;port, txq_ctrl-&gt;txp, txq_ctrl-&gt;txq, txq_ctrl-&gt;txq_size);
	if (txq_ctrl-&gt;q == NULL) {
		printk(KERN_ERR "%s: can't create TxQ - port=%d, txp=%d, txq=%d, desc=%d\n",
		       __func__, pp-&gt;port, txq_ctrl-&gt;txp, txq_ctrl-&gt;txp, txq_ctrl-&gt;txq_size);
		return -ENODEV;
	}

	txq_ctrl-&gt;shadow_txq = mvOsMalloc(txq_ctrl-&gt;txq_size * sizeof(MV_ULONG));
	if (txq_ctrl-&gt;shadow_txq == NULL)
		goto no_mem;

	/* reset txq */
	txq_ctrl-&gt;txq_count = 0;
	txq_ctrl-&gt;shadow_txq_put_i = 0;
	txq_ctrl-&gt;shadow_txq_get_i = 0;

#ifdef CONFIG_MV_ETH_HWF
	mvNetaHwfTxqInit(pp-&gt;port, txq_ctrl-&gt;txp, txq_ctrl-&gt;txq);
#endif /* CONFIG_MV_ETH_HWF */

	return 0;

no_mem:
	mv_eth_txq_delete(pp, txq_ctrl);
	return -ENOMEM;
}


static int mv_force_port_link_speed_fc(int port, MV_ETH_PORT_SPEED port_speed, int en_force)
{
	if (en_force) {
		if (mvNetaForceLinkModeSet(port, 1, 0)) {
			printk(KERN_ERR "mvNetaForceLinkModeSet failed\n");
			return -EIO;
		}
		if (mvNetaSpeedDuplexSet(port, port_speed, MV_ETH_DUPLEX_FULL)) {
			printk(KERN_ERR "mvNetaSpeedDuplexSet failed\n");
			return -EIO;
		}
		if (mvNetaFlowCtrlSet(port, MV_ETH_FC_ENABLE)) {
			printk(KERN_ERR "mvNetaFlowCtrlSet failed\n");
			return -EIO;
		}
	} else {
		if (mvNetaForceLinkModeSet(port, 0, 0)) {
			printk(KERN_ERR "mvNetaForceLinkModeSet failed\n");
			return -EIO;
		}
		if (mvNetaSpeedDuplexSet(port, MV_ETH_SPEED_AN, MV_ETH_DUPLEX_AN)) {
			printk(KERN_ERR "mvNetaSpeedDuplexSet failed\n");
			return -EIO;
		}
		if (mvNetaFlowCtrlSet(port, MV_ETH_FC_AN_SYM)) {
			printk(KERN_ERR "mvNetaFlowCtrlSet failed\n");
			return -EIO;
		}
	}
	return 0;
}

static void mv_eth_txq_delete(struct eth_port *pp, struct tx_queue *txq_ctrl)
{
	if (txq_ctrl-&gt;shadow_txq) {
		mvOsFree(txq_ctrl-&gt;shadow_txq);
		txq_ctrl-&gt;shadow_txq = NULL;
	}

	if (txq_ctrl-&gt;q) {
		mvNetaTxqDelete(pp-&gt;port, txq_ctrl-&gt;txp, txq_ctrl-&gt;txq);
		txq_ctrl-&gt;q = NULL;
	}
}

/* Free all packets pending transmit from all TXQs and reset TX port */
int mv_eth_txp_reset(int port, int txp)
{
	struct eth_port *pp = mv_eth_port_by_id(port);
	int queue;

	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "Port %d must be stopped before\n", port);
		return -EINVAL;
	}

	/* free the skb's in the hal tx ring */
	for (queue = 0; queue &lt; CONFIG_MV_ETH_TXQ; queue++) {
		struct tx_queue *txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + queue];

		if (txq_ctrl-&gt;q)
			mv_eth_txq_done_force(pp, txq_ctrl);
	}
	mvNetaTxpReset(port, txp);
	return 0;
}

/* Free received packets from all RXQs and reset RX of the port */
int mv_eth_rx_reset(int port)
{
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "Port %d must be stopped before\n", port);
		return -EINVAL;
	}

#ifndef CONFIG_MV_ETH_BM_CPU
	{
		int rxq = 0;

		for (rxq = 0; rxq &lt; CONFIG_MV_ETH_RXQ; rxq++) {
			struct eth_pbuf *pkt;
			struct neta_rx_desc *rx_desc;
			struct bm_pool *pool;
			int i, rx_done;
			MV_NETA_RXQ_CTRL *rx_ctrl = pp-&gt;rxq_ctrl[rxq].q;

			if (rx_ctrl == NULL)
				continue;

			rx_done = mvNetaRxqFreeDescNumGet(pp-&gt;port, rxq);
			mvOsCacheIoSync();
			for (i = 0; i &lt; rx_done; i++) {
				rx_desc = mvNetaRxqNextDescGet(rx_ctrl);
				mvOsCacheLineInv(NULL, rx_desc);

#if defined(MV_CPU_BE)
				mvNetaRxqDescSwap(rx_desc);
#endif /* MV_CPU_BE */

				pkt = (struct eth_pbuf *)rx_desc-&gt;bufCookie;
				pool = &amp;mv_eth_pool[pkt-&gt;pool];
				mv_eth_pool_put(pool, pkt);
			}
		}
	}
#endif /* CONFIG_MV_ETH_BM_CPU */

	mvNetaRxReset(port);
	return 0;
}

/***********************************************************
 * coal set functions		                           *
 ***********************************************************/
MV_STATUS mv_eth_rx_ptks_coal_set(int port, int rxq, MV_U32 value)
{
	MV_STATUS status = mvNetaRxqPktsCoalSet(port, rxq, value);
	struct eth_port *pp = mv_eth_port_by_id(port);
	if (status == MV_OK)
		pp-&gt;rxq_ctrl[rxq].rxq_pkts_coal = value;
	return status;
}

MV_STATUS mv_eth_rx_time_coal_set(int port, int rxq, MV_U32 value)
{
	MV_STATUS status = mvNetaRxqTimeCoalSet(port, rxq, value);
	struct eth_port *pp = mv_eth_port_by_id(port);
	if (status == MV_OK)
		pp-&gt;rxq_ctrl[rxq].rxq_time_coal = value;
	return status;
}

MV_STATUS mv_eth_tx_done_ptks_coal_set(int port, int txp, int txq, MV_U32 value)
{
	MV_STATUS status = mvNetaTxDonePktsCoalSet(port, txp, txq, value);
	struct eth_port *pp = mv_eth_port_by_id(port);
	if (status == MV_OK)
		pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq].txq_done_pkts_coal = value;
	return status;
}

/***********************************************************
 * mv_eth_start_internals --                               *
 *   fill rx buffers. start rx/tx activity. set coalesing. *
 *   clear and unmask interrupt bits                       *
 ***********************************************************/
int mv_eth_start_internals(struct eth_port *pp, int mtu)
{
	unsigned long rwflags;
	unsigned int status;
	int rxq, txp, txq, num, err = 0;
	int pkt_size = RX_PKT_SIZE(mtu);
	MV_BOARD_MAC_SPEED mac_speed;

	write_lock_irqsave(&amp;pp-&gt;rwlock, rwflags);

	if (mvNetaMaxRxSizeSet(pp-&gt;port, RX_PKT_SIZE(mtu))) {
		printk(KERN_ERR "%s: can't set maxRxSize=%d for port=%d, mtu=%d\n",
		       __func__, RX_PKT_SIZE(mtu), pp-&gt;port, mtu);
		err = -EINVAL;
		goto out;
	}

	if (mv_eth_ctrl_is_tx_enabled(pp)) {
		int cpu;
		for_each_possible_cpu(cpu) {
			if (!(MV_BIT_CHECK(pp-&gt;cpuMask, cpu)))
				continue;
			if (mv_eth_ctrl_txq_cpu_own(pp-&gt;port, pp-&gt;txp, pp-&gt;txq[cpu], 1) &lt; 0) {
				err = -EINVAL;
				goto out;
			}
		}
	}

	/* Allocate buffers for Long buffers pool */
	if (pp-&gt;pool_long == NULL) {
		struct bm_pool *new_pool;

		new_pool = mv_eth_long_pool_get(pp, pkt_size);
		if (new_pool == NULL) {
			printk(KERN_ERR "%s FAILED: port=%d, Can't find pool for pkt_size=%d\n",
			       __func__, pp-&gt;port, pkt_size);
			err = -ENOMEM;
			goto out;
		}
		if (new_pool-&gt;pkt_size == 0) {
			new_pool-&gt;pkt_size = pkt_size;
#ifdef CONFIG_MV_ETH_BM_CPU
			mvBmPoolBufSizeSet(new_pool-&gt;pool, RX_BUF_SIZE(pkt_size));
#endif /* CONFIG_MV_ETH_BM_CPU */
		}
		if (new_pool-&gt;pkt_size &lt; pkt_size) {
			printk(KERN_ERR "%s FAILED: port=%d, long pool #%d, pkt_size=%d less than required %d\n",
					__func__, pp-&gt;port, new_pool-&gt;pool, new_pool-&gt;pkt_size, pkt_size);
			err = -ENOMEM;
			goto out;
		}
		pp-&gt;pool_long = new_pool;
		pp-&gt;pool_long-&gt;port_map |= (1 &lt;&lt; pp-&gt;port);

		num = mv_eth_pool_add(pp-&gt;pool_long-&gt;pool, pp-&gt;pool_long_num);
		if (num != pp-&gt;pool_long_num) {
			printk(KERN_ERR "%s FAILED: mtu=%d, pool=%d, pkt_size=%d, only %d of %d allocated\n",
			       __func__, mtu, pp-&gt;pool_long-&gt;pool, pp-&gt;pool_long-&gt;pkt_size, num, pp-&gt;pool_long_num);
			err = -ENOMEM;
			goto out;
		}
	}

#ifdef CONFIG_MV_ETH_BM_CPU
	mvNetaBmPoolBufSizeSet(pp-&gt;port, pp-&gt;pool_long-&gt;pool, RX_BUF_SIZE(pp-&gt;pool_long-&gt;pkt_size));

	if (pp-&gt;pool_short == NULL) {
		int short_pool = mv_eth_bm_config_short_pool_get(pp-&gt;port);

		/* Allocate packets for short pool */
		if (short_pool &lt; 0) {
			err = -EINVAL;
			goto out;
		}
		pp-&gt;pool_short = &amp;mv_eth_pool[short_pool];
		pp-&gt;pool_short-&gt;port_map |= (1 &lt;&lt; pp-&gt;port);
		if (pp-&gt;pool_short-&gt;pool != pp-&gt;pool_long-&gt;pool) {
			num = mv_eth_pool_add(pp-&gt;pool_short-&gt;pool, pp-&gt;pool_short_num);
			if (num != pp-&gt;pool_short_num) {
				printk(KERN_ERR "%s FAILED: pool=%d, pkt_size=%d - %d of %d buffers added\n",
					   __func__, short_pool, pp-&gt;pool_short-&gt;pkt_size, num, pp-&gt;pool_short_num);
				err = -ENOMEM;
				goto out;
			}
			mvNetaBmPoolBufSizeSet(pp-&gt;port, pp-&gt;pool_short-&gt;pool, RX_BUF_SIZE(pp-&gt;pool_short-&gt;pkt_size));
		} else {
			int dummy_short_pool = (pp-&gt;pool_short-&gt;pool + 1) % MV_BM_POOLS;

			/* To disable short pool we choose unused pool and set pkt size to 0 (buffer size = pkt offset) */
			mvNetaBmPoolBufSizeSet(pp-&gt;port, dummy_short_pool, NET_SKB_PAD);
		}
	}
#endif /* CONFIG_MV_ETH_BM_CPU */

	for (rxq = 0; rxq &lt; CONFIG_MV_ETH_RXQ; rxq++) {
		if (pp-&gt;rxq_ctrl[rxq].q == NULL) {
			pp-&gt;rxq_ctrl[rxq].q = mvNetaRxqInit(pp-&gt;port, rxq, pp-&gt;rxq_ctrl[rxq].rxq_size);
			if (!pp-&gt;rxq_ctrl[rxq].q) {
				printk(KERN_ERR "%s: can't create RxQ port=%d, rxq=%d, desc=%d\n",
				       __func__, pp-&gt;port, rxq, pp-&gt;rxq_ctrl[rxq].rxq_size);
				err = -ENODEV;
				goto out;
			}
		}

		/* Set Offset */
		mvNetaRxqOffsetSet(pp-&gt;port, rxq, NET_SKB_PAD);

		/* Set coalescing pkts and time */
		mv_eth_rx_ptks_coal_set(pp-&gt;port, rxq, pp-&gt;rxq_ctrl[rxq].rxq_pkts_coal);
		mv_eth_rx_time_coal_set(pp-&gt;port, rxq, pp-&gt;rxq_ctrl[rxq].rxq_time_coal);

#if defined(CONFIG_MV_ETH_BM_CPU)
		/* Enable / Disable - BM support */
		if (pp-&gt;pool_short-&gt;pool == pp-&gt;pool_long-&gt;pool) {
			int dummy_short_pool = (pp-&gt;pool_short-&gt;pool + 1) % MV_BM_POOLS;

			/* To disable short pool we choose unused pool and set pkt size to 0 (buffer size = pkt offset) */
			mvNetaRxqBmEnable(pp-&gt;port, rxq, dummy_short_pool, pp-&gt;pool_long-&gt;pool);
		} else
			mvNetaRxqBmEnable(pp-&gt;port, rxq, pp-&gt;pool_short-&gt;pool, pp-&gt;pool_long-&gt;pool);
#else
		/* Fill RXQ with buffers from RX pool */
		mvNetaRxqBufSizeSet(pp-&gt;port, rxq, RX_BUF_SIZE(pkt_size));
		mvNetaRxqBmDisable(pp-&gt;port, rxq);
#endif /* CONFIG_MV_ETH_BM_CPU */

		if (mvNetaRxqFreeDescNumGet(pp-&gt;port, rxq) == 0)
			mv_eth_rxq_fill(pp, rxq, pp-&gt;rxq_ctrl[rxq].rxq_size);
	}

	for (txp = 0; txp &lt; pp-&gt;txp_num; txp++) {
		for (txq = 0; txq &lt; CONFIG_MV_ETH_TXQ; txq++) {
			struct tx_queue *txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];

			if ((txq_ctrl-&gt;q == NULL) &amp;&amp; (txq_ctrl-&gt;txq_size &gt; 0)) {
				err = mv_eth_txq_create(pp, txq_ctrl);
				if (err)
					goto out;
				spin_lock_init(&amp;txq_ctrl-&gt;queue_lock);
			}
			mv_eth_tx_done_ptks_coal_set(pp-&gt;port, txp, txq,
					pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq].txq_done_pkts_coal);
		}
		mvNetaTxpMaxTxSizeSet(pp-&gt;port, txp, RX_PKT_SIZE(mtu));
	}

#ifdef CONFIG_MV_ETH_HWF
#ifdef CONFIG_MV_ETH_BM_CPU
	mvNetaHwfBmPoolsSet(pp-&gt;port, pp-&gt;pool_short-&gt;pool, pp-&gt;pool_long-&gt;pool);
#else
	mv_eth_hwf_bm_create(pp-&gt;port, RX_PKT_SIZE(mtu));
#endif /* CONFIG_MV_ETH_BM_CPU */

	mvNetaHwfEnable(pp-&gt;port, 1);
#endif /* CONFIG_MV_ETH_HWF */

	if (!MV_PON_PORT(pp-&gt;port)) {
		/* force link, speed and duplex if necessary (e.g. Switch is connected) based on board information */
		mac_speed = mvBoardMacSpeedGet(pp-&gt;port);
		switch (mac_speed) {
		case BOARD_MAC_SPEED_10M:
			err = mv_force_port_link_speed_fc(pp-&gt;port, MV_ETH_SPEED_10, 1);
			if (err)
				goto out;
			break;
		case BOARD_MAC_SPEED_100M:
			err = mv_force_port_link_speed_fc(pp-&gt;port, MV_ETH_SPEED_100, 1);
			if (err)
				goto out;
			break;
		case BOARD_MAC_SPEED_1000M:
			err = mv_force_port_link_speed_fc(pp-&gt;port, MV_ETH_SPEED_1000, 1);
			if (err)
				goto out;
			break;
		case BOARD_MAC_SPEED_AUTO:
		default:
			/* do nothing */
			break;
		}
	}

	/* start the hal - rx/tx activity */
	status = mvNetaPortEnable(pp-&gt;port);
	if (status == MV_OK)
		set_bit(MV_ETH_F_LINK_UP_BIT, &amp;(pp-&gt;flags));
#ifdef CONFIG_MV_PON
	else if (MV_PON_PORT(pp-&gt;port) &amp;&amp; (mv_pon_link_status() == MV_TRUE)) {
		mvNetaPortUp(pp-&gt;port);
		set_bit(MV_ETH_F_LINK_UP_BIT, &amp;(pp-&gt;flags));
	}
#endif /* CONFIG_MV_PON */

	set_bit(MV_ETH_F_STARTED_BIT, &amp;(pp-&gt;flags));

 out:
	write_unlock_irqrestore(&amp;pp-&gt;rwlock, rwflags);
	return err;
}

/***********************************************************
 * mv_eth_stop_internals --                                *
 *   stop port rx/tx activity. free skb's from rx/tx rings.*
 ***********************************************************/
int mv_eth_stop_internals(struct eth_port *pp)
{
	unsigned long rwflags;
	int queue;

	write_lock_irqsave(&amp;pp-&gt;rwlock, rwflags);

	clear_bit(MV_ETH_F_STARTED_BIT, &amp;(pp-&gt;flags));

	/* stop the port activity, mask all interrupts */
	if (mvNetaPortDisable(pp-&gt;port) != MV_OK) {
		printk(KERN_ERR "GbE port %d: ethPortDisable failed\n", pp-&gt;port);
		goto error;
	}

	/* clear all ethernet port interrupts ???? */
	MV_REG_WRITE(NETA_INTR_MISC_CAUSE_REG(pp-&gt;port), 0);
	MV_REG_WRITE(NETA_INTR_OLD_CAUSE_REG(pp-&gt;port), 0);

	/* mask all ethernet port interrupts ???? */
	MV_REG_WRITE(NETA_INTR_NEW_MASK_REG(pp-&gt;port), 0);
	MV_REG_WRITE(NETA_INTR_OLD_MASK_REG(pp-&gt;port), 0);
	MV_REG_WRITE(NETA_INTR_MISC_MASK_REG(pp-&gt;port), 0);

#ifdef CONFIG_MV_ETH_HWF
	mvNetaHwfEnable(pp-&gt;port, 0);
#else
	{
		int txp;
		/* Reset TX port here. If HWF is supported reset must be called externally */
		for (txp = 0; txp &lt; pp-&gt;txp_num; txp++)
			mv_eth_txp_reset(pp-&gt;port, txp);
	}
#endif /* !CONFIG_MV_ETH_HWF */

	if (mv_eth_ctrl_is_tx_enabled(pp)) {
		int cpu;
		for_each_possible_cpu(cpu) {
			if (MV_BIT_CHECK(pp-&gt;cpuMask, cpu))
				mv_eth_ctrl_txq_cpu_own(pp-&gt;port, pp-&gt;txp, pp-&gt;txq[cpu], 0);
		}
	}

	/* free the skb's in the hal rx ring */
	for (queue = 0; queue &lt; CONFIG_MV_ETH_RXQ; queue++)
		mv_eth_rxq_drop_pkts(pp, queue);

	write_unlock_irqrestore(&amp;pp-&gt;rwlock, rwflags);

	return 0;

error:
	printk(KERN_ERR "GbE port %d: stop internals failed\n", pp-&gt;port);
	write_unlock_irqrestore(&amp;pp-&gt;rwlock, rwflags);

	return -1;
}

/* return positive if MTU is valid */
int mv_eth_check_mtu_valid(struct net_device *dev, int mtu)
{
	if (mtu &lt; 68) {
		printk(KERN_INFO "MTU must be at least 64, change mtu failed\n");
		return -EINVAL;
	}
	if (mtu &gt; 9676 /* 9700 - 20 and rounding to 8 */) {
		printk(KERN_ERR "%s: Illegal MTU value %d, ", dev-&gt;name, mtu);
		mtu = 9676;
		printk(KERN_CONT " rounding MTU to: %d \n", mtu);
	}

	if (MV_IS_NOT_ALIGN(RX_PKT_SIZE(mtu), 8)) {
		printk(KERN_ERR "%s: Illegal MTU value %d, ", dev-&gt;name, mtu);
		mtu = MV_ALIGN_UP(RX_PKT_SIZE(mtu), 8);
		printk(KERN_CONT " rounding MTU to: %d \n", mtu);
	}
	return mtu;
}

/* Check if MTU can be changed */
int mv_eth_check_mtu_internals(struct net_device *dev, int mtu)
{
	struct eth_port *pp = MV_ETH_PRIV(dev);
	struct bm_pool	*new_pool = NULL;

	new_pool = mv_eth_long_pool_get(pp, RX_PKT_SIZE(mtu));

	/* If no pool for new MTU or shared pool is used - MTU can be changed only when interface is stopped */
	if (new_pool == NULL) {
		printk(KERN_ERR "%s: No BM pool available for MTU=%d\n", __func__, mtu);
		return -EPERM;
	}
#ifdef CONFIG_MV_ETH_BM_CPU
	if (new_pool-&gt;pkt_size &lt; RX_PKT_SIZE(mtu)) {
		if (mv_eth_bm_config_pkt_size_get(new_pool-&gt;pool) != 0) {
			printk(KERN_ERR "%s: BM pool #%d - pkt_size = %d less than required for MTU=%d and can't be changed\n",
						__func__, new_pool-&gt;pool, new_pool-&gt;pkt_size, mtu);
			return -EPERM;
		}
		/* Pool packet size can be changed for new MTU, but pool is shared */
		if ((new_pool == pp-&gt;pool_long) &amp;&amp; (pp-&gt;pool_long-&gt;port_map != (1 &lt;&lt; pp-&gt;port))) {
			/* Shared pool */
			printk(KERN_ERR "%s: bmPool=%d is shared port_map=0x%x. Stop all ports uses this pool before change MTU\n",
						__func__, pp-&gt;pool_long-&gt;pool, pp-&gt;pool_long-&gt;port_map);
			return -EPERM;
		}
	}
#endif /* CONFIG_MV_ETH_BM_CPU */
	return 0;
}

/***********************************************************
 * mv_eth_change_mtu_internals --                          *
 *   stop port activity. release skb from rings. set new   *
 *   mtu in device and hw. restart port activity and       *
 *   and fill rx-buiffers with size according to new mtu.  *
 ***********************************************************/
int mv_eth_change_mtu_internals(struct net_device *dev, int mtu)
{
	struct bm_pool	*new_pool = NULL;
	struct eth_port *pp = MV_ETH_PRIV(dev);
	int             config_pkt_size;
	unsigned long	rwflags;

	if ((mtu != dev-&gt;mtu) &amp;&amp; (pp-&gt;pool_long)) {
		/* If long pool assigned and MTU really changed and can't use old pool - free buffers */
		write_lock_irqsave(&amp;pp-&gt;rwlock, rwflags);

		config_pkt_size = 0;
#ifdef CONFIG_MV_ETH_BM_CPU
		new_pool = mv_eth_long_pool_get(pp, RX_PKT_SIZE(mtu));
		if (new_pool != NULL)
			config_pkt_size = mv_eth_bm_config_pkt_size_get(new_pool-&gt;pool);
#else
		/* If BM is not used always free buffers */
		new_pool = NULL;
#endif /* CONFIG_MV_ETH_BM_CPU */

		/* Free all buffers from long pool */
		if ((new_pool == NULL) || (new_pool-&gt;pkt_size &lt; RX_PKT_SIZE(mtu)) || (pp-&gt;pool_long != new_pool) ||
			((new_pool-&gt;pkt_size &gt; RX_PKT_SIZE(mtu)) &amp;&amp; (config_pkt_size == 0))) {
			mv_eth_rx_reset(pp-&gt;port);
			mv_eth_pool_free(pp-&gt;pool_long-&gt;pool, pp-&gt;pool_long_num);

#ifdef CONFIG_MV_ETH_BM_CPU
			/* redefine pool pkt_size */
			if (pp-&gt;pool_long-&gt;buf_num == 0) {
				pp-&gt;pool_long-&gt;pkt_size = config_pkt_size;

				if (pp-&gt;pool_long-&gt;pkt_size == 0)
					mvBmPoolBufSizeSet(pp-&gt;pool_long-&gt;pool, 0);
				else
					mvBmPoolBufSizeSet(pp-&gt;pool_long-&gt;pool, RX_BUF_SIZE(pp-&gt;pool_long-&gt;pkt_size));
			}
#else
			pp-&gt;pool_long-&gt;pkt_size = config_pkt_size;
#endif /* CONFIG_MV_ETH_BM_CPU */

			pp-&gt;pool_long-&gt;port_map &amp;= ~(1 &lt;&lt; pp-&gt;port);
			pp-&gt;pool_long = NULL;
		}

		/* DIMA debug; Free all buffers from short pool */
/*
		if(pp-&gt;pool_short) {
			mv_eth_pool_free(pp-&gt;pool_short-&gt;pool, pp-&gt;pool_short_num);
			pp-&gt;pool_short = NULL;
		}
*/
		write_unlock_irqrestore(&amp;pp-&gt;rwlock, rwflags);
	}
	dev-&gt;mtu = mtu;

	mv_eth_netdev_update_features(dev);

	return 0;
}

/***********************************************************
 * mv_eth_tx_done_timer_callback --			   *
 *   N msec periodic callback for tx_done                  *
 ***********************************************************/
static void mv_eth_tx_done_timer_callback(unsigned long data)
{
	struct net_device *dev = (struct net_device *)data;
	struct eth_port *pp = MV_ETH_PRIV(dev);
	int tx_done = 0, tx_todo = 0;

	read_lock(&amp;pp-&gt;rwlock);
	STAT_INFO(pp-&gt;stats.tx_done_timer++);

	clear_bit(MV_ETH_F_TX_DONE_TIMER_BIT, &amp;(pp-&gt;flags));

	if (MV_PON_PORT(pp-&gt;port))
		tx_done = mv_eth_tx_done_pon(pp, &amp;tx_todo);
	else
		/* check all possible queues, as there is no indication from interrupt */
		tx_done = mv_eth_tx_done_gbe(pp,
			(((1 &lt;&lt; CONFIG_MV_ETH_TXQ) - 1) &amp; NETA_CAUSE_TXQ_SENT_DESC_ALL_MASK), &amp;tx_todo);

	if (tx_todo &gt; 0)
		mv_eth_add_tx_done_timer(pp);

	read_unlock(&amp;pp-&gt;rwlock);
}

/***********************************************************
 * mv_eth_cleanup_timer_callback --			   *
 *   N msec periodic callback for error cleanup            *
 ***********************************************************/
static void mv_eth_cleanup_timer_callback(unsigned long data)
{
	struct net_device *dev = (struct net_device *)data;
	struct eth_port *pp = MV_ETH_PRIV(dev);

	read_lock(&amp;pp-&gt;rwlock);
	STAT_INFO(pp-&gt;stats.cleanup_timer++);

	/* FIXME: check bm_pool-&gt;missed and pp-&gt;rxq_ctrl[rxq].missed counters and allocate */
	/* re-add timer if necessary (check bm_pool-&gt;missed and pp-&gt;rxq_ctrl[rxq].missed   */

	clear_bit(MV_ETH_F_CLEANUP_TIMER_BIT, &amp;(pp-&gt;flags));
	read_unlock(&amp;pp-&gt;rwlock);
}

void mv_eth_mac_show(int port)
{
#ifdef CONFIG_MV_ETH_PNC
	if (mv_eth_pnc_ctrl_en) {
		mvOsPrintf("PnC MAC Rules - port #%d:\n", port);
		pnc_mac_show();
	} else
		mvOsPrintf("%s: PNC control is disabled\n", __func__);
#else /* Legacy parser */
	mvEthPortUcastShow(port);
	mvEthPortMcastShow(port);
#endif /* CONFIG_MV_ETH_PNC */
}

void mv_eth_tos_map_show(int port)
{
	int tos, txq;
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (pp == NULL) {
		printk(KERN_ERR "%s: port %d entry is null \n", __func__, port);
		return;
	}

#ifdef CONFIG_MV_ETH_PNC
	if (mv_eth_pnc_ctrl_en)
		pnc_ipv4_dscp_show();
	else
		mvOsPrintf("%s: PNC control is disabled\n", __func__);
#else
	for (tos = 0; tos &lt; 0xFF; tos += 0x4) {
		int rxq;

		rxq = mvNetaTosToRxqGet(port, tos);
		if (rxq &gt; 0)
			printk(KERN_ERR "tos=0x%02x: codepoint=0x%02x, rxq=%d\n",
					tos, tos &gt;&gt; 2, rxq);
	}
#endif /* CONFIG_MV_ETH_PNC */

	printk(KERN_ERR "\n");
	printk(KERN_ERR " TOS &lt;=&gt; TXQ map for port #%d\n\n", port);

	for (tos = 0; tos &lt; sizeof(pp-&gt;txq_tos_map); tos++) {
		txq = pp-&gt;txq_tos_map[tos];
		if (txq != MV_ETH_TXQ_INVALID)
			printk(KERN_ERR "0x%02x &lt;=&gt; %d\n", tos, txq);
	}
}

int mv_eth_rxq_tos_map_set(int port, int rxq, unsigned char tos)
{
	int status = 1;

#ifdef CONFIG_MV_ETH_PNC
	if (mv_eth_pnc_ctrl_en)
		status = pnc_ip4_dscp(tos, 0xFF, rxq);
	else
		mvOsPrintf("%s: PNC control is disabled\n", __func__);
#else /* Legacy parser */
	status = mvNetaTosToRxqSet(port, tos, rxq);
#endif /* CONFIG_MV_ETH_PNC */

	if (status == 0)
		printk(KERN_ERR "Succeeded\n");
	else if (status == 1)
		printk(KERN_ERR "Not supported\n");
	else
		printk(KERN_ERR "Failed\n");

	return status;
}

int mv_eth_rxq_vlan_prio_set(int port, int rxq, unsigned char prio)
{
	int status = 1;

#ifdef CONFIG_MV_ETH_PNC
	if (mv_eth_pnc_ctrl_en)
		status = pnc_vlan_set(prio, rxq);
	else
		mvOsPrintf("%s: PNC control is disabled\n", __func__);
#else /* Legacy parser */
	status = mvNetaVprioToRxqSet(port, prio, rxq);
#endif /* CONFIG_MV_ETH_PNC */

	return status;
}

/* Set TXQ for special TOS value. txq=-1 - use default TXQ for this port */
int mv_eth_txq_tos_map_set(int port, int txq, unsigned char tos)
{
	MV_U8 old_txq;
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (mvNetaPortCheck(port))
		return -EINVAL;

	if ((pp == NULL) || (pp-&gt;txq_ctrl == NULL))
		return -ENODEV;

	old_txq = pp-&gt;txq_tos_map[tos];

	if (old_txq != MV_ETH_TXQ_INVALID) {
		if (old_txq == (MV_U8) txq)
			return 0;

		if (mv_eth_ctrl_txq_cpu_own(port, pp-&gt;txp, old_txq, 0))
			return -EINVAL;
	}

	if (txq == -1) {
		pp-&gt;txq_tos_map[tos] = MV_ETH_TXQ_INVALID;
		return 0;
	}

	if (mvNetaMaxCheck(txq, CONFIG_MV_ETH_TXQ))
		return -EINVAL;

	if (mv_eth_ctrl_txq_cpu_own(port, pp-&gt;txp, txq, 1))
		return -EINVAL;

	pp-&gt;txq_tos_map[tos] = (MV_U8) txq;

	return 0;
}

static int mv_eth_priv_init(struct eth_port *pp, int port)
{
	int cpu, i;
	u8	*ext_buf;

	TRC_INIT(0, 0, 0, 0);
	TRC_START();

	memset(pp, 0, sizeof(struct eth_port));

	pp-&gt;port = port;
	pp-&gt;txp_num = 1;
	pp-&gt;txp = 0;
	for_each_possible_cpu(cpu) {
		if ((MV_BIT_CHECK(pp-&gt;cpuMask, cpu)))
			pp-&gt;txq[cpu] = CONFIG_MV_ETH_TXQ_DEF;
	}

	pp-&gt;flags = 0;

#ifdef CONFIG_MV_ETH_BM_CPU
	pp-&gt;pool_long_num = mv_eth_bm_config_long_buf_num_get(port);
	if (pp-&gt;pool_long_num &gt; MV_BM_POOL_CAP_MAX)
		pp-&gt;pool_long_num = MV_BM_POOL_CAP_MAX;

	pp-&gt;pool_short_num = mv_eth_bm_config_short_buf_num_get(port);
	if (pp-&gt;pool_short_num &gt; MV_BM_POOL_CAP_MAX)
		pp-&gt;pool_short_num = MV_BM_POOL_CAP_MAX;
#else
	pp-&gt;pool_long_num = CONFIG_MV_ETH_RXQ * CONFIG_MV_ETH_RXQ_DESC * 2;
#endif /* CONFIG_MV_ETH_BM_CPU */

	for (i = 0; i &lt; 256; i++) {
		pp-&gt;txq_tos_map[i] = MV_ETH_TXQ_INVALID;

#ifdef CONFIG_MV_ETH_TX_SPECIAL
		pp-&gt;tx_special_check = NULL;
#endif /* CONFIG_MV_ETH_TX_SPECIAL */
	}

	mv_eth_port_config_parse(pp);

#ifdef CONFIG_MV_PON
	if (MV_PON_PORT(port)) {
		set_bit(MV_ETH_F_MH_BIT, &amp;(pp-&gt;flags));
		pp-&gt;txp_num = MV_ETH_MAX_TCONT();
		pp-&gt;txp = CONFIG_MV_PON_TXP_DEF;
		for_each_possible_cpu(i)
			pp-&gt;txq[i] = CONFIG_MV_PON_TXQ_DEF;
	}
#endif /* CONFIG_MV_PON */

#if defined(CONFIG_MV_ETH_RX_CSUM_OFFLOAD_DEF)
	pp-&gt;rx_csum_offload = 1;
#endif /* CONFIG_MV_ETH_RX_CSUM_OFFLOAD_DEF */

#ifdef CONFIG_MV_INCLUDE_SWITCH
	if (mvBoardSwitchConnectedPortGet(port) != -1) {
		set_bit(MV_ETH_F_SWITCH_BIT, &amp;(pp-&gt;flags));
		set_bit(MV_ETH_F_EXT_SWITCH_BIT, &amp;(pp-&gt;flags));
	}
#endif /* CONFIG_MV_INCLUDE_SWITCH */

	memset(&amp;pp-&gt;tx_done_timer, 0, sizeof(struct timer_list));
	pp-&gt;tx_done_timer.function = mv_eth_tx_done_timer_callback;
	init_timer(&amp;pp-&gt;tx_done_timer);
	clear_bit(MV_ETH_F_TX_DONE_TIMER_BIT, &amp;(pp-&gt;flags));
	memset(&amp;pp-&gt;cleanup_timer, 0, sizeof(struct timer_list));
	pp-&gt;cleanup_timer.function = mv_eth_cleanup_timer_callback;
	init_timer(&amp;pp-&gt;cleanup_timer);
	clear_bit(MV_ETH_F_CLEANUP_TIMER_BIT, &amp;(pp-&gt;flags));

	pp-&gt;weight = CONFIG_MV_ETH_RX_POLL_WEIGHT;
	rwlock_init(&amp;pp-&gt;rwlock);

	/* Init pool of external buffers for TSO, fragmentation, etc */
	spin_lock_init(&amp;pp-&gt;extLock);
	pp-&gt;extBufSize = CONFIG_MV_ETH_EXTRA_BUF_SIZE;
	pp-&gt;extArrStack = mvStackCreate(CONFIG_MV_ETH_EXTRA_BUF_NUM);
	if (pp-&gt;extArrStack == NULL) {
		printk(KERN_ERR "Error: failed create  extArrStack for port #%d\n", port);
		return -ENOMEM;
	}
	for (i = 0; i &lt; CONFIG_MV_ETH_EXTRA_BUF_NUM; i++) {
		ext_buf = mvOsMalloc(CONFIG_MV_ETH_EXTRA_BUF_SIZE);
		if (ext_buf == NULL) {
			printk(KERN_WARNING "%s Warning: %d of %d extra buffers allocated\n",
				__func__, i, CONFIG_MV_ETH_EXTRA_BUF_NUM);
			break;
		}
		mvStackPush(pp-&gt;extArrStack, (MV_U32)ext_buf);
	}

#ifdef CONFIG_MV_ETH_STAT_DIST
	pp-&gt;dist_stats.rx_dist = mvOsMalloc(sizeof(u32) * (CONFIG_MV_ETH_RXQ * CONFIG_MV_ETH_RXQ_DESC + 1));
	if (pp-&gt;dist_stats.rx_dist != NULL) {
		pp-&gt;dist_stats.rx_dist_size = CONFIG_MV_ETH_RXQ * CONFIG_MV_ETH_RXQ_DESC + 1;
		memset(pp-&gt;dist_stats.rx_dist, 0, sizeof(u32) * pp-&gt;dist_stats.rx_dist_size);
	} else
		printk(KERN_ERR "ethPort #%d: Can't allocate %d bytes for rx_dist\n",
		       pp-&gt;port, sizeof(u32) * (CONFIG_MV_ETH_RXQ * CONFIG_MV_ETH_RXQ_DESC + 1));

	pp-&gt;dist_stats.tx_done_dist =
	    mvOsMalloc(sizeof(u32) * (pp-&gt;txp_num * CONFIG_MV_ETH_TXQ * CONFIG_MV_ETH_TXQ_DESC + 1));
	if (pp-&gt;dist_stats.tx_done_dist != NULL) {
		pp-&gt;dist_stats.tx_done_dist_size = pp-&gt;txp_num * CONFIG_MV_ETH_TXQ * CONFIG_MV_ETH_TXQ_DESC + 1;
		memset(pp-&gt;dist_stats.tx_done_dist, 0, sizeof(u32) * pp-&gt;dist_stats.tx_done_dist_size);
	} else
		printk(KERN_ERR "ethPort #%d: Can't allocate %d bytes for tx_done_dist\n",
		       pp-&gt;port, sizeof(u32) * (pp-&gt;txp_num * CONFIG_MV_ETH_TXQ * CONFIG_MV_ETH_TXQ_DESC + 1));
#endif /* CONFIG_MV_ETH_STAT_DIST */

	return 0;
}

/***********************************************************************************
 ***  noqueue net device
 ***********************************************************************************/
extern struct Qdisc noop_qdisc;
void mv_eth_set_noqueue(struct net_device *dev, int enable)
{
	struct netdev_queue *txq = netdev_get_tx_queue(dev, 0);

	if (dev-&gt;flags &amp; IFF_UP) {
		printk(KERN_ERR "%s: device or resource busy, take it down\n", dev-&gt;name);
		return;
	}
	dev-&gt;tx_queue_len = enable ? 0 : CONFIG_MV_ETH_TXQ_DESC;

	if (txq)
		txq-&gt;qdisc_sleeping = &amp;noop_qdisc;
	else
		printk(KERN_ERR "%s: txq #0 is NULL\n", dev-&gt;name);

	printk(KERN_ERR "%s: device tx queue len is %d\n", dev-&gt;name, (int)dev-&gt;tx_queue_len);

}

/***********************************************************************************
 ***  print RX bm_pool status
 ***********************************************************************************/
void mv_eth_pool_status_print(int pool)
{
	struct bm_pool *bm_pool = &amp;mv_eth_pool[pool];

	printk(KERN_ERR "\nRX Pool #%d: pkt_size=%d, BM-HW support - %s\n",
	       pool, bm_pool-&gt;pkt_size, mv_eth_pool_bm(bm_pool) ? "Yes" : "No");

	printk(KERN_ERR "bm_pool=%p, stack=%p, capacity=%d, buf_num=%d, port_map=0x%x missed=%d\n",
	       bm_pool-&gt;bm_pool, bm_pool-&gt;stack, bm_pool-&gt;capacity, bm_pool-&gt;buf_num,
		   bm_pool-&gt;port_map, bm_pool-&gt;missed);

#ifdef CONFIG_MV_ETH_STAT_ERR
	printk(KERN_ERR "Errors: skb_alloc_oom=%u, stack_empty=%u, stack_full=%u\n",
	       bm_pool-&gt;stats.skb_alloc_oom, bm_pool-&gt;stats.stack_empty, bm_pool-&gt;stats.stack_full);
#endif /* #ifdef CONFIG_MV_ETH_STAT_ERR */

#ifdef CONFIG_MV_ETH_STAT_DBG
	printk(KERN_ERR "skb_alloc_ok=%u, bm_put=%u, stack_put=%u, stack_get=%u\n",
	       bm_pool-&gt;stats.skb_alloc_ok, bm_pool-&gt;stats.bm_put, bm_pool-&gt;stats.stack_put, bm_pool-&gt;stats.stack_get);

	printk(KERN_ERR "skb_recycled_ok=%u, skb_recycled_err=%u\n",
	       bm_pool-&gt;stats.skb_recycled_ok, bm_pool-&gt;stats.skb_recycled_err);
#endif /* CONFIG_MV_ETH_STAT_DBG */

	if (bm_pool-&gt;stack)
		mvStackStatus(bm_pool-&gt;stack, 0);

	memset(&amp;bm_pool-&gt;stats, 0, sizeof(bm_pool-&gt;stats));
}


/***********************************************************************************
 ***  print ext pool status
 ***********************************************************************************/
void mv_eth_ext_pool_print(struct eth_port *pp)
{
	printk(KERN_ERR "\nExt Pool Stack: bufSize = %u bytes\n", pp-&gt;extBufSize);
	mvStackStatus(pp-&gt;extArrStack, 0);
}

/***********************************************************************************
 ***  print net device status
 ***********************************************************************************/
void mv_eth_netdev_print(struct net_device *dev)
{
	struct eth_netdev *dev_priv = MV_DEV_PRIV(dev);

	printk(KERN_ERR "%s net_device status: dev=%p, pp=%p\n\n", dev-&gt;name, dev, MV_ETH_PRIV(dev));
	printk(KERN_ERR "ifIdx=%d, features=0x%x, flags=0x%x, mtu=%u, size=%d, MAC=" MV_MACQUAD_FMT "\n",
	       dev-&gt;ifindex, (unsigned int)(dev-&gt;features), (unsigned int)(dev-&gt;flags),
	       dev-&gt;mtu, RX_PKT_SIZE(dev-&gt;mtu), MV_MACQUAD(dev-&gt;dev_addr));

	if (dev_priv)
		printk(KERN_ERR "group=%d, tx_vlan_mh=0x%04x, switch_port_map=0x%x, switch_port_link_map=0x%x\n",
		       dev_priv-&gt;group, dev_priv-&gt;tx_vlan_mh, dev_priv-&gt;port_map, dev_priv-&gt;link_map);
}

void mv_eth_status_print(void)
{
	printk(KERN_ERR "totals: ports=%d, devs=%d\n", mv_eth_ports_num, mv_net_devs_num);

#ifdef CONFIG_NET_SKB_RECYCLE
	printk(KERN_ERR "SKB recycle = %s\n", mv_ctrl_recycle ? "Enabled" : "Disabled");
#endif /* CONFIG_NET_SKB_RECYCLE */

#ifdef CONFIG_MV_ETH_PNC
	printk(KERN_ERR "PnC control = %s\n", mv_eth_pnc_ctrl_en ? "Enabled" : "Disabled");
#endif /* CONFIG_MV_ETH_PNC */
}

/***********************************************************************************
 ***  print Ethernet port status
 ***********************************************************************************/
void mv_eth_port_status_print(unsigned int port)
{
	int txp, q;
	struct eth_port *pp = mv_eth_port_by_id(port);

	if (!pp)
		return;

	printk(KERN_ERR "\n");
	printk(KERN_ERR "port=%d, flags=0x%lx, rx_weight=%d\n", port, pp-&gt;flags, pp-&gt;weight);
	if ((!(pp-&gt;flags &amp; MV_ETH_F_SWITCH)) &amp;&amp; (pp-&gt;flags &amp; MV_ETH_F_CONNECT_LINUX))
		printk(KERN_ERR "%s: ", pp-&gt;dev-&gt;name);
	else
		printk(KERN_ERR "port %d: ", port);

	mv_eth_link_status_print(port);

#ifdef CONFIG_MV_ETH_NFP
	printk(KERN_ERR "NFP = ");
	if (pp-&gt;flags &amp; MV_ETH_F_NFP_EN)
		printk(KERN_CONT "Enabled\n");
	else
		printk(KERN_CONT "Disabled\n");
#endif /* CONFIG_MV_ETH_NFP */

	printk(KERN_ERR "rxq_coal(pkts)[ q]   = ");
	for (q = 0; q &lt; CONFIG_MV_ETH_RXQ; q++)
		printk(KERN_CONT "%3d ", mvNetaRxqPktsCoalGet(port, q));

	printk(KERN_CONT "\n");
	printk(KERN_ERR "rxq_coal(usec)[ q]   = ");
	for (q = 0; q &lt; CONFIG_MV_ETH_RXQ; q++)
		printk(KERN_CONT "%3d ", mvNetaRxqTimeCoalGet(port, q));

	printk(KERN_CONT "\n");
	printk(KERN_ERR "rxq_desc(num)[ q]    = ");
	for (q = 0; q &lt; CONFIG_MV_ETH_RXQ; q++)
		printk(KERN_CONT "%3d ", pp-&gt;rxq_ctrl[q].rxq_size);

	printk(KERN_CONT "\n");
	for (txp = 0; txp &lt; pp-&gt;txp_num; txp++) {
		printk(KERN_ERR "txq_coal(pkts)[%2d.q] = ", txp);
		for (q = 0; q &lt; CONFIG_MV_ETH_TXQ; q++)
			printk(KERN_CONT "%3d ", mvNetaTxDonePktsCoalGet(port, txp, q));
		printk(KERN_CONT "\n");

		printk(KERN_ERR "txq_mod(F,C,H)[%2d.q] = ", txp);
		for (q = 0; q &lt; CONFIG_MV_ETH_TXQ; q++) {
			int val, mode;

			mode = mv_eth_ctrl_txq_mode_get(port, txp, q, &amp;val);
			if (mode == MV_ETH_TXQ_CPU)
				printk(KERN_CONT " C%-d ", val);
			else if (mode == MV_ETH_TXQ_HWF)
				printk(KERN_CONT " H%-d ", val);
			else
				printk(KERN_CONT "  F ");
		}
		printk(KERN_CONT "\n");

		printk(KERN_ERR "txq_desc(num) [%2d.q] = ", txp);
		for (q = 0; q &lt; CONFIG_MV_ETH_TXQ; q++) {
			struct tx_queue *txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + q];
			printk(KERN_CONT "%3d ", txq_ctrl-&gt;txq_size);
		}
		printk(KERN_CONT "\n");
	}
	printk(KERN_ERR "\n");

#ifdef CONFIG_MV_ETH_TXDONE_ISR
	printk(KERN_ERR "Do tx_done in NAPI context triggered by ISR\n");
	for (txp = 0; txp &lt; pp-&gt;txp_num; txp++) {
		printk(KERN_ERR "txcoal(pkts)[%2d.q] = ", txp);
		for (q = 0; q &lt; CONFIG_MV_ETH_TXQ; q++)
			printk(KERN_CONT "%3d ", mvNetaTxDonePktsCoalGet(port, txp, q));
		printk(KERN_CONT "\n");
	}
	printk(KERN_ERR "\n");
#else
	printk(KERN_ERR "Do tx_done in TX or Timer context: tx_done_threshold=%d\n", mv_ctrl_txdone);
#endif /* CONFIG_MV_ETH_TXDONE_ISR */

	printk(KERN_ERR "txp=%d, zero_pad=%s, mh_en=%s (0x%04x), tx_cmd=0x%08x\n",
	       pp-&gt;txp, (pp-&gt;flags &amp; MV_ETH_F_NO_PAD) ? "Disabled" : "Enabled",
	       (pp-&gt;flags &amp; MV_ETH_F_MH) ? "Enabled" : "Disabled", pp-&gt;tx_mh, pp-&gt;hw_cmd);

	printk(KERN_CONT "\n");
	printk(KERN_CONT "CPU:   txq_def   causeRxTx    napi\n");
	{
		int cpu;
		for_each_possible_cpu(cpu) {
			if (MV_BIT_CHECK(pp-&gt;cpuMask, cpu))
				printk(KERN_ERR "  %d:      %d      0x%08x     %d\n",
					cpu, pp-&gt;txq[cpu], pp-&gt;causeRxTx[cpu], test_bit(NAPI_STATE_SCHED, &amp;pp-&gt;napi[cpu]-&gt;state));
		}
	}
	printk(KERN_CONT "\n");

#ifdef CONFIG_MV_ETH_SWITCH
	if (pp-&gt;flags &amp; MV_ETH_F_SWITCH)
		mv_eth_switch_status_print(port);
#endif /* CONFIG_MV_ETH_SWITCH */
}

/***********************************************************************************
 ***  print port statistics
 ***********************************************************************************/

void mv_eth_port_stats_print(unsigned int port)
{
	struct eth_port *pp = mv_eth_port_by_id(port);
	struct port_stats *stat = NULL;
	struct tx_queue *txq_ctrl;
	int txp, queue;
	u32 total_rx_ok, total_rx_fill_ok;
#ifdef CONFIG_MV_ETH_STAT_INF
	int i;
#endif

	TRC_OUTPUT();

	if (pp == NULL) {
		printk(KERN_ERR "eth_stats_print: wrong port number %d\n", port);
		return;
	}
	stat = &amp;(pp-&gt;stats);

#ifdef CONFIG_MV_ETH_STAT_ERR
	printk(KERN_ERR "\n====================================================\n");
	printk(KERN_ERR "ethPort_%d: Errors", port);
	printk(KERN_CONT "\n-------------------------------\n");
	printk(KERN_ERR "rx_error......................%10u\n", stat-&gt;rx_error);
	printk(KERN_ERR "tx_timeout....................%10u\n", stat-&gt;tx_timeout);
	printk(KERN_ERR "tx_netif_stop.................%10u\n", stat-&gt;netif_stop);
	printk(KERN_ERR "netif_wake....................%10u\n", stat-&gt;netif_wake);
	printk(KERN_ERR "ext_stack_empty...............%10u\n", stat-&gt;ext_stack_empty);
	printk(KERN_ERR "ext_stack_full ...............%10u\n", stat-&gt;ext_stack_full);
#endif /* CONFIG_MV_ETH_STAT_ERR */

#ifdef CONFIG_MV_ETH_STAT_INF
	printk(KERN_ERR "\n====================================================\n");
	printk(KERN_ERR "ethPort_%d: interrupt statistics", port);
	printk(KERN_CONT "\n-------------------------------\n");
	printk(KERN_ERR "irq...........................%10u\n", stat-&gt;irq);
	printk(KERN_ERR "irq_err.......................%10u\n", stat-&gt;irq_err);

	printk(KERN_ERR "\n====================================================\n");
	printk(KERN_ERR "ethPort_%d: Events", port);
	printk(KERN_CONT "\n-------------------------------\n");
	for (i = 0; i &lt; CONFIG_NR_CPUS; i++) {
		printk(KERN_ERR "poll[%d].....................%10u\n", i, stat-&gt;poll[i]);
		printk(KERN_ERR "poll_exit[%d]................%10u\n", i, stat-&gt;poll_exit[i]);
	}
	printk(KERN_ERR "tx_fragmentation..............%10u\n", stat-&gt;tx_fragment);
	printk(KERN_ERR "tx_done_event.................%10u\n", stat-&gt;tx_done);
	printk(KERN_ERR "tx_done_timer_event...........%10u\n", stat-&gt;tx_done_timer);
	printk(KERN_ERR "cleanup_timer_event...........%10u\n", stat-&gt;cleanup_timer);
	printk(KERN_ERR "link..........................%10u\n", stat-&gt;link);
#ifdef CONFIG_MV_ETH_RX_SPECIAL
	printk(KERN_ERR "rx_special....................%10u\n", stat-&gt;rx_special);
#endif /* CONFIG_MV_ETH_RX_SPECIAL */
#ifdef CONFIG_MV_ETH_TX_SPECIAL
	printk(KERN_ERR "tx_special....................%10u\n", stat-&gt;tx_special);
#endif /* CONFIG_MV_ETH_TX_SPECIAL */
#endif /* CONFIG_MV_ETH_STAT_INF */

	printk(KERN_ERR "\n");
	total_rx_ok = total_rx_fill_ok = 0;
	printk(KERN_ERR "RXQ:       rx_ok      rx_fill_ok     missed\n\n");
	for (queue = 0; queue &lt; CONFIG_MV_ETH_RXQ; queue++) {
		u32 rxq_ok = 0, rxq_fill = 0;

#ifdef CONFIG_MV_ETH_STAT_DBG
		rxq_ok = stat-&gt;rxq[queue];
		rxq_fill = stat-&gt;rxq_fill[queue];
#endif /* CONFIG_MV_ETH_STAT_DBG */

		printk(KERN_ERR "%3d:  %10u    %10u          %d\n",
			queue, rxq_ok, rxq_fill,
			pp-&gt;rxq_ctrl[queue].missed);
		total_rx_ok += rxq_ok;
		total_rx_fill_ok += rxq_fill;
	}
	printk(KERN_ERR "SUM:  %10u    %10u\n", total_rx_ok, total_rx_fill_ok);

#ifdef CONFIG_MV_ETH_STAT_DBG
	{
		printk(KERN_ERR "\n====================================================\n");
		printk(KERN_ERR "ethPort_%d: Debug statistics", port);
		printk(KERN_CONT "\n-------------------------------\n");

		printk(KERN_ERR "\n");

		printk(KERN_ERR "rx_nfp....................%10u\n", stat-&gt;rx_nfp);
		printk(KERN_ERR "rx_nfp_drop...............%10u\n", stat-&gt;rx_nfp_drop);

		printk(KERN_ERR "rx_gro....................%10u\n", stat-&gt;rx_gro);
		printk(KERN_ERR "rx_gro_bytes .............%10u\n", stat-&gt;rx_gro_bytes);

		printk(KERN_ERR "tx_tso....................%10u\n", stat-&gt;tx_tso);
		printk(KERN_ERR "tx_tso_bytes .............%10u\n", stat-&gt;tx_tso_bytes);

		printk(KERN_ERR "rx_netif..................%10u\n", stat-&gt;rx_netif);
		printk(KERN_ERR "rx_drop_sw................%10u\n", stat-&gt;rx_drop_sw);
		printk(KERN_ERR "rx_csum_hw................%10u\n", stat-&gt;rx_csum_hw);
		printk(KERN_ERR "rx_csum_sw................%10u\n", stat-&gt;rx_csum_sw);


		printk(KERN_ERR "tx_skb_free...............%10u\n", stat-&gt;tx_skb_free);
		printk(KERN_ERR "tx_sg.....................%10u\n", stat-&gt;tx_sg);
		printk(KERN_ERR "tx_csum_hw................%10u\n", stat-&gt;tx_csum_hw);
		printk(KERN_ERR "tx_csum_sw................%10u\n", stat-&gt;tx_csum_sw);

		printk(KERN_ERR "ext_stack_get.............%10u\n", stat-&gt;ext_stack_get);
		printk(KERN_ERR "ext_stack_put ............%10u\n", stat-&gt;ext_stack_put);

		printk(KERN_ERR "\n");
	}
#endif /* CONFIG_MV_ETH_STAT_DBG */

	printk(KERN_ERR "\n");
	printk(KERN_ERR "TXP-TXQ:  count        send          done      no_resource\n\n");

	for (txp = 0; txp &lt; pp-&gt;txp_num; txp++) {
		for (queue = 0; queue &lt; CONFIG_MV_ETH_TXQ; queue++) {
			u32 txq_tx = 0, txq_txdone = 0, txq_err = 0;

			txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + queue];
#ifdef CONFIG_MV_ETH_STAT_DBG
			txq_tx = txq_ctrl-&gt;stats.txq_tx;
			txq_txdone =  txq_ctrl-&gt;stats.txq_txdone;
#endif /* CONFIG_MV_ETH_STAT_DBG */
#ifdef CONFIG_MV_ETH_STAT_ERR
			txq_err = txq_ctrl-&gt;stats.txq_err;
#endif /* CONFIG_MV_ETH_STAT_ERR */

			printk(KERN_ERR "%d-%d:      %3d    %10u    %10u    %10u\n",
			       txp, queue, txq_ctrl-&gt;txq_count, txq_tx,
			       txq_txdone, txq_err);

			memset(&amp;txq_ctrl-&gt;stats, 0, sizeof(txq_ctrl-&gt;stats));
		}
	}
	printk(KERN_ERR "\n\n");

	memset(stat, 0, sizeof(struct port_stats));

	/* RX pool statistics */
#ifdef CONFIG_MV_ETH_BM_CPU
	if (pp-&gt;pool_short)
		mv_eth_pool_status_print(pp-&gt;pool_short-&gt;pool);
#endif /* CONFIG_MV_ETH_BM_CPU */

	if (pp-&gt;pool_long)
		mv_eth_pool_status_print(pp-&gt;pool_long-&gt;pool);

		mv_eth_ext_pool_print(pp);

#ifdef CONFIG_MV_ETH_STAT_DIST
	{
		int i;
		struct dist_stats *dist_stats = &amp;(pp-&gt;dist_stats);

		if (dist_stats-&gt;rx_dist) {
			printk(KERN_ERR "\n      Linux Path RX distribution\n");
			for (i = 0; i &lt; dist_stats-&gt;rx_dist_size; i++) {
				if (dist_stats-&gt;rx_dist[i] != 0) {
					printk(KERN_ERR "%3d RxPkts - %u times\n", i, dist_stats-&gt;rx_dist[i]);
					dist_stats-&gt;rx_dist[i] = 0;
				}
			}
		}

		if (dist_stats-&gt;tx_done_dist) {
			printk(KERN_ERR "\n      tx-done distribution\n");
			for (i = 0; i &lt; dist_stats-&gt;tx_done_dist_size; i++) {
				if (dist_stats-&gt;tx_done_dist[i] != 0) {
					printk(KERN_ERR "%3d TxDoneDesc - %u times\n", i, dist_stats-&gt;tx_done_dist[i]);
					dist_stats-&gt;tx_done_dist[i] = 0;
				}
			}
		}
#ifdef CONFIG_MV_ETH_TSO
		if (dist_stats-&gt;tx_tso_dist) {
			printk(KERN_ERR "\n      TSO stats\n");
			for (i = 0; i &lt; dist_stats-&gt;tx_tso_dist_size; i++) {
				if (dist_stats-&gt;tx_tso_dist[i] != 0) {
					printk(KERN_ERR "%3d KBytes - %u times\n", i, dist_stats-&gt;tx_tso_dist[i]);
					dist_stats-&gt;tx_tso_dist[i] = 0;
				}
			}
		}
#endif /* CONFIG_MV_ETH_TSO */
	}
#endif /* CONFIG_MV_ETH_STAT_DIST */
}


static int mv_eth_port_cleanup(int port)
{
	int txp, txq, rxq, i;
	struct eth_port *pp;
	struct tx_queue *txq_ctrl;
	struct rx_queue *rxq_ctrl;

	pp = mv_eth_port_by_id(port);

	if (pp == NULL)
		return -1;

	if (pp-&gt;flags &amp; MV_ETH_F_STARTED) {
		printk(KERN_ERR "%s: port %d is started, cannot cleanup\n", __func__, port);
		return -1;
	}

	/* Reset Tx ports */
	for (txp = 0; txp &lt; pp-&gt;txp_num; txp++) {
		if (mv_eth_txp_reset(port, txp))
			printk(KERN_ERR "Warning: Port %d Tx port %d reset failed\n", port, txp);
	}

	/* Delete Tx queues */
	for (txp = 0; txp &lt; pp-&gt;txp_num; txp++) {
		for (txq = 0; txq &lt; CONFIG_MV_ETH_TXQ; txq++) {
			txq_ctrl = &amp;pp-&gt;txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
			mv_eth_txq_delete(pp, txq_ctrl);
		}
	}

	mvOsFree(pp-&gt;txq_ctrl);
	pp-&gt;txq_ctrl = NULL;

#ifdef CONFIG_MV_ETH_STAT_DIST
	/* Free Tx Done distribution statistics */
	mvOsFree(pp-&gt;dist_stats.tx_done_dist);
#endif

	/* Reset RX ports */
	if (mv_eth_rx_reset(port))
		printk(KERN_ERR "Warning: Rx port %d reset failed\n", port);

	/* Delete Rx queues */
	for (rxq = 0; rxq &lt; CONFIG_MV_ETH_RXQ; rxq++) {
		rxq_ctrl = &amp;pp-&gt;rxq_ctrl[rxq];
		mvNetaRxqDelete(pp-&gt;port, rxq);
		rxq_ctrl-&gt;q = NULL;
	}

	mvOsFree(pp-&gt;rxq_ctrl);
	pp-&gt;rxq_ctrl = NULL;

#ifdef CONFIG_MV_ETH_STAT_DIST
	/* Free Rx distribution statistics */
	mvOsFree(pp-&gt;dist_stats.rx_dist);
#endif

	/* Free buffer pools */
	if (pp-&gt;pool_long) {
		mv_eth_pool_free(pp-&gt;pool_long-&gt;pool, pp-&gt;pool_long_num);
		pp-&gt;pool_long-&gt;port_map &amp;= ~(1 &lt;&lt; pp-&gt;port);
		pp-&gt;pool_long = NULL;
	}
#ifdef CONFIG_MV_ETH_BM_CPU
	if (pp-&gt;pool_short) {
		mv_eth_pool_free(pp-&gt;pool_short-&gt;pool, pp-&gt;pool_short_num);
		pp-&gt;pool_short-&gt;port_map &amp;= ~(1 &lt;&lt; pp-&gt;port);
		pp-&gt;pool_short = NULL;
	}
#endif /* CONFIG_MV_ETH_BM_CPU */

	/* Clear Marvell Header related modes - will be set again if needed on re-init */
	mvNetaMhSet(port, MV_NETA_MH_NONE);

	/* Clear any forced link, speed and duplex */
	mv_force_port_link_speed_fc(port, MV_ETH_SPEED_AN, 0);

	mvNetaPortDestroy(port);

	if (pp-&gt;flags &amp; MV_ETH_F_CONNECT_LINUX)
		for (i = 0; i &lt; CONFIG_MV_ETH_NAPI_GROUPS; i++)
			netif_napi_del(pp-&gt;napiGroup[i]);

	return 0;
}


int mv_eth_all_ports_cleanup(void)
{
	int port, pool, status = 0;

	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		status = mv_eth_port_cleanup(port);
		if (status != 0) {
			printk(KERN_ERR "Error: mv_eth_port_cleanup failed on port %d, stopping all ports cleanup\n", port);
			return status;
		}
	}

	for (pool = 0; pool &lt; MV_ETH_BM_POOLS; pool++)
		mv_eth_pool_destroy(pool);

	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		if (mv_eth_ports[port])
			mvOsFree(mv_eth_ports[port]);
	}

	memset(mv_eth_ports, 0, (mv_eth_ports_num * sizeof(struct eth_port *)));
	/* Note: not freeing mv_eth_ports - we will reuse them */

	return 0;
}

#ifdef CONFIG_MV_ETH_PNC_WOL

#define DEF_WOL_SIZE	42
MV_U8	wol_data[DEF_WOL_SIZE] = { 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
				   0x00, 0x00, 0x00, 0x00, 0x00, 0x39, 0x08, 0x00,
				   0x45, 0x00, 0x00, 0x4E, 0x00, 0x00, 0x00, 0x00,
				   0x00, 0x11, 0x00, 0x00, 0xc0, 0xa8, 0x01, 0xFA,
				   0x00, 0x00, 0x00, 0x00, 0x00, 0x89, 0x00, 0x89,
				   0x00, 0x3A };

MV_U8	wol_mask[DEF_WOL_SIZE] = { 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
				   0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
				   0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,
				   0x00, 0xff, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,
				   0x00, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,
				   0xff, 0xff };

int mv_eth_wol_pkts_check(int port)
{
	struct eth_port		*pp = mv_eth_port_by_id(port);
	struct neta_rx_desc *rx_desc;
	struct eth_pbuf     *pkt;
	struct bm_pool      *pool;
	int	                rxq, rx_done, i, wakeup, ruleId;
	MV_NETA_RXQ_CTRL    *rx_ctrl;

	write_lock(&amp;pp-&gt;rwlock);
	wakeup = 0;
	for (rxq = 0; rxq &lt; CONFIG_MV_ETH_RXQ; rxq++) {
		rx_ctrl = pp-&gt;rxq_ctrl[rxq].q;

		if (rx_ctrl == NULL)
			continue;

		rx_done = mvNetaRxqBusyDescNumGet(pp-&gt;port, rxq);

		for (i = 0; i &lt; rx_done; i++) {
			rx_desc = mvNetaRxqNextDescGet(rx_ctrl);
			mvOsCacheLineInv(NULL, rx_desc);

#if defined(MV_CPU_BE)
			mvNetaRxqDescSwap(rx_desc);
#endif /* MV_CPU_BE */

			pkt = (struct eth_pbuf *)rx_desc-&gt;bufCookie;
			mvOsCacheInvalidate(NULL, pkt-&gt;pBuf + pkt-&gt;offset, rx_desc-&gt;dataSize);

			if (mv_pnc_wol_pkt_match(pp-&gt;port, pkt-&gt;pBuf + pkt-&gt;offset, rx_desc-&gt;dataSize, &amp;ruleId))
				wakeup = 1;

			pool = &amp;mv_eth_pool[pkt-&gt;pool];
			mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);

			if (wakeup) {
				printk(KERN_INFO "packet match WoL rule=%d found on port=%d, rxq=%d\n",
						ruleId, port, rxq);
				i++;
				break;
			}
		}
		if (i) {
			mvNetaRxqDescNumUpdate(pp-&gt;port, rxq, i, i);
			printk(KERN_INFO "port=%d, rxq=%d: %d of %d packets dropped\n", port, rxq, i, rx_done);
		}
		if (wakeup) {
			/* Failed enter WoL mode */
			write_unlock(&amp;pp-&gt;rwlock);
			return 1;
		}
	}
	write_unlock(&amp;pp-&gt;rwlock);
	return 0;
}

void mv_eth_wol_wakeup(int port)
{
	int rxq;

	/* Restore RXQ coalescing */
	for (rxq = 0; rxq &lt; CONFIG_MV_ETH_RXQ; rxq++) {
		mvNetaRxqPktsCoalSet(port, rxq, CONFIG_MV_ETH_RX_COAL_PKTS);
		mvNetaRxqTimeCoalSet(port, rxq, CONFIG_MV_ETH_RX_COAL_USEC);
	}

	/* Set PnC to Active filtering mode */
	mv_pnc_wol_wakeup(port);
	printk(KERN_INFO "Exit wakeOnLan mode on port #%d\n", port);

}

int mv_eth_wol_sleep(int port)
{
	int rxq;

	/* Set PnC to WoL filtering mode */
	mv_pnc_wol_sleep(port);

	/* Check received packets in all RXQs */
	/* If match one of WoL pattern - wakeup, not match - drop */
	if (mv_eth_wol_pkts_check(port)) {
		/* Set PNC to Active filtering mode */
		mv_pnc_wol_wakeup(port);
		printk(KERN_INFO "Failed to enter wakeOnLan mode on port #%d\n", port);
		return 1;
	}
	printk(KERN_INFO "Enter wakeOnLan mode on port #%d\n", port);

	/* Set RXQ coalescing to minimum */
	for (rxq = 0; rxq &lt; CONFIG_MV_ETH_RXQ; rxq++) {
		mvNetaRxqPktsCoalSet(port, rxq, 0);
		mvNetaRxqTimeCoalSet(port, rxq, 0);
	}

	return 0;
}
#endif /* CONFIG_MV_ETH_PNC_WOL */


#ifdef CONFIG_MV_PON
/* PON link status api */
PONLINKSTATUSPOLLFUNC pon_link_status_polling_func;

void pon_link_status_notify_func(MV_BOOL link_state)
{
	struct eth_port *pon_port = mv_eth_port_by_id(MV_PON_PORT_ID_GET());
	mv_eth_link_event(pon_port, 1);
}

/* called by PON module */
void mv_pon_link_state_register(PONLINKSTATUSPOLLFUNC poll_func, PONLINKSTATUSNOTIFYFUNC *notify_func)
{
	pon_link_status_polling_func = poll_func;
	*notify_func = pon_link_status_notify_func;
}

MV_BOOL mv_pon_link_status(void)
{
	if (pon_link_status_polling_func != NULL)
		return pon_link_status_polling_func();
	printk(KERN_ERR "pon_link_status_polling_func is uninitialized\n");
	return MV_FALSE;
}
#endif /* CONFIG_MV_PON */

/* Support for platform driver */

#ifdef CONFIG_CPU_IDLE
int mv_eth_suspend(struct platform_device *pdev, pm_message_t state)
{
#ifdef CONFIG_MV_ETH_PNC_WOL
	int port;

	printk(KERN_INFO "Entering WoL mode on All Neta interfaces\n");
	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		/* Configure ETH port to be in WoL mode */
		if (mv_eth_wol_sleep(port))
			return 1;
	}
#endif /* CONFIG_MV_ETH_PNC_WOL */

	return 0;
}

int mv_eth_resume(struct platform_device *pdev)
{
#ifdef CONFIG_MV_ETH_PNC_WOL
	int port;

	printk(KERN_INFO "Exiting WoL mode on All Neta interfaces\n");
	for (port = 0; port &lt; mv_eth_ports_num; port++) {
		/* Configure ETH port to work in normal mode */
		mv_eth_wol_wakeup(port);
	}
#endif /* CONFIG_MV_ETH_PNC_WOL */
	return 0;
}
#endif /* CONFIG_CPU_IDLE */

static int mv_eth_remove(struct platform_device *pdev)
{
    printk(KERN_INFO "Removing Marvell Ethernet Driver\n");
    return 0;
}

static void mv_eth_shutdown(struct platform_device *pdev)
{
    printk(KERN_INFO "Shutting Down Marvell Ethernet Driver\n");
}

static struct platform_driver mv_eth_driver = {
	.probe = mv_eth_probe,
	.remove = mv_eth_remove,
	.shutdown = mv_eth_shutdown,
#ifdef CONFIG_CPU_IDLE
	.suspend = mv_eth_suspend,
	.resume = mv_eth_resume,
#endif /* CONFIG_CPU_IDLE */
	.driver = {
		.name = "mv88fx_neta",
	},
};

static int __init mv_eth_init_module(void)
{
	return platform_driver_register(&amp;mv_eth_driver);
}
module_init(mv_eth_init_module);

static void __exit mv_eth_cleanup_module(void)
{
	platform_driver_unregister(&amp;mv_eth_driver);
}
module_exit(mv_eth_cleanup_module);


EXPORT_SYMBOL(dpa_rx_hook);
EXPORT_SYMBOL(dpa_os_tx_hook);
MODULE_DESCRIPTION("Marvell Ethernet Driver - www.marvell.com");
MODULE_AUTHOR("Dmitri Epshtein &lt;dima@marvell.com&gt;");
MODULE_LICENSE("GPL");
</Insert>
</MostRecent>
<Delta Version="0" Comment="" NL="\10" Encoding="text" Date="2012/01/29" Time="12:50:5000">
<Copy StartSeek="0" EndSeek="2707"/>
<Insert>
</Insert>
<Copy StartSeek="2734" EndSeek="144953"/>
</Delta>
<Delta Version="1" Comment="" NL="\10" Encoding="text" Date="2012/06/20" Time="18:01:29000">
<Copy StartSeek="0" EndSeek="2707"/>
<Insert>extern void dpa_classify()

</Insert>
<Copy StartSeek="2832" EndSeek="145050"/>
</Delta>
<Delta Version="2" Comment="" NL="\10" Encoding="text" Date="2012/06/20" Time="18:02:34000">
<Copy StartSeek="0" EndSeek="2720"/>
<Insert>extern int dpa_classify(struct eth_port *pp, int rxq, struct neta_rx_desc *rx_desc,
				struct eth_pbuf *pkt);

</Insert>
<Copy StartSeek="2836" EndSeek="145054"/>
</Delta>
<Delta Version="3" Comment="" NL="\10" Encoding="text" Date="2012/06/20" Time="18:02:41000">
<Copy StartSeek="0" EndSeek="2775"/>
<Insert>			struct neta_rx_desc *rx_desc, 
			struct eth_pbuf *pkt);

</Insert>
<Copy StartSeek="2832" EndSeek="145050"/>
</Delta>
<Delta Version="4" Comment="" NL="\10" Encoding="text" Date="2012/06/20" Time="18:02:45000">
<Copy StartSeek="0" EndSeek="36082"/>
<Insert>#ifdef CONFIG_MV_ETH_DEBUG_CODE
</Insert>
<Copy StartSeek="36116" EndSeek="36278"/>
<Insert>#endif /* CONFIG_MV_ETH_DEBUG_CODE */
</Insert>
<Copy StartSeek="36318" EndSeek="145054"/>
</Delta>
<Delta Version="5" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:30:46000">
<Copy StartSeek="0" EndSeek="36082"/>
<Insert>//#ifdef CONFIG_MV_ETH_DEBUG_CODE
</Insert>
<Copy StartSeek="36114" EndSeek="36276"/>
<Insert>//#endif /* CONFIG_MV_ETH_DEBUG_CODE */
</Insert>
<Copy StartSeek="36314" EndSeek="145050"/>
</Delta>
<Delta Version="6" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:34:23000">
<Copy StartSeek="0" EndSeek="36314"/>
<Insert>
</Insert>
<Copy StartSeek="36473" EndSeek="145208"/>
</Delta>
<Delta Version="7" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:40:33000">
<Copy StartSeek="0" EndSeek="36351"/>
<Insert>			data = rx_desc-&gt;bufCookie;
</Insert>
<Copy StartSeek="36351" EndSeek="145178"/>
</Delta>
<Delta Version="8" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:40:36000">
<Copy StartSeek="0" EndSeek="36327"/>
<Insert>			struct eth_hdr *eth;
</Insert>
<Copy StartSeek="36350" EndSeek="36425"/>
<Insert>		}
</Insert>
<Copy StartSeek="36595" EndSeek="145343"/>
</Delta>
<Delta Version="9" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:42:17000">
<Copy StartSeek="0" EndSeek="36585"/>
<Insert>
			}
		}
</Insert>
<Copy StartSeek="36594" EndSeek="145342"/>
</Delta>
<Delta Version="10" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:42:19000">
<Copy StartSeek="0" EndSeek="2720"/>
<Insert>extern int dpa_classify(struct eth_port *pp, int rxq, 
			struct neta_rx_desc *rx_desc, struct eth_pbuf *pkt);

</Insert>
<Copy StartSeek="2721" EndSeek="145231"/>
</Delta>
<Delta Version="11" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:43:12000">
<Copy StartSeek="0" EndSeek="2707"/>
<Insert>extern void 

</Insert>
<Copy StartSeek="2708" EndSeek="145218"/>
</Delta>
<Delta Version="12" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:43:18000">
<Copy StartSeek="0" EndSeek="2708"/>
<Copy StartSeek="2709" EndSeek="145219"/>
</Delta>
<Delta Version="13" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:43:37000">
<Copy StartSeek="0" EndSeek="36191"/>
<Copy StartSeek="36192" EndSeek="145220"/>
</Delta>
<Delta Version="14" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:43:50000">
<Copy StartSeek="0" EndSeek="36337"/>
<Insert>			   eth-&gt;h_dest[1] == 0xfe) {
</Insert>
<Copy StartSeek="36407" EndSeek="145258"/>
</Delta>
<Delta Version="15" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:51:50000">
<Copy StartSeek="0" EndSeek="36303"/>
<Insert>			if (eth-&gt;h_dest[0] == 0xca &amp;&amp; 
			   eth-&gt;h_dest[1] == 0xfe || eth-&gt;h_proto == ntohs(ETH_P_8021Q)) {
</Insert>
<Copy StartSeek="36409" EndSeek="145260"/>
</Delta>
<Delta Version="16" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="11:52:14000">
<Copy StartSeek="0" EndSeek="36303"/>
<Insert>			if ((eth-&gt;h_dest[0] == 0xca &amp;&amp; 
</Insert>
<Copy StartSeek="36343" EndSeek="145265"/>
</Delta>
<Delta Version="17" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:10:23000">
<Copy StartSeek="0" EndSeek="36303"/>
<Insert>			if (1 || (eth-&gt;h_dest[0] == 0xca &amp;&amp; 
</Insert>
<Copy StartSeek="36345" EndSeek="36416"/>
<Copy StartSeek="36478" EndSeek="145329"/>
</Delta>
<Delta Version="18" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:17:14000">
<Copy StartSeek="0" EndSeek="36345"/>
<Insert>			   eth-&gt;h_dest[1] == 0xfe) || eth-&gt;h_proto == ntohs(ETH_P_8021Q)) {
</Insert>
<Copy StartSeek="36418" EndSeek="145331"/>
</Delta>
<Delta Version="19" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:17:20000">
<Copy StartSeek="0" EndSeek="36448"/>
<Insert>					printk("%08x",eth-&gt;)
</Insert>
<Copy StartSeek="36486" EndSeek="145343"/>
</Delta>
<Delta Version="20" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:17:30000">
<Copy StartSeek="0" EndSeek="36228"/>
<Copy StartSeek="36238" EndSeek="145353"/>
</Delta>
<Delta Version="21" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:18:2000">
<Copy StartSeek="0" EndSeek="36238"/>
<Copy StartSeek="36239" EndSeek="145354"/>
</Delta>
<Delta Version="22" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:18:3000">
<Copy StartSeek="0" EndSeek="36459"/>
<Insert>					printk("%08x",eth-&gt;h_source[i]);
</Insert>
<Copy StartSeek="36495" EndSeek="145352"/>
</Delta>
<Delta Version="23" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:20:18000">
<Copy StartSeek="0" EndSeek="36459"/>
<Insert>					printk("%x",eth-&gt;h_source[i]);
</Insert>
<Copy StartSeek="36496" EndSeek="145353"/>
</Delta>
<Delta Version="24" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:20:23000">
<Copy StartSeek="0" EndSeek="36205"/>
<Insert>			struct ethhdr *eth;
			int i;

			eth = (struct eth_hdr *) ((char *)rx_desc-&gt;bufCookie + MV_ETH_MH_SIZE);
			if (1 || (eth-&gt;h_source[0] == 0xca &amp;&amp; 
			   eth-&gt;h_source[1] == 0xfe) || eth-&gt;h_proto == ntohs(ETH_P_8021Q)) {
				for (i = 0; i&lt; 6 ; i++) {
					printk("%x", eth-&gt;h_source[i]);
				}
				printk(KERN_ERR "(%s:%d) HAIM\n", __func__, __LINE__);
				mv_eth_rx_desc_print(rx_desc);
			}
		}
</Insert>
<Copy StartSeek="36301" EndSeek="145049"/>
</Delta>
<Delta Version="25" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:22:17000">
<Copy StartSeek="0" EndSeek="36205"/>
<Insert>			printk(KERN_ERR "(%s:%d) HAIM\n", __func__, __LINE__);
			mv_eth_rx_desc_print(rx_desc);
		}
</Insert>
<Copy StartSeek="36605" EndSeek="145353"/>
</Delta>
<Delta Version="26" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:29:6000">
<Copy StartSeek="0" EndSeek="36205"/>
<Insert>			struct ethhdr *eth;
			int i;

			eth = (struct eth_hdr *) ((char *)rx_desc-&gt;bufCookie + MV_ETH_MH_SIZE);
			if (1 || (eth-&gt;h_source[0] == 0xca &amp;&amp; 
			   eth-&gt;h_source[1] == 0xfe) || eth-&gt;h_proto == ntohs(ETH_P_8021Q)) {
				for (i = 0; i&lt; 6 ; i++) {
					printk("%x", eth-&gt;h_source[i]);
				}
				printk(KERN_ERR "(%s:%d) HAIM\n", __func__, __LINE__);
				mv_eth_rx_desc_print(rx_desc);
			}
		}
</Insert>
<Copy StartSeek="36301" EndSeek="145049"/>
</Delta>
<Delta Version="27" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:29:31000">
<Copy StartSeek="0" EndSeek="36192"/>
<Insert>/* HAIM */	{
</Insert>
<Copy StartSeek="36211" EndSeek="145055"/>
</Delta>
<Delta Version="28" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:29:37000">
<Copy StartSeek="0" EndSeek="36192"/>
<Insert>/* HAIM START */	{
</Insert>
<Copy StartSeek="36210" EndSeek="145054"/>
</Delta>
<Delta Version="29" Comment="" NL="\10" Encoding="text" Date="2012/06/27" Time="12:29:39000">
<Copy StartSeek="0" EndSeek="3818"/>
<Copy StartSeek="3859" EndSeek="145095"/>
</Delta>
<Delta Version="30" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="14:09:52000">
<Copy StartSeek="0" EndSeek="3839"/>
<Copy StartSeek="3840" EndSeek="145096"/>
</Delta>
<Delta Version="31" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="14:20:4000">
<Copy StartSeek="0" EndSeek="3839"/>
<Insert>
</Insert>
<Copy StartSeek="3911" EndSeek="145167"/>
</Delta>
<Delta Version="32" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="15:22:32000">
<Copy StartSeek="0" EndSeek="3839"/>
<Insert>int (dpa_callback)(struct neta_rx_desc *rx_desc, struct eth_pbuf *pkt);
</Insert>
<Copy StartSeek="3913" EndSeek="145169"/>
</Delta>
<Delta Version="33" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="15:23:56000">
<Copy StartSeek="0" EndSeek="3839"/>
<Insert>int (* dpa_callback)(struct neta_rx_desc *rx_desc, struct eth_pbuf *pkt);
</Insert>
<Copy StartSeek="3912" EndSeek="145168"/>
</Delta>
<Delta Version="34" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="15:24:18000">
<Copy StartSeek="0" EndSeek="145026"/>
<Insert>
</Insert>
<Copy StartSeek="145055" EndSeek="145196"/>
</Delta>
<Delta Version="35" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="15:24:50000">
<Copy StartSeek="0" EndSeek="38623"/>
<Insert>
</Insert>
<Copy StartSeek="38681" EndSeek="145253"/>
</Delta>
<Delta Version="36" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="15:26:33000">
<Copy StartSeek="0" EndSeek="38645"/>
<Insert>			*dpa_callback(rx_desc, pkt);
</Insert>
<Copy StartSeek="38676" EndSeek="145252"/>
</Delta>
<Delta Version="37" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="15:26:54000">
<Copy StartSeek="0" EndSeek="3839"/>
<Insert>int (*dpa_callback)(struct neta_rx_desc *rx_desc, struct eth_pbuf *pkt);
</Insert>
<Copy StartSeek="3915" EndSeek="38626"/>
<Insert>		if (dpa_callback) {
			dpa_callback(rx_desc, pkt);
</Insert>
<Copy StartSeek="38685" EndSeek="145091"/>
<Insert>EXPORT_SYMBOL(dpa_callback);
</Insert>
<Copy StartSeek="145123" EndSeek="145264"/>
</Delta>
<Delta Version="38" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="15:42:30000">
<Copy StartSeek="0" EndSeek="38626"/>
<Insert>		if (dpa_rx_callback) {
			dpa_rx_callback(rx_desc, pkt);
		}
</Insert>
<Copy StartSeek="38627" EndSeek="39194"/>
<Insert>
</Insert>
<Copy StartSeek="39258" EndSeek="145265"/>
</Delta>
<Delta Version="39" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="17:11:2000">
<Copy StartSeek="0" EndSeek="39219"/>
<Insert>			dpa_rx_callback(rx_desc, pkt);
</Insert>
<Copy StartSeek="39253" EndSeek="145265"/>
</Delta>
<Delta Version="40" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="17:11:10000">
<Copy StartSeek="0" EndSeek="3839"/>
<Insert>int (*dpa_rx_callback)(struct neta_rx_desc *rx_desc, struct eth_pbuf *pkt);
</Insert>
<Copy StartSeek="3907" EndSeek="145257"/>
</Delta>
<Delta Version="41" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="17:12:25000">
<Copy StartSeek="0" EndSeek="39087"/>
<Insert>
</Insert>
<Copy StartSeek="39150" EndSeek="39248"/>
<Insert>		if (dpa_rx_callback) {
			dpa_rx_callback(rx_desc, skb);
		}

</Insert>
<Copy StartSeek="39250" EndSeek="145257"/>
</Delta>
<Delta Version="42" Comment="" NL="\10" Encoding="text" Date="2012/07/19" Time="17:39:2000">
<Copy StartSeek="0" EndSeek="39112"/>
<Copy StartSeek="39122" EndSeek="39156"/>
<Insert>		}
</Insert>
<Copy StartSeek="39228" EndSeek="145335"/>
</Delta>
<Delta Version="43" Comment="" NL="\10" Encoding="text" Date="2012/07/22" Time="11:14:59000">
<Copy StartSeek="0" EndSeek="39156"/>
<Insert>			for (i = 0; i &lt; 30; i++) {
				printf("%2X",skb-&gt;data[i]);
</Insert>
<Copy StartSeek="39219" EndSeek="39224"/>
<Insert>
</Insert>
<Copy StartSeek="39241" EndSeek="145352"/>
</Delta>
<Delta Version="44" Comment="" NL="\10" Encoding="text" Date="2012/07/22" Time="11:15:25000">
<Copy StartSeek="0" EndSeek="39224"/>
<Insert>			printk("\n")l
</Insert>
<Copy StartSeek="39241" EndSeek="145352"/>
</Delta>
<Delta Version="45" Comment="" NL="\10" Encoding="text" Date="2012/07/22" Time="11:15:26000">
<Copy StartSeek="0" EndSeek="39156"/>
<Insert>			for (i = 0; i &lt; 20; i++) {
				printk("%2X", skb-&gt;data[i]);
			}
			printk("\n");
		}
</Insert>
<Copy StartSeek="39160" EndSeek="145267"/>
</Delta>
<Delta Version="46" Comment="" NL="\10" Encoding="text" Date="2012/07/22" Time="11:16:1000">
<Copy StartSeek="0" EndSeek="39122"/>
<Insert>			dpa_rx_callback(rx_desc, skb);
</Insert>
<Copy StartSeek="39162" EndSeek="145273"/>
</Delta>
<Delta Version="47" Comment="" NL="\10" Encoding="text" Date="2012/07/22" Time="12:17:39000">
<Copy StartSeek="0" EndSeek="39112"/>
<Insert>			int i;
</Insert>
<Copy StartSeek="39112" EndSeek="145263"/>
</Delta>
<Delta Version="48" Comment="" NL="\10" Encoding="text" Date="2012/07/23" Time="14:50:1000">
<Copy StartSeek="0" EndSeek="3839"/>
<Insert>int (*dpa_rx_callback)(struct neta_rx_desc *rx_desc, uint8_t *pkt);
</Insert>
<Copy StartSeek="3912" EndSeek="145268"/>
</Delta>
<Delta Version="49" Comment="" NL="\10" Encoding="text" Date="2012/07/23" Time="15:03:49000">
<Copy StartSeek="0" EndSeek="3839"/>
<Insert>uint32_t (*dpa_rx_callback)(struct neta_rx_desc *rx_desc, uint8_t *pkt);
</Insert>
<Copy StartSeek="3939" EndSeek="145295"/>
</Delta>
<Delta Version="50" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="11:55:44000">
<Copy StartSeek="0" EndSeek="39144"/>
<Insert>			dpa_rx_callback(rx_desc, skb-&gt;data);
</Insert>
<Copy StartSeek="39202" EndSeek="145313"/>
</Delta>
<Delta Version="51" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="11:56:17000">
<Copy StartSeek="0" EndSeek="3839"/>
<Insert>uint32_t (*dpa_rx_callback)(struct neta_rx_desc *rx_desc, uint8_t *pkt, 
</Insert>
<Copy StartSeek="4010" EndSeek="4037"/>
<Copy StartSeek="4038" EndSeek="145412"/>
</Delta>
<Delta Version="52" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:23:48000">
<Copy StartSeek="0" EndSeek="4037"/>
<Insert>
</Insert>
<Copy StartSeek="4037" EndSeek="145411"/>
</Delta>
<Delta Version="53" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:23:49000">
<Copy StartSeek="0" EndSeek="4010"/>
<Insert>			    void **dpa_cookie);
</Insert>
<Copy StartSeek="4036" EndSeek="145410"/>
</Delta>
<Delta Version="54" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:26:5000">
<Copy StartSeek="0" EndSeek="145269"/>
<Copy StartSeek="145301" EndSeek="145442"/>
</Delta>
<Delta Version="55" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:26:14000">
<Copy StartSeek="0" EndSeek="3938"/>
<Insert>uint32_t (*dpa_tx_callback)(struct neta_rx_desc *rx_desc, uint8_t *pkt,
			    void *dpa_cookie);
</Insert>
<Copy StartSeek="3999" EndSeek="145405"/>
</Delta>
<Delta Version="56" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:31:8000">
<Copy StartSeek="0" EndSeek="41597"/>
<Insert>
</Insert>
<Copy StartSeek="41673" EndSeek="145480"/>
</Delta>
<Delta Version="57" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:33:37000">
<Copy StartSeek="0" EndSeek="41621"/>
<Insert>		dpa_tx_callback(skb-&gt;data, &amp;skb-&gt;dpa_cookie);
</Insert>
<Copy StartSeek="41668" EndSeek="145479"/>
</Delta>
<Delta Version="58" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:33:43000">
<Copy StartSeek="0" EndSeek="41597"/>
<Insert>	if (dpa_tx_callback) {
		dpa_tx_callback(skb-&gt;data, skb-&gt;dpa_cookie);
	}

</Insert>
<Copy StartSeek="41598" EndSeek="42732"/>
<Insert>
</Insert>
<Copy StartSeek="42807" EndSeek="145479"/>
</Delta>
<Delta Version="59" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:41:10000">
<Copy StartSeek="0" EndSeek="36393"/>
<Insert>/* HAIM START */{
</Insert>
<Copy StartSeek="36416" EndSeek="145484"/>
</Delta>
<Delta Version="60" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:41:39000">
<Copy StartSeek="0" EndSeek="36393"/>
<Insert>/* HAIM DEBUGSTART */{
</Insert>
<Copy StartSeek="36417" EndSeek="145485"/>
</Delta>
<Delta Version="61" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:41:41000">
<Copy StartSeek="0" EndSeek="39185"/>
<Copy StartSeek="39186" EndSeek="39372"/>
<Insert>
</Insert>
<Copy StartSeek="39372" EndSeek="145280"/>
<Copy StartSeek="145281" EndSeek="145486"/>
</Delta>
<Delta Version="62" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:42:34000">
<Copy StartSeek="0" EndSeek="39211"/>
<Copy StartSeek="39238" EndSeek="145513"/>
</Delta>
<Delta Version="63" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:52:7000">
<Copy StartSeek="0" EndSeek="39211"/>
<Insert>			skb-&gt;dpa_cookie = NULL;
</Insert>
<Copy StartSeek="39211" EndSeek="145486"/>
</Delta>
<Delta Version="64" Comment="" NL="\10" Encoding="text" Date="2012/07/24" Time="13:53:31000">
<Copy StartSeek="0" EndSeek="3938"/>
<Insert>uint32_t (*dpa_tx_callback)(uint8_t *pkt, void *dpa_cookie);
</Insert>
<Copy StartSeek="3988" EndSeek="145475"/>
</Delta>
<Delta Version="65" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="10:43:15000">
<Copy StartSeek="0" EndSeek="42751"/>
<Insert>		dpa_tx_callback(skb-&gt;data, skb-&gt;dpa_cookie);
</Insert>
<Copy StartSeek="42775" EndSeek="145452"/>
</Delta>
<Delta Version="66" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="10:43:24000">
<Copy StartSeek="0" EndSeek="3938"/>
<Insert>uint32_t (*dpa_tx_callback)(struct sk_buff *skb);
</Insert>
<Copy StartSeek="4038" EndSeek="145502"/>
</Delta>
<Delta Version="67" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="10:48:41000">
<Copy StartSeek="0" EndSeek="42801"/>
<Insert>		dpa_tx_callback(skb);
	}

</Insert>
<Copy StartSeek="42912" EndSeek="145585"/>
</Delta>
<Delta Version="68" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="10:49:30000">
<Copy StartSeek="0" EndSeek="4057"/>
<Insert>
</Insert>
<Copy StartSeek="4057" EndSeek="145584"/>
</Delta>
<Delta Version="69" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="15:33:57000">
<Copy StartSeek="0" EndSeek="42875"/>
<Insert>				skb_transport_header(skb));
</Insert>
<Copy StartSeek="42903" EndSeek="145580"/>
</Delta>
<Delta Version="70" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="16:50:39000">
<Copy StartSeek="0" EndSeek="42846"/>
<Insert>				skb_network_header(skb),
				skb-&gt;transport_header);
	}

</Insert>
<Copy StartSeek="42880" EndSeek="145553"/>
</Delta>
<Delta Version="71" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="16:56:56000">
<Copy StartSeek="0" EndSeek="3938"/>
<Insert>uint32_t (*dpa_tx_callback)(uint8_t *pkt, void *dpa_cookie, struct iphdr *iph,
			    uint8_t *l4);
</Insert>
<Copy StartSeek="4018" EndSeek="145533"/>
</Delta>
<Delta Version="72" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="16:57:35000">
<Copy StartSeek="0" EndSeek="42826"/>
<Insert>				skb_network_header(skb));
</Insert>
<Copy StartSeek="42872" EndSeek="145549"/>
</Delta>
<Delta Version="73" Comment="" NL="\10" Encoding="text" Date="2012/07/29" Time="17:28:0000">
<Copy StartSeek="0" EndSeek="42780"/>
<Insert>		dpa_tx_callback(skb-&gt;data, skb-&gt;dpa_cookie,
</Insert>
<Copy StartSeek="42843" EndSeek="145566"/>
</Delta>
<Delta Version="74" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="15:01:10000">
<Copy StartSeek="0" EndSeek="42780"/>
<Insert>		dpa_tx_callback(skb-&gt;data - MV_ETH_MH_SIZE, skb-&gt;dpa_cookie,
</Insert>
<Copy StartSeek="42843" EndSeek="145566"/>
</Delta>
<Delta Version="75" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="15:04:43000">
<Copy StartSeek="0" EndSeek="39229"/>
<Insert>			dpa_rx_callback(rx_desc, skb-&gt;data, &amp;skb-&gt;dpa_cookie);
		}
</Insert>
<Copy StartSeek="39307" EndSeek="145582"/>
</Delta>
<Delta Version="76" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="19:55:21000">
<Copy StartSeek="0" EndSeek="39287"/>
<Insert>					pp-&gt;port);
</Insert>
<Copy StartSeek="39313" EndSeek="145592"/>
</Delta>
<Delta Version="77" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="19:55:45000">
<Copy StartSeek="0" EndSeek="39229"/>
<Insert>			dpa_rx_callback(rx_desc, skb-&gt;data, &amp;skb-&gt;dpa_cookie, 
					(uint32_t)pp-&gt;port);
		}
</Insert>
<Copy StartSeek="39291" EndSeek="145566"/>
</Delta>
<Delta Version="78" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="19:57:47000">
<Copy StartSeek="0" EndSeek="39229"/>
<Insert>			dpa_rx_callback(rx_desc, skb-&gt;data, &amp;skb-&gt;dpa_cookie);
		}
</Insert>
<Copy StartSeek="39334" EndSeek="145609"/>
</Delta>
<Delta Version="79" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:34:38000">
<Copy StartSeek="0" EndSeek="39229"/>
<Copy StartSeek="39318" EndSeek="39419"/>
<Insert>		}
</Insert>
<Copy StartSeek="39446" EndSeek="145721"/>
</Delta>
<Delta Version="80" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:35:29000">
<Copy StartSeek="0" EndSeek="35269"/>
<Copy StartSeek="35544" EndSeek="39694"/>
<Insert>			if (ret == ) {
			}
		}
</Insert>
<Copy StartSeek="39729" EndSeek="146004"/>
</Delta>
<Delta Version="81" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:36:7000">
<Copy StartSeek="0" EndSeek="2272"/>
<Insert>
</Insert>
<Copy StartSeek="2587" EndSeek="35583"/>
<Insert>struct dpa_entry {
	struct {
		struct dpa_pri_key cls_tuple;
		struct dpa_l2 l2;
	} org;
	struct dpa_entry *next;
	uint32_t state;
	struct {
		struct dpa_pri_key mod_tuple;
		struct dpa_l2 l2;
		uint32_t tx_port;
	} mod;
	struct dpa_counters counters;
	uint32_t mod_ops;
};

</Insert>
<Copy StartSeek="35583" EndSeek="146043"/>
</Delta>
<Delta Version="82" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:36:37000">
<Copy StartSeek="0" EndSeek="2568"/>
<Copy StartSeek="2650" EndSeek="146125"/>
</Delta>
<Delta Version="83" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:36:54000">
<Copy StartSeek="0" EndSeek="39625"/>
<Insert>			enum dpa_entry_states {
				DPE_NEW = 0,
				DPE_LRN,
				DPE_FWD,
				DPE_LCL,
			};
</Insert>
<Copy StartSeek="39625" EndSeek="146036"/>
</Delta>
<Delta Version="84" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:37:15000">
<Copy StartSeek="0" EndSeek="39751"/>
<Insert>
</Insert>
<Copy StartSeek="39765" EndSeek="146049"/>
</Delta>
<Delta Version="85" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:43:16000">
<Copy StartSeek="0" EndSeek="39726"/>
<Insert>			if (ret == DPE_FWD) {
</Insert>
<Copy StartSeek="39749" EndSeek="39763"/>
<Insert>			}
		}
</Insert>
<Copy StartSeek="39767" EndSeek="146042"/>
</Delta>
<Delta Version="86" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:43:20000">
<Copy StartSeek="0" EndSeek="35665"/>
<Copy StartSeek="38911" EndSeek="149288"/>
</Delta>
<Delta Version="87" Comment="" NL="\10" Encoding="text" Date="2012/07/30" Time="20:48:52000">
<Copy StartSeek="0" EndSeek="36336"/>
<Insert>	/* Do fragmentation if needed */
	if (mv_eth_nfp_need_fragment(res)) {
		frags = mv_eth_nfp_fragment_tx(pp, dev, res, txq_ctrl, pkt);
		if (frags == 0) {
			dev-&gt;stats.tx_dropped++;
			status = MV_DROPPED;
		}
		STAT_INFO(pp-&gt;stats.tx_fragment++);
		goto out;
	}

</Insert>
<Copy StartSeek="36336" EndSeek="149023"/>
</Delta>
<Delta Version="88" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:09:11000">
<Copy StartSeek="0" EndSeek="36072"/>
<Insert>	if ((res-&gt;flags &amp; MV_NFP_RES_TXQ_VALID) == 0)
		res-&gt;txq = pp-&gt;txq[smp_processor_id()];
</Insert>
<Copy StartSeek="36134" EndSeek="36135"/>
<Insert>	if ((res-&gt;flags &amp; MV_NFP_RES_TXP_VALID) == 0)
		res-&gt;txp = pp-&gt;txp;

</Insert>
<Copy StartSeek="36135" EndSeek="148926"/>
</Delta>
<Delta Version="89" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:09:42000">
<Copy StartSeek="0" EndSeek="36373"/>
<Insert>
</Insert>
<Copy StartSeek="36373" EndSeek="148925"/>
</Delta>
<Delta Version="90" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:09:44000">
<Copy StartSeek="0" EndSeek="37218"/>
<Insert>#ifdef CONFIG_MV_ETH_BM_CPU
	use_bm = 1;
#else
	use_bm = 0;
#endif /* CONFIG_MV_ETH_BM_CPU */

</Insert>
<Copy StartSeek="37218" EndSeek="37329"/>
<Insert>		use_bm = 0;
</Insert>
<Copy StartSeek="37329" EndSeek="38012"/>
<Insert>	/* FIXME: PON only? --BK */
	tx_desc-&gt;hw_cmd = pp-&gt;hw_cmd;

</Insert>
<Copy StartSeek="38035" EndSeek="148778"/>
</Delta>
<Delta Version="91" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:11:25000">
<Copy StartSeek="0" EndSeek="38273"/>
<Insert>	STAT_DBG(txq_ctrl-&gt;stats.txq_tx++);

</Insert>
<Copy StartSeek="38275" EndSeek="148742"/>
</Delta>
<Delta Version="92" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:11:40000">
<Copy StartSeek="0" EndSeek="35665"/>
<Insert>static void __dpa_wrapper_xmit(struct eth_pbuf *pkt, MV_NFP_RESULT *res)
</Insert>
<Copy StartSeek="35718" EndSeek="42406"/>
<Insert>			if (ret == DPE_FWD)
</Insert>
<Copy StartSeek="42460" EndSeek="42474"/>
<Insert>		}
</Insert>
<Copy StartSeek="42483" EndSeek="148758"/>
</Delta>
<Delta Version="93" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:33:3000">
<Copy StartSeek="0" EndSeek="35665"/>
<Insert>static void __dpa_wrapper_ffwd(struct eth_pbuf *pkt)
</Insert>
<Copy StartSeek="35741" EndSeek="42454"/>
<Insert>				__dpa_wrapper_ffwd(pkt);
</Insert>
<Copy StartSeek="42485" EndSeek="148783"/>
</Delta>
<Delta Version="94" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:33:56000">
<Copy StartSeek="0" EndSeek="46060"/>
<Insert>				(struct iphdr *)skb_network_header(skb));
</Insert>
<Copy StartSeek="46116" EndSeek="148793"/>
</Delta>
<Delta Version="95" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:47:55000">
<Copy StartSeek="0" EndSeek="4334"/>
<Insert>uint32_t (*dpa_tx_callback)(uint8_t *pkt, void *dpa_cookie, struct iphdr *iph);
</Insert>
<Copy StartSeek="4417" EndSeek="45976"/>
<Insert>	if (dpa_tx_callback) {
		dpa_tx_callback(skb-&gt;data + MV_ETH_MH_SIZE, skb-&gt;dpa_cookie,
</Insert>
<Copy StartSeek="46069" EndSeek="148629"/>
<Insert>EXPORT_SYMBOL(dpa_tx_callback);
</Insert>
<Copy StartSeek="148664" EndSeek="148805"/>
</Delta>
<Delta Version="96" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:49:26000">
<Copy StartSeek="0" EndSeek="4334"/>
<Insert>uint32_t (*dpa_os_tx_callback)(uint8_t *pkt, void *dpa_cookie, struct iphdr *iph);
</Insert>
<Copy StartSeek="4413" EndSeek="45972"/>
<Insert>	if (dpa_os_tx_callback) {
		dpa_os_tx_callback(skb-&gt;data + MV_ETH_MH_SIZE, skb-&gt;dpa_cookie,
</Insert>
<Copy StartSeek="46057" EndSeek="148617"/>
<Insert>EXPORT_SYMBOL(dpa_os_tx_callback);
</Insert>
<Copy StartSeek="148648" EndSeek="148789"/>
</Delta>
<Delta Version="97" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:51:5000">
<Copy StartSeek="0" EndSeek="4235"/>
<Insert>uint32_t (*dpa_rx_callback)(struct neta_rx_desc *rx_desc, uint8_t *pkt,
</Insert>
<Copy StartSeek="4303" EndSeek="42298"/>
<Insert>		if (dpa_rx_callback) {
			uint32_t ret = dpa_rx_callback(rx_desc, 
</Insert>
<Copy StartSeek="42359" EndSeek="148573"/>
<Insert>EXPORT_SYMBOL(dpa_rx_callback);
</Insert>
<Copy StartSeek="148601" EndSeek="148773"/>
</Delta>
<Delta Version="98" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:51:16000">
<Copy StartSeek="0" EndSeek="4330"/>
<Insert>uint32_t (*dpa_os_tx_hook)(uint8_t *pkt, void *dpa_cookie, struct iphdr *iph);
</Insert>
<Copy StartSeek="4431" EndSeek="148795"/>
</Delta>
<Delta Version="99" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:51:31000">
<Copy StartSeek="0" EndSeek="42463"/>
<Insert>				__dpa_wrapper_ffwd(pkt, );
</Insert>
<Copy StartSeek="42497" EndSeek="148798"/>
</Delta>
<Delta Version="100" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:52:7000">
<Copy StartSeek="0" EndSeek="35760"/>
<Insert>	struct net_device *dev = (struct net_device *)res-&gt;dev;
</Insert>
<Copy StartSeek="35825" EndSeek="148806"/>
</Delta>
<Delta Version="101" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="09:52:32000">
<Copy StartSeek="0" EndSeek="35920"/>
<Insert>	MV_STATUS status = MV_OK;
</Insert>
<Copy StartSeek="35920" EndSeek="36428"/>
<Insert>		status = MV_DROPPED;
</Insert>
<Copy StartSeek="36428" EndSeek="38323"/>
<Insert>	return status;
}
</Insert>
<Copy StartSeek="38326" EndSeek="38327"/>
<Insert>
</Insert>
<Copy StartSeek="38327" EndSeek="148740"/>
</Delta>
<Delta Version="102" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="11:58:2000">
<Copy StartSeek="0" EndSeek="36444"/>
<Insert>	if (res-&gt;flags &amp; MV_NFP_RES_L4_CSUM_NEEDED) {
</Insert>
<Copy StartSeek="36481" EndSeek="148730"/>
</Delta>
<Delta Version="103" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="11:59:54000">
<Copy StartSeek="0" EndSeek="36444"/>
<Copy StartSeek="36491" EndSeek="148777"/>
</Delta>
<Delta Version="104" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:00:26000">
<Copy StartSeek="0" EndSeek="36996"/>
<Insert>				((res-&gt;ipInfo.ipHdrLen &gt;&gt; 2) &lt;&lt; NETA_TX_IP_HLEN_OFFS);
		} else {
			tx_cmd |= NETA_TX_L3_IP6 |
</Insert>
<Copy StartSeek="36926" EndSeek="148607"/>
</Delta>
<Delta Version="105" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:02:39000">
<Copy StartSeek="0" EndSeek="2611"/>
<Insert>	DPE_MOD_RT = 2,
	DPE_MOD_NAT = 4,
</Insert>
<Copy StartSeek="2629" EndSeek="36771"/>
<Insert>	if (res-&gt;flags &amp; MV_NFP_RES_IP_INFO_VALID) {
</Insert>
<Copy StartSeek="36806" EndSeek="148579"/>
</Delta>
<Delta Version="106" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:03:20000">
<Copy StartSeek="0" EndSeek="36806"/>
<Insert>		if (res-&gt;ipInfo.family == MV_INET) {
			tx_cmd |= NETA_TX_L3_IP4 | NETA_TX_IP_CSUM_MASK |
</Insert>
<Copy StartSeek="36858" EndSeek="36987"/>
<Insert>		}
	}

</Insert>
<Copy StartSeek="36991" EndSeek="148535"/>
</Delta>
<Delta Version="107" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:03:45000">
<Copy StartSeek="0" EndSeek="36858"/>
<Insert>				((res-&gt;ipInfo.ipOffset - res-&gt;shift) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
				((res-&gt;ipInfo.ipHdrLen &gt;&gt; 2) &lt;&lt; NETA_TX_IP_HLEN_OFFS);
</Insert>
<Copy StartSeek="36989" EndSeek="148537"/>
</Delta>
<Delta Version="108" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:03:51000">
<Copy StartSeek="0" EndSeek="36992"/>
<Insert>
</Insert>
<Copy StartSeek="36992" EndSeek="148536"/>
</Delta>
<Delta Version="109" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:04:5000">
<Copy StartSeek="0" EndSeek="37209"/>
<Insert>		if (use_bm) {
			tx_cmd |= NETA_TX_BM_ENABLE_MASK | NETA_TX_BM_POOL_ID_MASK(pkt-&gt;pool);
			txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = (u32) NULL;
		} else
			txq_ctrl-&gt;shadow_txq[txq_ctrl-&gt;shadow_txq_put_i] = (u32) pkt;
</Insert>
<Copy StartSeek="37273" EndSeek="148370"/>
</Delta>
<Delta Version="110" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:06:52000">
<Copy StartSeek="0" EndSeek="35931"/>
<Insert>	int use_bm, pkt_offset, frags = 1;
</Insert>
<Copy StartSeek="35959" EndSeek="148362"/>
</Delta>
<Delta Version="111" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:07:0000">
<Copy StartSeek="0" EndSeek="35931"/>
<Insert>	int pkt_offset, frags = 1;
</Insert>
<Copy StartSeek="35948" EndSeek="148351"/>
</Delta>
<Delta Version="112" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:07:10000">
<Copy StartSeek="0" EndSeek="35880"/>
<Insert>	u32 tx_cmd, physAddr;
</Insert>
<Copy StartSeek="35913" EndSeek="36047"/>
<Insert>	res-&gt;txq = pp-&gt;txq[smp_processor_id()];
	res-&gt;txp = pp-&gt;txp;
</Insert>
<Copy StartSeek="36099" EndSeek="36100"/>
<Insert>	txq_ctrl = &amp;pp-&gt;txq_ctrl[res-&gt;txp * CONFIG_MV_ETH_TXQ + res-&gt;txq];
</Insert>
<Copy StartSeek="36158" EndSeek="37696"/>
<Insert>	mvNetaTxqPendDescAdd(pp-&gt;port, res-&gt;txp, res-&gt;txq, 1);
</Insert>
<Copy StartSeek="37742" EndSeek="148331"/>
</Delta>
<Delta Version="113" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:19:15000">
<Copy StartSeek="0" EndSeek="36641"/>
<Insert>
</Insert>
<Copy StartSeek="36679" EndSeek="36779"/>
<Insert>	if (dpe-&gt;mod_ops &amp; DPE_MOD_NAT) {
</Insert>
<Copy StartSeek="36868" EndSeek="148422"/>
</Delta>
<Delta Version="114" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:26:34000">
<Copy StartSeek="0" EndSeek="36641"/>
<Insert>	if (dpe-&gt;mod_ops &amp; DPE_MOD_L2) {

	}
</Insert>
<Copy StartSeek="36641" EndSeek="148384"/>
</Delta>
<Delta Version="115" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:26:41000">
<Copy StartSeek="0" EndSeek="37016"/>
<Copy StartSeek="37055" EndSeek="148423"/>
</Delta>
<Delta Version="116" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:27:33000">
<Copy StartSeek="0" EndSeek="37051"/>
<Insert>
</Insert>
<Copy StartSeek="37078" EndSeek="148449"/>
</Delta>
<Delta Version="117" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:28:22000">
<Copy StartSeek="0" EndSeek="37051"/>
<Insert>		tx_cmd |= NETA_TX_L4_UDP
</Insert>
<Copy StartSeek="37079" EndSeek="148450"/>
</Delta>
<Delta Version="118" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:28:24000">
<Copy StartSeek="0" EndSeek="37079"/>
<Insert>	}
</Insert>
<Copy StartSeek="37083" EndSeek="148451"/>
</Delta>
<Delta Version="119" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:28:27000">
<Copy StartSeek="0" EndSeek="37051"/>
<Insert>		tx_cmd |= NETA_TX_L4_UDP;

	}
</Insert>
<Copy StartSeek="37235" EndSeek="148603"/>
</Delta>
<Delta Version="120" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:31:6000">
<Copy StartSeek="0" EndSeek="37231"/>
<Insert>
	}
</Insert>
<Copy StartSeek="37234" EndSeek="148602"/>
</Delta>
<Delta Version="121" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:31:7000">
<Copy StartSeek="0" EndSeek="36445"/>
<Insert>	if (!(dpe-&gt;mod_ops &amp; DPE_MOD_L2)) {
</Insert>
<Copy StartSeek="36492" EndSeek="148612"/>
</Delta>
<Delta Version="122" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:49:11000">
<Copy StartSeek="0" EndSeek="36676"/>
<Insert>	/* tx_cmd - word accumulated by NFP processing */
	tx_cmd = res-&gt;tx_cmd;

</Insert>
<Copy StartSeek="36676" EndSeek="148537"/>
</Delta>
<Delta Version="123" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="12:49:50000">
<Copy StartSeek="0" EndSeek="36534"/>
<Insert>
		if (res-&gt;shift &lt; 0)
			pData += res-&gt;shift;

</Insert>
<Copy StartSeek="36534" EndSeek="148489"/>
</Delta>
<Delta Version="124" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:16:37000">
<Copy StartSeek="0" EndSeek="36445"/>
<Insert>	if (res-&gt;flags &amp; MV_NFP_RES_L4_CSUM_NEEDED) {
</Insert>
<Copy StartSeek="36482" EndSeek="148479"/>
</Delta>
<Delta Version="125" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:17:31000">
<Copy StartSeek="0" EndSeek="36524"/>
<Insert>		mvOsCacheMultiLineFlushInv(NULL, pData, (res-&gt;pWrite - pData));
</Insert>
<Copy StartSeek="36571" EndSeek="148460"/>
</Delta>
<Delta Version="126" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:18:20000">
<Copy StartSeek="0" EndSeek="37092"/>
<Insert>	pkt_offset = pkt-&gt;offset + res-&gt;shift;
</Insert>
<Copy StartSeek="37119" EndSeek="148447"/>
</Delta>
<Delta Version="127" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:19:18000">
<Copy StartSeek="0" EndSeek="37092"/>
<Insert>	pkt_offset = pkt-&gt;offset;
</Insert>
<Copy StartSeek="37094" EndSeek="148422"/>
</Delta>
<Delta Version="128" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:19:27000">
<Copy StartSeek="0" EndSeek="37092"/>
<Insert>	
</Insert>
<Copy StartSeek="37119" EndSeek="148447"/>
</Delta>
<Delta Version="129" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:19:40000">
<Copy StartSeek="0" EndSeek="36398"/>
<Insert>	/* l2 stays the same only in bridging mode */
</Insert>
<Copy StartSeek="36446" EndSeek="148448"/>
</Delta>
<Delta Version="130" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:21:5000">
<Copy StartSeek="0" EndSeek="36398"/>
<Insert>	/* pkt stays the same only in bridging mode */
</Insert>
<Copy StartSeek="36441" EndSeek="148443"/>
</Delta>
<Delta Version="131" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:21:9000">
<Copy StartSeek="0" EndSeek="36398"/>
<Insert>	/* pkt stays the same only in bridging */
</Insert>
<Copy StartSeek="36441" EndSeek="148443"/>
</Delta>
<Delta Version="132" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:21:14000">
<Copy StartSeek="0" EndSeek="36398"/>
<Insert>	/* pkt stays the same only if bridged  */
</Insert>
<Copy StartSeek="36440" EndSeek="148442"/>
</Delta>
<Delta Version="133" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:21:15000">
<Copy StartSeek="0" EndSeek="36735"/>
<Insert>			  ((res-&gt;ipInfo.ipOffset - res-&gt;shift) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
</Insert>
<Copy StartSeek="36793" EndSeek="148429"/>
</Delta>
<Delta Version="134" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:22:43000">
<Copy StartSeek="0" EndSeek="36793"/>
<Insert>			  ((res-&gt;ipInfo.ipHdrLen &gt;&gt; 2) &lt;&lt; NETA_TX_IP_HLEN_OFFS);
</Insert>
<Copy StartSeek="36857" EndSeek="148433"/>
</Delta>
<Delta Version="135" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:24:44000">
<Copy StartSeek="0" EndSeek="36793"/>
<Insert>			  ((/*res-&gt;ipInfo.ipHdrLen &gt;&gt; 2*/) &lt;&lt; NETA_TX_IP_HLEN_OFFS);
</Insert>
<Copy StartSeek="36859" EndSeek="148435"/>
</Delta>
<Delta Version="136" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:24:49000">
<Copy StartSeek="0" EndSeek="36735"/>
<Insert>			  ((res-&gt;ipInfo.ipOffset) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
</Insert>
<Copy StartSeek="36799" EndSeek="148441"/>
</Delta>
<Delta Version="137" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:25:18000">
<Copy StartSeek="0" EndSeek="36735"/>
<Insert>			  (( /*res-&gt;ipInfo.ipOffset */) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
</Insert>
<Copy StartSeek="36800" EndSeek="148442"/>
</Delta>
<Delta Version="138" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:25:22000">
<Copy StartSeek="0" EndSeek="36800"/>
<Insert>			  ((/* res-&gt;ipInfo.ipHdrLen &gt;&gt; 2 */) &lt;&lt; NETA_TX_IP_HLEN_OFFS);
</Insert>
<Copy StartSeek="36868" EndSeek="148444"/>
</Delta>
<Delta Version="139" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:27:1000">
<Copy StartSeek="0" EndSeek="35665"/>
<Insert>static void __dpa_wrapper_ffwd(struct eth_pbuf *pkt, struct dpa_entry *dpe)
</Insert>
<Copy StartSeek="35738" EndSeek="42106"/>
<Insert>				__dpa_wrapper_ffwd(pkt, dpe);
</Insert>
<Copy StartSeek="42137" EndSeek="148438"/>
</Delta>
<Delta Version="140" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:29:32000">
<Copy StartSeek="0" EndSeek="42137"/>
<Copy StartSeek="42185" EndSeek="148486"/>
</Delta>
<Delta Version="141" Comment="" NL="\10" Encoding="text" Date="2012/07/31" Time="15:30:39000">
<Copy StartSeek="0" EndSeek="35665"/>
<Insert>static void dpa_wrapper_fwd(struct eth_pbuf *pkt, struct dpa_entry *dpe)
{
</Insert>
<Copy StartSeek="35778" EndSeek="35994"/>
<Copy StartSeek="36111" EndSeek="148641"/>
</Delta>
<Delta Version="142" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="09:59:29000">
<Copy StartSeek="0" EndSeek="36887"/>
<Insert>			  (( /*res-&gt;ipInfo.ipOffset */ ) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
			  ((/* res-&gt;ipInfo.ipHdrLen &gt;&gt; 2 */ 5) &lt;&lt; NETA_TX_IP_HLEN_OFFS);
</Insert>
<Copy StartSeek="37026" EndSeek="148647"/>
</Delta>
<Delta Version="143" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:00:16000">
<Copy StartSeek="0" EndSeek="35994"/>
<Insert>	pIpHdr-&gt;ipOffset = NETA_RX_GET_IPHDR_OFFSET(pRxDesc);
	pIpHdr-&gt;ipHdrLen =  NETA_RX_GET_IPHDR_HDRLEN(pRxDesc) &lt;&lt; 2;

</Insert>
<Copy StartSeek="35994" EndSeek="148530"/>
</Delta>
<Delta Version="144" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:00:23000">
<Copy StartSeek="0" EndSeek="42181"/>
<Insert>				mv_eth_refill(pp, rxq, pkt, pool, rx_desc);
</Insert>
<Copy StartSeek="42181" EndSeek="148482"/>
</Delta>
<Delta Version="145" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:09:42000">
<Copy StartSeek="0" EndSeek="42150"/>
<Insert>				dpa_wrapper_fwd(pkt, dpe);
</Insert>
<Copy StartSeek="42190" EndSeek="148491"/>
</Delta>
<Delta Version="146" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:09:48000">
<Copy StartSeek="0" EndSeek="2293"/>
<Copy StartSeek="2462" EndSeek="148660"/>
</Delta>
<Delta Version="147" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:12:33000">
<Copy StartSeek="0" EndSeek="2462"/>
<Copy StartSeek="2579" EndSeek="148777"/>
</Delta>
<Delta Version="148" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:12:45000">
<Copy StartSeek="0" EndSeek="2579"/>
<Copy StartSeek="2662" EndSeek="148860"/>
</Delta>
<Delta Version="149" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:13:9000">
<Copy StartSeek="0" EndSeek="37139"/>
<Insert>			  ((NETA_RX_GET_IPHDR_OFFSET(pRxDesc)) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
			  ((NETA_RX_GET_IPHDR_HDRLEN(pRxDesc)) &lt;&lt; NETA_TX_IP_HLEN_OFFS);
</Insert>
<Copy StartSeek="37278" EndSeek="148860"/>
</Delta>
<Delta Version="150" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:14:7000">
<Copy StartSeek="0" EndSeek="2293"/>
<Copy StartSeek="2365" EndSeek="148932"/>
</Delta>
<Delta Version="151" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:14:35000">
<Copy StartSeek="0" EndSeek="42591"/>
<Insert>				dpa_wrapper_fwd(pkt, dpe, rx_desc);
</Insert>
<Copy StartSeek="42669" EndSeek="148970"/>
</Delta>
<Delta Version="152" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:15:13000">
<Copy StartSeek="0" EndSeek="36356"/>
<Insert>	u32 tx_cmd, physAddr, txq, txp;
</Insert>
<Copy StartSeek="36393" EndSeek="148974"/>
</Delta>
<Delta Version="153" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="10:15:44000">
<Copy StartSeek="0" EndSeek="46246"/>
<Insert>				(struct iphdr *)skb_network_header(skb), skb-&gt;dev);
</Insert>
<Copy StartSeek="46297" EndSeek="148969"/>
</Delta>
<Delta Version="154" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="14:16:10000">
<Copy StartSeek="0" EndSeek="38441"/>
<Insert>	
</Insert>
<Copy StartSeek="38470" EndSeek="148996"/>
</Delta>
<Delta Version="155" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="14:22:16000">
<Copy StartSeek="0" EndSeek="39710"/>
<Insert>			printk(KERN_ERR "(%s:%d) HAIM\n", __func__, __LINE__);
			mv_eth_rx_desc_print(rx_desc);
</Insert>
<Copy StartSeek="39816" EndSeek="149010"/>
</Delta>
<Delta Version="156" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="14:40:11000">
<Copy StartSeek="0" EndSeek="36439"/>
<Copy StartSeek="36499" EndSeek="149070"/>
</Delta>
<Delta Version="157" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="14:41:23000">
<Copy StartSeek="0" EndSeek="36439"/>
<Insert>	printk(KERN_ERR "(%s:%d) FASTFWD!\n", __func__, __LINE__);
</Insert>
<Copy StartSeek="36526" EndSeek="149097"/>
</Delta>
<Delta Version="158" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="16:10:32000">
<Copy StartSeek="0" EndSeek="36922"/>
<Copy StartSeek="36994" EndSeek="149169"/>
</Delta>
<Delta Version="159" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="16:11:12000">
<Copy StartSeek="0" EndSeek="42770"/>
<Insert>			if (ret == DPE_FWD) {
</Insert>
<Copy StartSeek="42800" EndSeek="149174"/>
</Delta>
<Delta Version="160" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="16:42:32000">
<Copy StartSeek="0" EndSeek="42800"/>
<Copy StartSeek="42875" EndSeek="149249"/>
</Delta>
<Delta Version="161" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:12:30000">
<Copy StartSeek="0" EndSeek="42770"/>
<Insert>			if (0 &amp;&amp; ret == DPE_FWD) {
</Insert>
<Copy StartSeek="42795" EndSeek="149244"/>
</Delta>
<Delta Version="162" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:12:32000">
<Copy StartSeek="0" EndSeek="36356"/>
<Insert>	u32 tx_cmd = 0, physAddr, txq, txp;
</Insert>
<Copy StartSeek="36397" EndSeek="149248"/>
</Delta>
<Delta Version="163" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:13:1000">
<Copy StartSeek="0" EndSeek="36356"/>
<Insert>	u32 tx_cmd = 0, physAddr, txq, txp = 0;
</Insert>
<Copy StartSeek="36394" EndSeek="149245"/>
</Delta>
<Delta Version="164" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:13:21000">
<Copy StartSeek="0" EndSeek="36356"/>
<Insert>	u32 tx_cmd = 0, physAddr, txq, txp ;
</Insert>
<Copy StartSeek="36393" EndSeek="149244"/>
</Delta>
<Delta Version="165" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:13:21000">
<Copy StartSeek="0" EndSeek="36356"/>
<Insert>	u32 tx_cmd = 0, physAddr, txq, txp;
</Insert>
<Copy StartSeek="36397" EndSeek="149248"/>
</Delta>
<Delta Version="166" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:14:8000">
<Copy StartSeek="0" EndSeek="36356"/>
<Insert>	u32 tx_cmd = 0, physAddr, txq = 0, txp;
</Insert>
<Copy StartSeek="36393" EndSeek="36614"/>
<Insert>	txq = pp-&gt;txq[smp_processor_id()];
</Insert>
<Copy StartSeek="36657" EndSeek="149251"/>
</Delta>
<Delta Version="167" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:14:27000">
<Copy StartSeek="0" EndSeek="36657"/>
<Insert>	txp = pp-&gt;txp;
</Insert>
<Copy StartSeek="36678" EndSeek="149256"/>
</Delta>
<Delta Version="168" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:14:40000">
<Copy StartSeek="0" EndSeek="38639"/>
<Insert>	
</Insert>
<Copy StartSeek="38691" EndSeek="149306"/>
</Delta>
<Delta Version="169" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:15:33000">
<Copy StartSeek="0" EndSeek="42994"/>
<Insert>						rx_desc);
</Insert>
<Copy StartSeek="43015" EndSeek="149311"/>
</Delta>
<Delta Version="170" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:18:58000">
<Copy StartSeek="0" EndSeek="36180"/>
<Insert>			    struct neta_rx_desc *rx_desc)
</Insert>
<Copy StartSeek="36226" EndSeek="149320"/>
</Delta>
<Delta Version="171" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:19:4000">
<Copy StartSeek="0" EndSeek="38698"/>
<Insert>	
</Insert>
<Copy StartSeek="38698" EndSeek="149318"/>
</Delta>
<Delta Version="172" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:19:11000">
<Copy StartSeek="0" EndSeek="36430"/>
<Copy StartSeek="36480" EndSeek="149368"/>
</Delta>
<Delta Version="173" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:20:27000">
<Copy StartSeek="0" EndSeek="36430"/>
<Insert>	struct bm_pool *pool = &amp;mv_eth_pool[pkt-&gt;pool];;
</Insert>
<Copy StartSeek="36479" EndSeek="149367"/>
</Delta>
<Delta Version="174" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:20:29000">
<Copy StartSeek="0" EndSeek="37149"/>
<Insert>	if (!(dpe-&gt;mod_ops &amp; DPE_MOD_L2)) {
</Insert>
<Copy StartSeek="37170" EndSeek="149351"/>
</Delta>
<Delta Version="175" Comment="" NL="\10" Encoding="text" Date="2012/08/01" Time="17:59:0000">
<Copy StartSeek="0" EndSeek="36497"/>
<Insert>	printk(KERN_ERR "(%s:%d) FASTFWD port %s!\n", __func__, __LINE__,
	       dev-&gt;name);
</Insert>
<Copy StartSeek="36497" EndSeek="149264"/>
</Delta>
<Delta Version="176" Comment="" NL="\10" Encoding="text" Date="2012/08/05" Time="12:00:33000">
<Copy StartSeek="0" EndSeek="38567"/>
<Copy StartSeek="38568" EndSeek="149265"/>
</Delta>
<Delta Version="177" Comment="" NL="\10" Encoding="text" Date="2012/08/05" Time="12:10:34000">
<Copy StartSeek="0" EndSeek="38338"/>
<Insert>
</Insert>
<Copy StartSeek="38433" EndSeek="149359"/>
</Delta>
<Delta Version="178" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="14:41:20000">
<Copy StartSeek="0" EndSeek="36497"/>
<Copy StartSeek="36584" EndSeek="149446"/>
</Delta>
<Delta Version="179" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:03:3000">
<Copy StartSeek="0" EndSeek="42870"/>
<Copy StartSeek="42923" EndSeek="43020"/>
<Copy StartSeek="43073" EndSeek="149552"/>
</Delta>
<Delta Version="180" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:08:8000">
<Copy StartSeek="0" EndSeek="43173"/>
<Copy StartSeek="43227" EndSeek="149606"/>
</Delta>
<Delta Version="181" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:08:15000">
<Copy StartSeek="0" EndSeek="42870"/>
<Insert>			printk(KERN_ERR "(%s:%d)\n", __func__, __LINE__);
</Insert>
<Copy StartSeek="42870" EndSeek="149553"/>
</Delta>
<Delta Version="182" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:09:10000">
<Copy StartSeek="0" EndSeek="43089"/>
<Insert>				pkt-&gt;offset = NET_SKB_PAD;
</Insert>
<Copy StartSeek="43123" EndSeek="149556"/>
</Delta>
<Delta Version="183" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:18:21000">
<Copy StartSeek="0" EndSeek="43089"/>
<Insert>				pkt-&gt;offset = MV_ETH_MH_SIZE;
</Insert>
<Copy StartSeek="43120" EndSeek="149553"/>
</Delta>
<Delta Version="184" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:22:0000">
<Copy StartSeek="0" EndSeek="42967"/>
<Insert>			printk(KERN_ERR "(%s:%d)\n", __func__, __LINE__);
</Insert>
<Copy StartSeek="43032" EndSeek="149565"/>
</Delta>
<Delta Version="185" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:28:12000">
<Copy StartSeek="0" EndSeek="43057"/>
<Insert>				pkt-&gt;bytes = rx_bytes + MV_ETH_MH_SIZE;
				pkt-&gt;offset = NET_SKB_PAD;
</Insert>
<Copy StartSeek="43118" EndSeek="149551"/>
</Delta>
<Delta Version="186" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:36:27000">
<Copy StartSeek="0" EndSeek="42967"/>
<Insert>			printk(KERN_ERR "(%s:%d) rx_bytes %u\n", __func__, __LINE__);
</Insert>
<Copy StartSeek="43042" EndSeek="149561"/>
</Delta>
<Delta Version="187" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:37:48000">
<Copy StartSeek="0" EndSeek="43067"/>
<Insert>				pkt-&gt;bytes = rx_bytes;
</Insert>
<Copy StartSeek="43111" EndSeek="149578"/>
</Delta>
<Delta Version="188" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:45:35000">
<Copy StartSeek="0" EndSeek="43111"/>
<Insert>				pkt-&gt;offset = MV_ETH_MH_SIZE;
</Insert>
<Copy StartSeek="43142" EndSeek="149575"/>
</Delta>
<Delta Version="189" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:45:40000">
<Copy StartSeek="0" EndSeek="43111"/>
<Insert>				pkt-&gt;offset = NET_SKB_PAD;
</Insert>
<Copy StartSeek="43132" EndSeek="149565"/>
</Delta>
<Delta Version="190" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="22:57:44000">
<Copy StartSeek="0" EndSeek="43111"/>
<Insert>				pkt-&gt;offset = 0;
</Insert>
<Copy StartSeek="43142" EndSeek="149575"/>
</Delta>
<Delta Version="191" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:00:21000">
<Copy StartSeek="0" EndSeek="43067"/>
<Insert>				pkt-&gt;bytes = rx_bytes + MV_ETH_MH_SIZE;
</Insert>
<Copy StartSeek="43094" EndSeek="149558"/>
</Delta>
<Delta Version="192" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:01:4000">
<Copy StartSeek="0" EndSeek="38748"/>
<Insert>
</Insert>
<Copy StartSeek="38748" EndSeek="43066"/>
<Insert>				pkt-&gt;bytes = rx_bytes;
</Insert>
<Copy StartSeek="43110" EndSeek="149574"/>
</Delta>
<Delta Version="193" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:10:42000">
<Copy StartSeek="0" EndSeek="46789"/>
<Insert>		dpa_os_tx_hook(skb-&gt;data + MV_ETH_MH_SIZE, skb-&gt;dpa_cookie,
</Insert>
<Copy StartSeek="46834" EndSeek="149557"/>
</Delta>
<Delta Version="194" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:30:20000">
<Copy StartSeek="0" EndSeek="46789"/>
<Insert>		dpa_os_tx_hook(skb-&gt;data, skb-&gt;dpa_cookie,
</Insert>
<Copy StartSeek="46851" EndSeek="149574"/>
</Delta>
<Delta Version="195" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:31:28000">
<Copy StartSeek="0" EndSeek="46789"/>
<Insert>		dpa_os_tx_hook(skb-&gt;data - MV_ETH_MH_SIZE, skb-&gt;dpa_cookie,
</Insert>
<Copy StartSeek="46834" EndSeek="149557"/>
</Delta>
<Delta Version="196" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:38:2000">
<Copy StartSeek="0" EndSeek="43066"/>
<Insert>				pkt-&gt;bytes = rx_bytes + MV_ETH_MH_SIZE;
</Insert>
<Copy StartSeek="43114" EndSeek="149561"/>
</Delta>
<Delta Version="197" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:48:47000">
<Copy StartSeek="0" EndSeek="43114"/>
<Insert>				pkt-&gt;offset = NET_SKB_PAD;
</Insert>
<Copy StartSeek="43162" EndSeek="149578"/>
</Delta>
<Delta Version="198" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:51:39000">
<Copy StartSeek="0" EndSeek="43114"/>
<Insert>				pkt-&gt;offset = NET_SKB_PAD - MV_ETH_MH_SIZE;
</Insert>
<Copy StartSeek="43162" EndSeek="149578"/>
</Delta>
<Delta Version="199" Comment="" NL="\10" Encoding="text" Date="2012/08/22" Time="23:56:12000">
<Copy StartSeek="0" EndSeek="38425"/>
<Insert>	printk(KERN_ERR "(%s:%d) tx_desc-&gt;cmd %#x\n", __func__, __LINE__, 
	       tx_desc-&gt;command);
</Insert>
<Copy StartSeek="38426" EndSeek="42872"/>
<Insert>			printk(KERN_ERR "(%s:%d) rx_bytes %u\n", __func__, __LINE__, rx_bytes);
</Insert>
<Copy StartSeek="42872" EndSeek="42993"/>
<Insert>				printk(KERN_ERR "(%s:%d)\n", __func__, __LINE__);
</Insert>
<Copy StartSeek="42994" EndSeek="149356"/>
</Delta>
<Delta Version="200" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:04:47000">
<Copy StartSeek="0" EndSeek="42897"/>
<Insert>				pkt-&gt;bytes = rx_bytes /*+ MV_ETH_MH_SIZE*/;
</Insert>
<Copy StartSeek="42924" EndSeek="149335"/>
</Delta>
<Delta Version="201" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:05:28000">
<Copy StartSeek="0" EndSeek="38425"/>
<Insert>
</Insert>
<Copy StartSeek="38519" EndSeek="149428"/>
</Delta>
<Delta Version="202" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:07:30000">
<Copy StartSeek="0" EndSeek="37567"/>
<Insert>	}
</Insert>
<Copy StartSeek="37641" EndSeek="149499"/>
</Delta>
<Delta Version="203" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:44:11000">
<Copy StartSeek="0" EndSeek="37568"/>
<Insert>		ip_hdr(skb)-&gt;check = ip_fast_csum((unsigned char *)iph, iph-&gt;ihl);
</Insert>
<Copy StartSeek="37571" EndSeek="149433"/>
</Delta>
<Delta Version="204" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:44:46000">
<Copy StartSeek="0" EndSeek="37428"/>
<Insert>			  ((NETA_RX_GET_IPHDR_OFFSET(rx_desc)) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
</Insert>
<Copy StartSeek="37480" EndSeek="149414"/>
</Delta>
<Delta Version="205" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:56:13000">
<Copy StartSeek="0" EndSeek="37428"/>
<Insert>			  (pkt_offset + 14) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
</Insert>
<Copy StartSeek="37482" EndSeek="149416"/>
</Delta>
<Delta Version="206" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:57:14000">
<Copy StartSeek="0" EndSeek="37428"/>
<Insert>			  ((pkt_offset + 14)) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
</Insert>
<Copy StartSeek="37481" EndSeek="149415"/>
</Delta>
<Delta Version="207" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:57:39000">
<Copy StartSeek="0" EndSeek="37428"/>
<Insert>			  ((pkt_offset + 14) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
</Insert>
<Copy StartSeek="37482" EndSeek="149416"/>
</Delta>
<Delta Version="208" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="00:58:19000">
<Copy StartSeek="0" EndSeek="37342"/>
<Copy StartSeek="37430" EndSeek="149504"/>
</Delta>
<Delta Version="209" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:14:24000">
<Copy StartSeek="0" EndSeek="37408"/>
<Insert>	       pkt-&gt;offset);
</Insert>
<Copy StartSeek="37432" EndSeek="37466"/>
<Copy StartSeek="37568" EndSeek="149608"/>
</Delta>
<Delta Version="210" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:15:40000">
<Copy StartSeek="0" EndSeek="37538"/>
<Insert>		       pkt-&gt;offset + 14, );
</Insert>
<Copy StartSeek="37594" EndSeek="149634"/>
</Delta>
<Delta Version="211" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:16:39000">
<Copy StartSeek="0" EndSeek="37466"/>
<Insert>		printk(KERN_ERR "(%s:%d) pkt-&gt;offset + 14 %#x\n", __func__, __LINE__,
</Insert>
<Copy StartSeek="37544" EndSeek="149640"/>
</Delta>
<Delta Version="212" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:16:49000">
<Copy StartSeek="0" EndSeek="37408"/>
<Insert>	       pkt-&gt;offset, );
</Insert>
<Copy StartSeek="37430" EndSeek="149638"/>
</Delta>
<Delta Version="213" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:17:24000">
<Copy StartSeek="0" EndSeek="36106"/>
<Copy StartSeek="36367" EndSeek="149899"/>
</Delta>
<Delta Version="214" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:22:55000">
<Copy StartSeek="0" EndSeek="37725"/>
<Insert>		printk(KERN_ERR "(%s:%d) pkt-&gt;offset + 14 %#x [%#x]\n", __func__, __LINE__,
		       pkt-&gt;offset + 14, *(pkt-&gt;pBuf) + pkt-&gt;offset);
</Insert>
<Copy StartSeek="37753" EndSeek="149793"/>
</Delta>
<Delta Version="215" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:23:54000">
<Copy StartSeek="0" EndSeek="37725"/>
<Insert>		__dump_pkt(pkt-&gt;pBuf, 5);
</Insert>
<Copy StartSeek="37754" EndSeek="149794"/>
</Delta>
<Delta Version="216" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:23:57000">
<Copy StartSeek="0" EndSeek="37725"/>
<Insert>		__dump_pkt(pkt-&gt;pBuf, 10);
</Insert>
<Copy StartSeek="37725" EndSeek="149765"/>
</Delta>
<Delta Version="217" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:29:17000">
<Copy StartSeek="0" EndSeek="37777"/>
<Insert>			  ((pkt-&gt;offset + 14) &lt;&lt; NETA_TX_L3_OFFSET_OFFS) |
</Insert>
<Copy StartSeek="37817" EndSeek="149751"/>
</Delta>
<Delta Version="218" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:31:31000">
<Copy StartSeek="0" EndSeek="37885"/>
<Insert>
		

	}
</Insert>
<Copy StartSeek="37888" EndSeek="149746"/>
</Delta>
<Delta Version="219" Comment="" NL="\10" Encoding="text" Date="2012/08/23" Time="01:32:18000">
<Copy StartSeek="0" EndSeek="36106"/>
<Insert>static void __dump_pkt(uint8_t *pkt, uint32_t bytes)
{
//#ifdef DEBUG_DUMP_PKT
	uint32_t i = 0;

	printk("%4x: ", i);
	for (i = 1; i&lt;= bytes; i++) {
		printk("%.2x ", (*pkt) &amp; 0xff);
		if ((i &amp; 0xf) == 0) {
			printk("\n%4x: ", i);
		}
		pkt++;
	}
//#endif
}


</Insert>
<Copy StartSeek="36106" EndSeek="37342"/>
<Insert>	printk(KERN_ERR "(%s:%d) pkt-&gt;offset %#x\n", __func__, __LINE__,
	       pkt-&gt;offset);
</Insert>
<Copy StartSeek="37342" EndSeek="38394"/>
<Insert>	printk(KERN_ERR "(%s:%d) tx_desc-&gt;cmd %#x\n", __func__, __LINE__,
	       tx_desc-&gt;command);
</Insert>
<Copy StartSeek="38395" EndSeek="149304"/>
</Delta>
</DeltaFile>
